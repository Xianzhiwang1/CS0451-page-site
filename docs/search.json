[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Xianzhi Wang’s Flabbergasted Blog for CS0451 Machine Learning class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Flabbergasted CSCI 0451 Blog",
    "section": "",
    "text": "My blog post on Palmer Penguins classification\n\n\n\n\n\n\nMay 3, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Image Compression with SVD and Spectral Community Detection\n\n\n\n\n\n\nApr 5, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy Blog post on Auditing Allocative Bias\n\n\n\n\n\n\nMar 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Linear Regression\n\n\n\n\n\n\nMar 20, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Kernel Logistic Regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimplementing gradient descent\n\n\n\n\n\n\nFeb 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy first Blog post on Perceptron\n\n\n\n\n\n\nFeb 15, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/my-blog-post-01/index.html",
    "href": "posts/my-blog-post-01/index.html",
    "title": "My first Blog post on Perceptron",
    "section": "",
    "text": "Introduction.\nIn this blog post, we implement the perceptron algorithm, which is oftentimes the first machine learning algorithm a student encounters in a machine learning class (which is at least true in my case). We write code in Python for this implementation, and our goal is to classify binary labeled artificial data.\n\n\nImplementation\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries that we need.\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nIn the following code cell, we will generate a linearly separable dataset of binary-labeled 2D points. The make_blobs function essentially takes \\(n\\) samples, a number of features, and classes, and spits out a dataset of points with a data set with the given size, and label the data points using the classes. Visually, we see two clusters of points of two different color. In this special case where I set the seed, those two clusters seems linearly separable, which just means we could draw a straight line that completely seprates them. If we go to higher dimensions, then we need precise mathematical definitions, but we don’t need to worry about that right now.\n\nnp.random.seed(42)\nn=100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features=p_features-1, centers=[ (-1.7,-1.7, -1.7), (1.7,1.7, 1.7) ])\n\nfig=plt.scatter(X[:,0], X[:,1], c=y)\nxlab=plt.xlabel(\"feature 1\")\nylab=plt.ylabel(\"feature 2\")\n\n\n\n\n\n\nThe Perceptron Algorithm\nOur goal is to find the separating line using the perceptron algorithm. The algorithm takes in our feature matrix X and our vector of labels y. As detailed in the source code (link at the start of the blog), the algorithm performs the following steps: * Initialize the weights vector w * Iterate through the data points (randomly), updating the weights w until either a user-specified maximum number of iteration is reached. * record the accuracy score in self.history.\nState in mathematical terms, we would like to apply the perceptron algorithm to find the hyperplane that separates those data points, given that they are separable (so perceptron algorithm will converge). A key equation in the perceptron algorithm that defines the update is the following:\n\n\n\\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\]\nAnd this will provide us with the step to update w in each iteration.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, maxiter=10000)\n\nprint(p.history[-10:])\nprint(p.w_)\n# print(X)\n\n# print(w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n[ 50.44366343  39.1028252   21.79442178 -40.        ]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(p.score(X,y))\n\n1.0\n\n\n\nprint((1<2)*2)\nprint((1>2)*2)\n\n2\n0\n\n\n\nprint(p.predict(X))\nprint(\"\\n\\n\")\nprint(y)\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\nprint(p.w_)\nprint(np.size(p.w_))\n\n[39.71808898 55.01564117 -7.        ]\n3\n\n\n\n\nTime complexity for update step\nRecall our equation for the update step: \\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\] This involves taking an inner product $ ^{(t)}, _i , $ which has time complexity \\(O(p)\\) where \\(p\\) is a constant denoting the number of features. The other operations are addition, multiplication, taking the simple (step) function, which have constant time complexity \\(O(1)\\)."
  },
  {
    "objectID": "posts/my-blog-post-02-gradient-descent/index.html",
    "href": "posts/my-blog-post-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Reference\nHere is a link to the main reference we are using when crafting this blog post.\n\n\nIntroduction\nLet’s recall what problem we are investigating. We are working on the empirical risk minimization problem, which involves finding a weight vector w, that satisfy the following general form: \\[\n\\hat{w} = \\arg \\min_{w} L(w).\n\\] where\n\\[\nL(w) =  \\frac{1}{n} \\sum_{i=1}^{n} \\ell [ f_w(x_i), y_i ]  \n\\] is our loss function. In a previous blog post, we took \\(\\ell(\\cdot, \\cdot)\\) to be the 0-1 loss, but this time, we are going to use a different function called logistic loss, and it is detailed below. First, let’s recall what is matrix X and what are we doing.\nRemember from our previous blog post that our data includes a feature matrix X, which is a \\(n\\times p\\) matrix with entries being real numbers. The feature matrix X is a bunch of rows stacked together, and each row is going to represent a data point in our data set. Hence, since we have \\(n\\) data points in our data set, we have \\(n\\) rows in our feature matrix X. Since we record in each data point \\(p\\) many features that constitutes this data point, our feature matrix X has \\(p\\) columns. In other words, the number \\(n\\) represents the number of distinct observations, corresponding to \\(n\\) rows in X. \\(p\\) will always denote the number of features in this blog post. Our data also have a y, which is called target vector and lives in \\(\\mathbb{R}^n\\). The target vector gives a label for each observation. Hence, we have X, which contains a lot of information, and we want to predict y.\nWe also need some formulas that’s computed using pen and paper by our friends in the math department. First, we remember this piece of notation \\[ f_w(x) := \\langle w, x \\rangle \\] and we could obtain the following: \\[ \\nabla L(w) = \\nabla ( \\frac{1}{n} \\sum_{i=1}^{n} \\ell [ f_w(x_i), y_i ] ). \\] And remember $ = w, x_i $ (another piece of notation!), the logistic loss we are using is \\[ \\ell(\\hat{y}, y) = -y \\log \\sigma (\\hat{y}) - (1-y) \\log(1-\\sigma(\\hat{y})), \\] where $ () $ denotes the logistical sigmoid function. as demonstrated in the link under the Reference heading above, we have \\[ \\frac{d \\ell(\\hat{y},y)}{d \\hat{y}} = \\sigma (\\hat{y}) -y. \\] Therefore, with some effort, one can do this computation and obtain the following formula: \\[ \\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^{n} (\\sigma(\\hat{y_i}) - y_i) x_i, \\] and this will help us to implement the gradient of the empirical risk for logistic regression in python code using numpy library.\nRecall that in single variable calculus, gradient is just the derivative of a function. In multivariable calculus, since we have more than one variable, we take derivative with respect to each variable and put them in a vector to get our gradient. In formulas, let \\(f(z_1, z_2, \\cdots, z_p): \\mathbb{R}^p \\mapsto \\mathbb{R}\\) be our function, and the gradient of \\(f\\), denoted by \\(\\nabla f\\) is given by \\[ \\nabla f(z_1, z_2, \\cdots, z_p) := \\begin{bmatrix}\n&\\frac{\\partial f}{\\partial z_1}\\\\\n&\\frac{\\partial f}{\\partial z_2}\\\\\n&\\vdots\\\\\n&\\frac{\\partial f}{\\partial z_p}\\\\\n\\end{bmatrix}\n\\] Hence, given a vector \\[\\mathbf{z} = \\begin{bmatrix}\n&z_1\\\\\n&z_2\\\\\n&\\vdots\\\\\n&z_p\n\\end{bmatrix}\n\\] we know that \\(\\nabla f(\\mathbf{z})\\) is also going to be a vector of the same dimesion, we could put them in the same equations. Hence, the following Batch Gradient Descent Algorithm makes sense.\n\n\nBatch Gradient Descent Algorithm:\nfor function \\(f\\), starting point \\(z^{(0)}\\), and learning rate \\(\\alpha\\), we perform the following update step many many times: \\[\nz^{(t+1)} \\leftarrow z^{(t)} - \\alpha \\nabla f(z^{(t)})\n\\] Return the final value \\(z^{(t)}\\).\nIn code, we need a precise way to decide when to stop after performing the update many times. One way is to stop when we reached the maximum_number_of_iteration or max_iter that is specified by the user, or until convergence, in the sense that \\(\\nabla f(z^{(t)})\\) is close to 0. Also, there are math theorems guarantee that \\(z^{(0)}, z^{(1)}, \\cdots, z^{(t)}\\) converges to \\(z^{*}\\) under suitable conditions. Now, with the math background out of the way, let’s see this in code.\n\n%load_ext autoreload\n%autoreload 2\n\nWe start by importing the relavant libraries and creating some data points using the make_blobs function that we imported from sklearn.datasets. We would like to create some non-separable data, which means graphically in 2 dimension, we cannot draw a straight line to separate the data points of the two different classes (as indicated by the color). Notice that the horizontal axis is Feature 1, and the vertical axis is Feature 2.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (4,4)\n\nimport numpy as np\nnp.random.seed(42)\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nRecall that we have feature matrix X, which is a \\(n\\times p\\) matrix with entries being real numbers. The number \\(n\\) represents the number of distinct observations, and we have \\(n\\) rows in X. \\(p\\) is the number of features. Our data also have a y, which is called target vector and lives in \\(\\mathbb{R}^n\\). The target vector gives a label, value, or outcome for each observation. In the solutions_logistic.py, we implemented the gradient descent using the following update step.\n\\[ w^{(t+1)} \\leftarrow w^{(t)}  - \\alpha \\cdot \\nabla L(w^{(t)}), \\]\nwhere \\(\\nabla L(w)\\) is given by the following equation: \\[ \\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell(f_{w}(x_i), y_i)\\] Now let’s import our implementation and create plots.\n\nfrom solutions_logistic import LogisticRegression \nLR = LogisticRegression()\nX_ = LR.pad(X)\n\n# inspect the fitted value of w\nLR.fit(X, y, alpha = 0.01, max_epochs = 2000)\nprint(LR.w_)\n\n[ 1.59965852  1.45281308 -0.20047656]\n\n\nAfter calling the function fit, we obtain the weight vector w_, but are they doing what they are supposed to do? How big is the loss for this perticular case? We could visualize this result by plotting the line that hopefully separates the data points in a intuitive way. See the picture on the left. Now we would like to find out about how the empirical loss evolves as the number of iteration goes up. Let’s plot this in the picture on the right.\n\nnp.random.seed(42)\n# pick a random weight vector and calculate the loss\nw = .5 - np.random.rand(p_features)\n# fig = plt.scatter(X_[:,0], X_[:,1], c = y)\n# xlab = plt.xlabel(\"Feature 1\")\n# ylab = plt.ylabel(\"Feature 2\")\n# f1 = np.linspace(-3, 3, 101)\n# p = plt.plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n# title = plt.gca().set_title(f\"Loss = {LR.last_loss}\")\n\n\nplt.rcParams[\"figure.figsize\"] = (18,6)\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nFrom the plot on the left, we see that our gradient descent algorithm is doing a good job at finding the line that separates the data. From the plot on the right, we see that as the number of itermations on the x-axis increases, the empirical risk goes down. The loss is around \\(0.15\\) to \\(0.20\\), and this is a reasonable number since our data is not linear separable, as we can see from the picture.\n\n\nAccuracy of regular gradint descent\nAgain, we draw the scatter plot and the fitted line on the left, and on the right, we plot the evolution of the accuracy score as the number of iteration increases.\n\nmyScore = LR.score(X_,y)\n\nfig, axarr = plt.subplots(1, 2)\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Score = {myScore}\")\nf1 = np.linspace(-3, 3, 101)\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\naxarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nWe could also print out the vector y and the predicted vector given by the function predict(). In this way, we could have a look “under the hood” and obtain a rough sense how good is our prediction.\n\nprint(f\"our actual labels: {y}\")\nprint(f\"our predicted labels: {LR.predict(X_)}\")\n\nour actual labels: [0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1\n 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0\n 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0\n 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0]\nour predicted labels: [0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0\n 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0\n 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0]\n\n\n\n\nStochastic Gradient Descent\nHere, by “Stochastic” we just mean we introduce a certain amount of randomness to our gradient descent step. The modification from the regular gradient descent is as follows. We pick a random subset \\(S \\subset [n]\\) and we let \\[ \\nabla_S L(w) = \\frac{1}{|S|} \\sum_{i \\in S} \\nabla \\ell(f_{w}(x_i), y_i).\\] And the rest is business as usual. We deem our weights w as “good enough” when: either the user-specified maximum number of iteration is reached, or the current empirical risk function is “close enough” to the one from the previous iteration. With the mathematics technicality out of the way, let’s visualize the scatter plot, the best-fit-line, and the evolution of the empirical risk, and the evolution of the accuracy score all in one go.\n\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = 1) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 3)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\naxarr[2].plot(LR.score_history)\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nStochastic gradient uses random batches of the data to compute the gradient, and in this case it performs similar to regular gradient descent. We see that as Iteration number increases, the empirical risk decreased and the accuracy score increased. Sometimes, there are kinks in the curve, which means more iterations is not always better. However, this time, there’s no kinks, and we see that our stochastic gradient did a good job at minimizing empirical risk as iteration increases.\n\nComparison of Gradient Descent, Stochastic Gradient Descent, and Stochastic Gradient with Momentum\nHaving seen how regular gradient descent and stochastic gradient descent perform, we could add a momentum feature to the stochastic gradient descent. Then we have the choice of selecting momentum = True when we call the function fit_stochastic. Hence, we could compare the three versions of gradient descent and plot their respective empirical risk (loss) evolution in one picture, where the horizontal axis is number of iterations, and the vertical axis is empirical risk. Also, let’s try having 5 features in our artificial data set for this comparison.\n\n# 5 features\np_features = 5 \nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 100, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe have just ploted the loss history over iteration number of the 3 methods we implemented. As we see in this plot, regular gradient descent is the worst at minimizing the empirical risk, and it also takes the longest to converge. Stochastic gradient did a better job than regular gradient descent at minimzing the empirical risk, and it converges faster. Stochastic gradient descent with momentum is clearly the best here, since it converged before hitting 100 iterations, and it did a good job at minimizing empirical risk, outperforms the regular stochastic after \\(10\\) iterations.\n\n\nChoosing an Alpha too big\nAgain, we take a look at regular gradient descent, and this time, we experiment with different values of alpha that are big.\n\n# back to 2 features\np_features = 2 \nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nWhen we choose a big alpha, such as alpha = 10, we see that the empirical risk is minimized in a slower way, and our curve on the right is less steep. Still our algorithm managed to find a line that reasonably separates the data.\n\nLR.fit(X, y, alpha = 90, max_epochs = 1000)\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nThis time, we let alpha=90, which is creating some strange behaviors if we look at the plot on the right. Again, x-axis is the iteration number, and the y-axis is the empirical risk that we are trying to minimize. However, instead of monotonically decreasing empirical risk as iteration increase, we see that empirical risk jumps up and goes down many times. Also, the separating line in the left plot is also slightly off compared to before. In this case, alpha is too big, and when we take a step in the correct direction, which is \\(-\\nabla f\\), we overshot and empirical risk goes up instead of down. Hence we have this periodic behavior.\n\n\nExperimenting with batch size that affects convergence speed.\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 5000, \n                  momentum = False, \n                  batch_size = 15, \n                  alpha = 1) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 3)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\naxarr[2].plot(LR.score_history)\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nWe see that as batch size gets smaller, we see Stochastic gradient might not do as well as before, since there’s kinks in the accuracy score history. The score hits it’s highest point, and then actually descreases slowly as iteration increases. Also, the separating line in the graph on the left is slightly off compared to the ones we had before, and we see that the loss is at \\(0.32\\) this time, which is higher than before. In the plot that’s below, we use different batch size and compare convergence behavior as iteration incrases.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 15, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient batch: 15\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 30, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient batch: 30\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 200, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient batch: 200\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 10000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe see that the Stochastic gradient with batch size \\(200\\) is performing relatively okay, using about \\(1000\\) iterations to converge, which is close to the performance of regular gradient descent. We see that smaller batch size actually performs better here, and the one with batch size 15 has converged with \\(100\\) iterations, and it did a good job at minimizing empirical risk. The one with batch size \\(30\\) also did well, converging using about \\(300\\) iterations, definitely faster than the one with batch size 200.\n\n\nDoes Momentum speeds up convergence and performs better than just Stochastic gradient?\n\n# 10 features\n# make the data\np_features = 10 \nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 5000, \n                  momentum = True, \n                  batch_size = 90, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 5000, \n                  momentum = False, \n                  batch_size = 90, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIn this case, with a data set having \\(10\\) features and a batch size of \\(90\\), we see that momentum clearly outperforms regular stochastic gradient descent. At some number of iteration, momentum achieves smaller empirical risk, and it also takes less number of iterations to converge than regular stochastic. And we conclude our blog post here."
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Reference for this blog post\nHere is a link to the main reference we are using when creating this blog post.\n\n\nIntroduction.\nIn this blog post I am going to discuss kernel logistic regression for binary classification. Recall that in previous blog posts, the visualization of the data set with \\(2\\) features looks graphically like it could be reasonably separated by a line. With more features, we could no longer visualize in \\(2\\) dimension, but the idea is the same, we use linear separation, like a hyperplane. However, what if the data is dominated by other patterns that are not suitable for a straight line (or a straight plane if you will) separation? Hence, we need to use some other methods, and hence let us use kernel logistic regression.\nRecall the empirical risk minimization problem in previous blog posts, which is finding the weight vector w that minimize the loss function \\(L(w)\\). \\[ \\hat{w} = \\arg \\min_{w} L(w), \\] where the loss function \\(L(w)\\) is of the following form: \\[ L(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell( \\langle w, x_i \\rangle, y_i ), \\] and $(, y ) $ is the logistic loss from the previous blog post. However, previously, we only studied linear decision boundaries, and now we want to study nonlinear patterns. Hence, instead of feature vector \\(x_i\\), we need modified feature vector \\(\\kappa(x_i)\\) that does the following: \\[\n\\kappa(x_i) =\n\\begin{bmatrix}\n&k(x_1, x_i)\\\\\n&k(x_2, x_i)\\\\\n&\\vdots\\\\\n&k(x_n, x_i)\\\\\n\\end{bmatrix}\n\\] where \\(k: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) is called a kernel function. Hence our empirical risk function (loss function) is now \\[ L_k(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell( \\langle w, \\kappa(x_i) \\rangle, y_i ), \\] where the subscript \\(k\\) in \\(L_k(w)\\) indicates we are using a kernel function \\(k\\).\n\n\n%load_ext autoreload\n%autoreload 2\n\nFirst, let’s import some libraries, and let’s create an artificial data set that has nonlinear patterns.\n\n\n# from sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles, make_blobs, make_biclusters, make_classification\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (6,3)\nplt.rcParams['figure.dpi'] = 117 \nplt.rcParams['savefig.dpi'] = 78 \nimport numpy as np\nnp.random.seed(42)\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\nnp.seterr(all=\"ignore\")\n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nFirst, we use make_moons from sklearn.datasets to generate some artificial data. How to deal with the nonlinear pattern we see in the plot below (graph on the left)? it seems a linear separator like a straight line will not do a satisfactory job here. Hence, we need kernel logistic regression, which we implemented in kernel_logistic.py, and we are going to import it into this notebook.\n\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom kernel_logistic import KLR \nX_moon, y_moon = make_moons(200, shuffle = True, noise = 0.1)\n\nAfter seeing this data set below, we would guess that a linear classification would not do a great job at classifying the patterns we see here with our eyes. Hence, without further ado, let’s see how our KLR, kernel logistic regression handle this binary classification problem. Let’s choose a gamma = 1.8 and fit our model.\n\nfrom kernel_logistic import KLR \nKLR5 = KLR(rbf_kernel, gamma = 1.8)\nKLR5.fit(X_moon, y_moon)\n\n\nKLR5.my_plot(X_moon, y_moon, \"make_moons\")\n\n\n\n\nWe see that we achieved a training score of \\(0.985\\). Not bad at all! Also, just looking at the graph on the right, we see that the decision boundary looks spot on, and we could tell it is a good classification by just looking at it. The blue squares land correctly in the blue region, and the orange triangles land correctly in the orange region. Still, let’s create a fresh synthetic data that our Kernel Logistic Regression has not seen before, and test it on that.\n\nX_moon2, y_moon2 = make_moons(200, shuffle = True, noise = 0.1)\n\nBelow, we see a testing score of \\(0.935\\), which is not bad at all considering our model has not seen this perticular synthetic data before!\n\nKLR5.my_plot(X_moon2, y_moon2, \"make_moons\")\n\n\n\n\nNot bad at all! We see that KLR did a decent job fitting the non-linear decision boundary, and achieved an accuracy score above \\(0.9\\). The data is quite neatly separated by this curvy decision boundary our KLR generated. Now, let’s explore the parameter gamma and also experiment with data that has different amount of noise in it.\n\n\nChoosing gamma and Try on a tiny example\nTo start us off, we manually create a tiny data set, and we call the fit function that we implemented in the source code (link at the start of the blog). We start with a big value for gamma: gamma = 100, and in this tiny data set, we could see clearly what’s going on with the decision boundary, and a tiny data set could also help with debugging purposes.\n\nXX = np.array([\n[-40.84739307, 30.71154296],\n [ 11.46814927, -9.28580296],\n [ -40.5192833,   -70.94984582],\n [ 10.73327397,  10.17310931],\n [ 10.33197143,  0.43375035],\n\n [ -1.62726102, -0.54736954],\n[-7.84739307, 5.71154296],\n [ -21.46814927, -19.28580296],\n [ -10.5192833,   -50.94984582],\n [ 7.73327397,  0.17310931],\n])\nyy = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\nplt.rcParams[\"figure.figsize\"] = (3,3)\nplt.scatter(XX[:,0], XX[:,1], c = yy)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nIt seems that those \\(10\\) data points does not display a linear pattern, which is what we want. Now we create an instance of the KLR class with gamma = 100 and test our fit function to see if it could classify these \\(10\\) points with \\(100 \\%\\) accuracy.\n\nfrom kernel_logistic import KLR \nKLR = KLR(rbf_kernel, gamma = 100)\nKLR.fit(XX, yy)\n\nWe see that our classifier makes a little circular boundary around each orange triangular data points, while the rest of the region is all classified as blue for the blue squares. gamma essentially controls how “wiggly” our decision boundary is allowed to be. If gamma is small, our decision boundary would be less curvy, and if gamma is too big, we tend to overfit and produce the picture below.\n\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAlso, we do achieve \\(100\\%\\) accuracy here, since our accuracy is \\(1.0\\). But if you insist on pop open the hood and inspect what’s underneath, let’s print out the actual label yy and our predicted labels, and see that they are indeed the same.\n\nprint(f\"Actual labels: {yy}\")\nprint(f\"Predicted labels: {KLR.predict(XX)}\")\n\nActual labels: [1 1 1 1 1 0 0 0 0 0]\nPredicted labels: [1 1 1 1 1 0 0 0 0 0]\n\n\nNow, with the same tiny data set, we use a very small gamma and see how the decision boundary would look like. Hence, let gamma = 0.05.\n\nfrom kernel_logistic import KLR \nKLR = KLR(rbf_kernel, gamma = 0.05)\nKLR.fit(XX, yy)\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nwe see that our decision boundary is less “smooth” and looks more “jagged”, so choosing a good gamma would decide the right complexity for our decision boundary, hence the parameter gamma is quite important to this binary classification problem.\n\n\nChoosing different noise level and Try on a synthetic data set with make_circles\nThis time, we set the size of the data set to contain \\(200\\) data points, and we call mak_circle to generate our data. We set noise = 0.8, so our data set is very noisy, and in the visualization below (the plot on the right), we see that we cannot really distinguish the circles by eye, so the data set is indeed very noisy and chaotic.\n\nX_cir, y_cir = make_circles(200, shuffle = True, noise = 0.8)\n\nLet’s set gamma = 1.8 and train our model by calling the .fit() function.\n\nfrom kernel_logistic import KLR \nKLR6 = KLR(rbf_kernel, gamma = 1.8)\nKLR6.fit(X_cir, y_cir)\n\nBelow, in the plot on the left, we see that with very noisy data, our Kernel Logistic Regression only scored \\(0.63\\) in accuracy, and judging by looking at the picture, we feel that our classifier only did a slightly above average job at classifying the data points. Hence, noisy data does negatively affect the performace of our Kernel Logistic Regression.\n\nplt.rcParams[\"figure.figsize\"] = (8,4)\nKLR6.my_plot(X_cir, y_cir, \"make_circles\")\n\n\n\n\nStill, let’s test our trained model on a fresh copy of synthetic data generated using make_circles. This time, the testing accuracy is only \\(0.435\\), which is less than \\(0.5\\). Hence, we might be better off just guess randomly instead of applying our training model in this case!\n\nX_cir2, y_cir2 = make_circles(200, shuffle = True, noise = 0.8)\nKLR6.my_plot(X_cir2, y_cir2, \"make_circles\")\n\n\n\n\nNow, we try with a data set with very little noise by setting noise = 0.05, and let the rest of the parameter stay the same. Hence, we keep gamma = 1.8, number of data points \\(200\\), etc.\n\nX_cir3, y_cir3 = make_circles(200, shuffle = True, noise = 0.05)\n\n\nfrom kernel_logistic import KLR \nKLR2 = KLR(rbf_kernel, gamma = 1.8)\nKLR2.fit(X_cir3, y_cir3)\n\n\n\nfig, axarr = plt.subplots(1,2)\naxarr[0].scatter(X_cir3[:,0], X_cir3[:,1], c = y_cir3)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = \"artificial data created by make_circles\")\n\n\naxarr[1].plot()\nplot_decision_regions(X_cir3, y_cir3, clf = KLR2)\nyourpredict = KLR2.predict(X_cir3)\naxarr[1].set(title = f\"Accuracy = {(yourpredict == y_cir3).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.tight_layout()\n\n\n\n\nThis time, since our data has pretty clear patterns, and has very little noise, our Kernel Logistic Regression did a great job at classifying the data points and engineering the decision boundary. We see that decision region plot on the right looks spot on, and the accuracy score is around \\(0.95\\), which is pretty high compared to the score we had before when the data was noisy.\n\n\nTry other problem geometries\nlet’s start with a data set generated by make_blobs function, with a reasonable amount of standard deviations (which is a proxy for noise in this case) in it.\n\nX, y = make_blobs(n_samples = 200, centers = 2, n_features = 2, cluster_std=3, random_state=42)\n\nLet us set gamma = 2.45 and see how well we do in this case.\n\nfrom kernel_logistic import KLR\nKLR3 = KLR(rbf_kernel, gamma = 2.45)\nKLR3.fit(X, y)\n\n\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8,4)\nKLR3.my_plot(X,y, \"make_blobs\")\n\n\n\n\n\nWe see that we achieved a very high accuracy score of \\(0.995\\). We could also call the .predict() function to find out the predicted value and compare that to the actual value of y, and see that they indeed match up.\n\nprint(f\"Actual labels: {y}\")\nprint(f\"Predicted labels: {KLR3.predict(X)}\")\n\nActual labels: [1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0\n 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0]\nPredicted labels: [1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0\n 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0]\n\n\nIn the next example, we use make_classification to make some sythetic data with \\(4\\) features. Since we have more than \\(2\\) features, we could only plot \\(2\\) dimensions at a time to visualize how our classification performs. Let’s generate the data set first.\n\nX_class, y_class = make_classification(n_samples = 200, \n                           n_classes = 2, \n                           n_redundant= 0,\n                           n_informative= 2,\n                           n_features = 4, \n                           n_clusters_per_class = 1, \n                           class_sep = 1.5,\n                           random_state=42)\n\nNow, we set gamma = 2.45 and we train our Kernel Logistic Regression on this data set.\n\nfrom kernel_logistic import KLR\nKLR4 = KLR(rbf_kernel, gamma = 2.45)\nKLR4.fit(X_class, y_class)\n\nNext, we call our costom function .myprint() that generate the plots for us that only focuses on the first \\(2\\) dimensions, since we can only plot \\(2\\) dimension at a time in 2D.\n\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (6,3)\nvalue = 1 \nwidth = 1 \nKLR4.myprint(X_class, y_class, \"make_classification\", value, width)\n\n\n\n\nNow, let’s also take a peak at how our Kernel Logistic Regression is doing in dimension \\(3\\) and \\(4\\). We see that the decision boundary looks pretty good! Notice that feature is labeled 0, 1, 2, 3, so we are looking at feature 2 and 3 here!\n\nplt.rcParams[\"figure.figsize\"] = (3,3)\nplot_decision_regions(X_class, y_class, clf = KLR4, \n                    feature_index=[2,3],\n                    filler_feature_values={0: value, 1: value},\n                    filler_feature_ranges={0: width, 1: width})\nyourpredict = KLR4.predict(X_class)\ntitle = plt.gca().set(title = f\"Accuracy = {(yourpredict == y_class).mean()}\",\n                xlabel = \"Feature 2\", \n                ylabel = \"Feature 3\")"
  },
  {
    "objectID": "posts/my-blog-post-04-linear-regress/index.html",
    "href": "posts/my-blog-post-04-linear-regress/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nIn this blog post let us take a look at Linear Regression. Linear regression, especially OLS (ordinary least squares) regression, is the bread and butter of many fields like economics and statistics. When we first learn about OLS regression, often times it was in some other setting, at least for yours truly. I have learned this in a economics class on regression analysis. However, we could easily formulate the same concept in a way that’s closer to the style of machine learning, and specifically classification tasks. For OLS linear regression, the loss function is \\[\n\\ell (\\hat{y}, y) = (\\hat{y} - y)^2,\n\\] where \\(\\hat{y} = \\langle w, x \\rangle\\) is the predicted labels (which the inner product of \\(w\\), the weights, and \\(x\\), our data point), and \\(y\\) is the true label. Hence, when we are trying to minimize loss, we are trying to minimize the squared difference between our prediction and the true label. The empirical risk minimization problem is \\[\n\\hat{w} = \\argmin_{w} L(w) \\\\\n\\] where \\[\nL(w) = \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\lvert \\lvert Xw - y \\rvert \\rvert ^2_2.\\\\\n\\] Hence, least square is simply referring to minimizing the sum of squares for the loss function.\nBefore we start the implementation, we first record the following code snippet that will help us to automatically load our source code when we are constantly editing the .ipynb and .py files.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries, then we perform our fit_gradient and fit_analytic on the following simple data set with only one features to visualize our linear regression.\n\nimport numpy as np\nnp.random.seed(42)\nfrom matplotlib import pyplot as plt\nimport matplotlib\nplt.rcParams[\"figure.figsize\"] = (18,6)\nplt.rcParams['figure.dpi'] = 156 \nplt.rcParams['savefig.dpi'] = 156 \nfrom linear_regression import LinearRegression \n\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\n\nFitting Linear Regression using gradient descent; a.k.a. fit_gradient\nIn fit_gradient, the key step is to compute the gradient using a descent algorithm so that we could solve the following problem: \\[ \\hat{w} = \\arg \\min_{w} L(w). \\] Equivalently, we could unpact this equation: \\[ \\hat{w} = \\sum_{i=1}^{n} \\ell(\\hat{y}_i, y_i) = \\argmin_{w} \\sum_{i=1}^{n} ( \\langle w, x_i \\rangle - y_i)^2.\\] Recall that our loss function is of the form $ (, y) = (-y)^2 $ since we are using ordinary least square regression.\nWe start by taking derivative with respect to \\(w.\\) Using chain rule for matrices, we obtain the following expression: \\[ \\nabla L(w) = 2 X^{T}(X\\cdot w -y).\\] Then, we use gradient descent to find the \\(w\\) that is “good enough.” We achieve this by the following iteration: \\[ w^{(t+1)} \\leftarrow w^{(t)} - 2 \\cdot \\alpha \\cdot X^{T} (X \\cdot w^{(t)} - y).\\]\nWe use the following code block to generate a small data set for testing our linear regression implementation. Let’s plot our data, and in this 2-Dim case we could see the linear pattern with our eyes, and it’s quite intuitive that the best-fit-line obtained by our algorithm should also “fit” the data visually as if we were to draw it by hand on the plot of this sythetic data set.\n\n# We start by generate a small data set.\nw0 = -0.5\nw1 =  0.7\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.figure(figsize=(6,6))\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nWe are able to generate data and visualize this problem when p_features = 1. Graphically, we are trying to draw a line “of best fit” through the data points in the sense of OLS, which stands for Ordinary Least Squares. The line we draw just means given the feature x, we find the corresponding predicted y using the line, which will be close to the original y, if we have done a good job.\nAfter importing linear_regression.py, we could call the fit_gradient method that implements the gradient descent algorithm for us, as illustrated in the above cell. In the following cell, we plot the “line of best fit” using the weights LR1.w that we obtained after running fit_gradient. Also, we print out the weights vector w that we fited, which seems to be doing a good job judging from the pictures below, plotted together with Lasso and fit_analytic.\n\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\nprint(f\"the weights that we obtained after calling fit_gradient are: {LR1.w}\")\n\nthe weights that we obtained after calling fit_gradient are: [[ 0.62864267]\n [-0.46564827]]\n\n\n\n\nUsing LASSO from scikit-learn\nRoughly, LASSO algorithm is OLS regression plus a regulariation term, so the loss function looks like this: \\[\nL(w) = \\lvert \\lvert Xw - y \\rvert \\rvert ^2_2 + \\lvert \\lvert w' \\rvert \\rvert_1.\\\\\n\\] Where \\[\n\\lvert \\lvert w' \\rvert \\rvert_1 = \\sum_{j=1}^{p-1} |w_j|.\\\\\n\\] Hence, we could think of this regularization term as some sort of penalty term when we have problems with overparameterization. In this post, we are not going to dive deep into the details of LASSO regression, so we are just going to use the implementation from scikit-learn. Again, we plot the result of fitting this regression together with the fit_gradient and fit_analytic in the big visualization below.\n\nL.fit(x,y)\nL.score(x,y)\nprint(\"*\")\nL_w = np.hstack([L.coef_, L.intercept_])\nprint(f\"the weights that we obtained after calling fit_gradient are: {L_w}\")\n\n*\nthe weights that we obtained after calling fit_gradient are: [ 0.6426091  -0.47312394]\n\n\n\n\nFitting Linear Regression using a analytic formula; a.k.a. fit_analytic\nSimilarly to fit_gradient, we also have a method called fit_analytic, which uses a formula to compute the weights w exactly, and this is implemented using the followiing equation: \\[ \\hat{w} = (X^T X)^{-1} X^T y, \\] where \\(\\hat{w}\\) denotes the weights we obtained after calling the function fit_analytic. Note that in order for this formula to make sense, we need X to be a invertible matrix. Now, with the math part out of the way, let’s see this in action in the following block, where we plot the three regressions we introduced so far together in one plot.\n\nmatplotlib.rc('font', size=6)\n# gradient\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\n\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\naxarr[0].scatter(x,y)\naxarr[0].plot(x, X_@LR1.w, color = \"black\")\n\n# Analytic \nLR2 = LinearRegression()\nX_ = LR2.pad(x)\nLR2.fit_analytic(X_,y)\n\naxarr[1].scatter(x,y)\naxarr[1].plot(x, X_@LR2.w, color = \"black\")\n\n# LASSO\naxarr[2].scatter(x,y)\naxarr[2].plot(x, X_@L_w, color = \"black\")\n\nlabs = axarr[0].set(title=\"Best-fit-line by implementing gradient descent\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[1].set(title=\"Best-fit-line by implementing the analytic formula\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[2].set(title=\"Best-fit-line by implementing LASSO\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\nplt.tight_layout()\n\n\n\n\n\n\nMore than one feature\nNow we use the following function to create both testing and validation data. At this stage, we could experiment with more features. We use the following code to create artificial data sets that has any number of features that we specify.\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n        # print(w)\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = LR.pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = LR.pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nWhen the number of features is one, p_features = 1, we could plot the artificial training data set and the validation data set. We lose this luxury when we have 2 or more features. Let’s plot the data set we are going to use.\n\nn_train = 100\nn_val = 100\np_features = 1 \nnoise = 0.2\n\n# create some data\nLR = LinearRegression()\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow we experiment with the number of features being n_train - 1, which is quite a lot features. Are we going to have a high training score? Let’s find out!\n\n\nn_train = 100\nn_val = 100\np_features = n_train - 1 \nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nHere’s the snippets within the fit_gradient function that makes the same code work for different number of features:\nfeatures = X_.shape[1]\nself.w = np.random.rand(features)\n\nfrom linear_regression import LinearRegression \nLR = LinearRegression()\nX_train_ = LR.pad(X_train)\nX_val_ = LR.pad(X_val)\nLR.fit_analytic(X_train_, y_train) # I used the analytical formula as my default fit method\nprint(f\"Training score = {LR.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val_, y_val).round(4)}\")\n\nTraining score = 0.0333\nValidation score = -0.2332\n\n\nWe see that our training score is very close to zero, hence very low.\n\nprint(f\"The estimated weight vector w is: {LR.w}\")\nprint(f\"Training Loss = {LR.Big_L(X_train_, y_train).round(2)}\")\nprint(f\"Validation Loss = {LR.Big_L(X_val_, y_val).round(2)}\")\n\nThe estimated weight vector w is: [-1.60048517 -3.1978511   0.80566383  1.10203529  1.36187644 -0.47123621\n  1.06288478 -0.49759679  0.88232925 -3.21484205  1.00067548 -0.85983013\n  0.36202332  0.61615846  3.23278271  0.49621812  1.51714629 -1.97442441\n  2.91490824 -0.56270441  1.34873889 -4.33324786 -3.39289948 -1.86798015\n -4.24483306 -3.9345999  -0.38623783  0.77311176  2.2738731  15.68998649]\nTraining Loss = 0.0\nValidation Loss = 20.17\n\n\nLet’s plot the score history and loss history to get a better idea of what’s going on here. We see that we get pretty low scores, and our loss history doesn’t look very good. Hence, our OLS regression is not super good when the number of features gets to large.\n\nLR5 = LinearRegression()\n\nLR5.fit_gradient(X_train_, y_train, 0.0001, 1000)\nprint(f\"Training score = {LR5.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR5.score(X_val_, y_val).round(4)}\")\n\n# plt.plot(LR2.score_history)\n# labels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].plot(LR5.score_history)\naxarr[1].plot(LR5.loss_history)\nlabs = axarr[0].set(title = \"Score History\", xlabel = \"Iteration\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Loss History\", xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.tight_layout()\n\nTraining score = -5.9295\nValidation score = -6.2086\n\n\n\n\n\n\nLASSO Regularization\nIn this last section, let us recall that LASSO uses a modified loss function of the following expression: \\[ L(w) = \\lVert X \\cdot w -y \\rVert ^2_2 + \\sum_{j=1}^{p-1} \\alpha \\cdot | w_j |. \\] And hopefully, LASSO will be a better option when number of features gets too big.\n\nL2 = Lasso(alpha = 0.01)\n\n\n\nn_train = 30 \nn_val = 30\np_features = 1 \nnoise = 0.2\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2.score(X_val, y_val)\n\n0.5877132844037414\n\n\nHey, this score is not bad!\n\nLR4 = LinearRegression()\nLR4.lasso_score(n_train, n_val, noise)\nLR4.lin_regress_score(n_train, n_val, noise)\nLR4.lin_regress_score_analytic(n_train, n_val, noise)\n\nLet’s use a custom function that we implemented in linear_regression.py to increase the number of features one by one, and for each number of feature, we generate a new sythetic data set and fit the \\(3\\) regressions we introduced so far. Then we plot the Score history for all three regressions against number of features, so y-axis is score, and x-axis is number of features. In this way, we could see clearly how fit_gradient, fit_analytic, and LASSO perform as the number of features increases to up to \\(n\\), which is the number of data points.\n\n# from matplotlib.pyplot import figure\n# figure(figsize=(8, 6), dpi = 156)\nfig, axarr = plt.subplots(1, 3, sharex = False, sharey = False)\naxarr[0].plot(LR4.lasso_score_history)\naxarr[1].plot(LR4.fit_gradient_score_history)\naxarr[2].plot(LR4.fit_analytic_score_history)\nlabs = axarr[0].set(title = \"LASSO Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Linear regression fit_gradient Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[2].set(title = \"Linear regression fit_analytic Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nplt.tight_layout()\n\n\n\n\nWe see, it turns out that all three are not doing super well as the number of feature increases. The score goes up and down wildly, and especially for fit_analyitic, it seems that the score just fall off the cliff at one point and then bounced back somehow. Hence, as the number of features increases too much, we tend to have problems with the three regressions we introduced in this post."
  },
  {
    "objectID": "posts/my-blog-post-05-audit-bias/index.html",
    "href": "posts/my-blog-post-05-audit-bias/index.html",
    "title": "My Blog post on Auditing Allocative Bias",
    "section": "",
    "text": "Here is a link to the source code for this blog post. ### Reference Here is a link to the main guide and reference when we write this blog post. Another reference is this paper that documents which variable means what in the PUMS data set we are going to use.\n### Introduction. From the PUMS official website, we can learn all we want about the PUMS data. The American Community Survey (ACS) Public Use Microdata Sample (PUMS) files are data about individual people and housing units. There are two types of PUMS files, one for Person records and one for Housing Unit records. Each record in the Person file represents a single person. Individuals are organized into households. PUMS files for an individual year contain data on approximately one percent of the United States population.\nIn this blog post, we are going to focus on PUMS data on Indiana in 2018. The reason we pick Indiana is that we want to work with a state with a smallish population size, so that it would be easier to download this data using folktables to our local machine and work with it. Why not pick Illinois, one might have asked. Anyway, it’s kind of random, the author admits.\nTo be specific, we would like to use this data and models such as logistic regression to predict whether someone is employed or not in Indiana in 2018. Then, we would like to investigate whether the prediction given by our model is racially biased."
  },
  {
    "objectID": "posts/my-blog-post-05-audit-bias/index.html#fit-a-logistic-regression-model",
    "href": "posts/my-blog-post-05-audit-bias/index.html#fit-a-logistic-regression-model",
    "title": "My Blog post on Auditing Allocative Bias",
    "section": "fit a logistic regression model",
    "text": "fit a logistic regression model\nAfter consideration, we decide to go with logistic regression. We build our model, and we fit our model on our training data, which is stored in variable X_train, and y_train.\n\n# model = make_pipeline(StandardScaler(), LogisticRegression())\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(random_state=0, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0, solver='liblinear')\n\n\n\ntrain_score = model.score(X_train,y_train)\ntest_score = model.score(X_test,y_test)\nprint(f\"The overall training score is: {round(train_score,3)} for logistic regression with solver equal to liblinear\")\nprint(f\"The overall testing score is: {round(test_score, 3)} for logistic regression with solver equal to liblinear\")\n\nThe overall training score is: 0.84 for logistic regression with solver equal to liblinear\nThe overall testing score is: 0.833 for logistic regression with solver equal to liblinear\n\n\n\ny_hat = model.predict(X_test)\nprint(f\" The overall testing accuracy in predicting whether someone is employed in 2018 in Indiana is: {(y_hat == y_test).mean()}\", \"\\n\",\n    f\" The accuracy for white individuals is {(y_hat == y_test)[group_test == 1].mean()}\", \"\\n\", \n    f\" The accuracy for black individuals is {(y_hat == y_test)[group_test == 2].mean()}\")\nprint(f\" Classification Report:\\n{classification_report(y_test, y_hat)}\")\n\n The overall testing accuracy in predicting whether someone is employed in 2018 in Indiana is: 0.8331855791962175 \n  The accuracy for white individuals is 0.8338424983027835 \n  The accuracy for black individuals is 0.8265086206896551\n Classification Report:\n              precision    recall  f1-score   support\n\n       False       0.82      0.89      0.85      7277\n        True       0.86      0.77      0.81      6259\n\n    accuracy                           0.83     13536\n   macro avg       0.84      0.83      0.83     13536\nweighted avg       0.83      0.83      0.83     13536\n\n\n\n\nCross Validation for Logistic Regression\n\n# cross validation\ncv_scores = cross_val_score(model, X_train, y_train, cv=5)\nprint(f\"the cross validation scores are: {cv_scores}\")\ncv_average = cv_scores.mean()\nprint(f\"the average score for cross validation is: {round(cv_average,3)}\")\n\nthe cross validation scores are: [0.83876628 0.83876628 0.83830455 0.84246006 0.8405061 ]\nthe average score for cross validation is: 0.84\n\n\n\n\nLogistic Regression with Polynomial Features\n\nplr = PolynomialFeatures(degree=2, include_bias=False)\npoly_feature = plr.fit_transform(X_train)\n\nBy using fit_transform(), we have fitted and transformed our traing data X_train. We also created the square of the numbers, since we have set the degree to \\(2\\). We could print out the shapes of poly_feature and X_train for comparison.\n\nprint(poly_feature.shape, X_train.shape)\n\n(54144, 152) (54144, 16)\n\n\n\nLR_poly = LogisticRegression()\nLR_poly.fit(poly_feature, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nprint(f\"The training score is: {LR_poly.score(poly_feature, y_train)}\")\ny_hat_poly = LR_poly.predict(poly_feature)\nscore = (y_hat_poly == y_train).mean()\nprint(f\" The overall training accuracy for polynomial features to predict whether someone is employed in 2018 in Indiana is: {round(score, 3)}\")\nprint(f\" Classification Report:\\n{classification_report(y_train, y_hat_poly)}\")\n\nThe training score is: 0.7287603427895981\n The overall training accuracy for polynomial features to predict whether someone is employed in 2018 in Indiana is: 0.729\n Classification Report:\n              precision    recall  f1-score   support\n\n       False       1.00      0.50      0.67     29286\n        True       0.63      1.00      0.77     24858\n\n    accuracy                           0.73     54144\n   macro avg       0.81      0.75      0.72     54144\nweighted avg       0.83      0.73      0.71     54144\n\n\n\n\nCross Validation for Polynomial Features Logistic Regression\n\n# cross validation\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"always\")\n    warnings.filterwarnings(\"ignore\")\n    cv_scores_plr = cross_val_score(LR_poly, X_train, y_train, cv=5)\nprint(f\"\\nthe cross validation scores for degree two polynomial logistic regression are: {cv_scores_plr}\")\ncv_average_plr = cv_scores_plr.mean()\nprint(f\"the average score for cross validation is: {round(cv_average_plr,3)} for degree two polynomial logistic regression\\n\")\n\n\nthe cross validation scores for degree two polynomial logistic regression are: [0.83313325 0.83830455 0.82814664 0.82500693 0.83053195]\nthe average score for cross validation is: 0.831 for degree two polynomial logistic regression"
  },
  {
    "objectID": "posts/my-blog-post-05-audit-bias/index.html#fit-a-decisiontreeclassifier-model",
    "href": "posts/my-blog-post-05-audit-bias/index.html#fit-a-decisiontreeclassifier-model",
    "title": "My Blog post on Auditing Allocative Bias",
    "section": "Fit a DecisionTreeClassifier model",
    "text": "Fit a DecisionTreeClassifier model\nNow, let us fit another model.\n\ndecisiontree = DecisionTreeClassifier(max_depth=5,\n                       splitter=\"best\",\n                        max_features=None,\n                         random_state=None,\n                          max_leaf_nodes=None,\n                           class_weight=None)\ndecisiontree.fit(X_train, y_train)\n\nDecisionTreeClassifier(max_depth=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=5)\n\n\nNow we score our Decision Tree Classifier on the test sets, and we could read off the overall test score as \\(0.833\\).\n\ntrain_score_dt = decisiontree.score(X_train,y_train)\ntest_score_dt = decisiontree.score(X_test,y_test)\n\nprint(f\"The overall training score is: {round(train_score_dt,3)} for Decision Tree Classifier with max_depth equals 5\")\nprint(f\"The overall testing score is: {round(test_score_dt, 3)} for Decision Tree Classifier with max_depth equals 5\")\n\nThe overall training score is: 0.886 for Decision Tree Classifier with max_depth equals 5\nThe overall testing score is: 0.884 for Decision Tree Classifier with max_depth equals 5\n\n\n\nCross Validation for Decision Tree Classifier\n\n# cross validation\ncv_scores_df = cross_val_score(decisiontree, X_train, y_train, cv=5)\nprint(f\"the cross validation scores are: {cv_scores_df}\")\ncv_average_df = cv_scores_df.mean()\nprint(f\"the average score for cross validation is: {round(cv_average_df,3)}\")\n\nthe cross validation scores are: [0.88170653 0.88854003 0.8847539  0.88262998 0.88520502]\nthe average score for cross validation is: 0.885\n\n\n\n\nTuning DecisionTreeClassifier\n\n\ndecisiontree2 = DecisionTreeClassifier(max_depth=None,\n                       splitter=\"best\",\n                        max_features=None,\n                         random_state=None,\n                          max_leaf_nodes=None,\n                           class_weight=None)\ndecisiontree2.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\ntrain_score_dt = decisiontree.score(X_train,y_train)\ntest_score_dt = decisiontree.score(X_test,y_test)\nprint(f\"The overall training score is: {round(train_score_dt,3)} for Decision Tree Classifier with max_depth equals 5\")\nprint(f\"The overall testing score is: {round(test_score_dt, 3)} for Decision Tree Classifier with max_depth equals 5\")\n# cross validation\ncv_scores_df = cross_val_score(decisiontree, X_train, y_train, cv=5)\nprint(f\"the cross validation scores are: {cv_scores_df}\")\ncv_average_df = cv_scores_df.mean()\nprint(f\"the average score for cross validation is: {round(cv_average_df,3)}\")\n\nThe overall training score is: 0.886 for Decision Tree Classifier with max_depth equals 5\nThe overall testing score is: 0.884 for Decision Tree Classifier with max_depth equals 5\nthe cross validation scores are: [0.88170653 0.88854003 0.8847539  0.88262998 0.88520502]\nthe average score for cross validation is: 0.885"
  },
  {
    "objectID": "posts/my-blog-post-06-unsupervised-learning/index.html",
    "href": "posts/my-blog-post-06-unsupervised-learning/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "# %reload_ext autoreload\n\n\nLink to Source Code\nHereis a link to the source code for this post.\n\n\nLink to reference for this blog post\nHere is a link to the main reference we use as we implement this post.\n\n\nPart I: Image Compression with the Singular Value Decomposition\nWe use the image of a cat which can be accessed here for free download: www.pexels.com. I have already downloaded a copy of an image of a tabby cat, and I have stored it in the same directory as this jupyter notebook.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nimport PIL\nfrom PIL import Image\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n# url = \"https://images.pexels.com/photos/1170986/pexels-photo-1170986.jpeg?cs=srgb&dl=pexels-evg-kowalievska-1170986.jpg&fm=jpg\"\n# myimg = read_image(url)\n\n\n# open the image from working directory\nimg = Image.open(\"./tabby_cat.png\")\nprint(f\"format: {img.format}\")\nprint(f\"size: {img.size}\")\nprint(f\"mode: {img.mode}\")\n# convert PIL images into numpy arrays.\nmyimg = np.asarray(img)\n\nformat: JPEG\nsize: (1771, 2657)\nmode: RGB\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(myimg)\n\naxarr[0].imshow(myimg)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\nprint(grey_img.shape[0])\nprint(grey_img.shape[1])\n\n2657\n1771\n\n\n\nfrom hidden_images import svd\nsvd1 = svd()\nk = 18 \nA_ = svd1.reconstruct(grey_img, k)\nsvd1.compare_images(grey_img, A_)\n\n\n\n\n\nsvd1.experiment(grey_img) \n\n\n\n\n\n\nPart II: Spectral Community Detection\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nWe need to return a vector of binary labels to split the graph.\n\nclubs = nx.get_node_attributes(G, \"club\")\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\n\nImplementing Laplacian Spectral Clustering\n\nfrom sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\nX, y = make_blobs(n_samples=100, n_features=2, \n                                centers=2, random_state=1)\n\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(42)\n\nn = 500\nX, y = make_circles(n_samples=n, shuffle=True, noise=0.07, random_state=None, factor = 0.5)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\nk = 10\nnbrs = NearestNeighbors(n_neighbors=k).fit(X)\nA = nbrs.kneighbors_graph().toarray()\n\n# symmetrize the matrix\nA = A + A.T\nA[A > 1] = 1\n\n\nimport networkx as nx\nfrom hidden_spectral import spectral \nspec  = spectral()\nspec.plot_graph(X, A)\n\n\n\n\n\n# fig, axarr = plt.subplots(1, 2, figsize = (8, 4))\ny_bad = np.random.randint(0, 2, n)\n\n# plot_graph(X, A, z = y, ax = axarr[0])\n# plot_graph(X, A, z = y_bad, ax = axarr[1])\n\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import pairwise_distances\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ngood labels cut = 22.0\nbad labels cut = 3000.0\ngood labels cut = 22.0\nbad labels cut = 3000.0\n\n\n\n\nUse a theorem from linear algebra\n\\(z\\) should be the eigenvector with the second smallest eigenvalue of the matrix \\[L = D^{-1}\\left[ D-A \\right]. \\] This matrix \\(L\\) is called the normalized Laplacian.\n\nfrom hidden_spectral import spectral \nspec = spectral()\nfig, ax = plt.subplots(figsize = (4, 4))\nz_ = spec.second_laplacian_eigenvector(A=A)\nspec.plot_graph(X, A, z=z_, ax = ax, show_edge_cuts = False)\n\n\n\n\n\nz = z_ > 0\nfig, ax = plt.subplots(figsize = (4, 4))\nspec.plot_graph(X, A, z, show_edge_cuts = True, ax = ax)\n\n\n\n\n\nfrom sklearn.datasets import make_moons\n\nH, z = make_moons(n_samples=100, random_state=1, noise = .1)\nfig, ax = plt.subplots(figsize = (4, 4))\na = ax.scatter(H[:, 0], H[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfig, ax = plt.subplots(figsize = (4, 4))\nz_hat = spec.spectral_clustering(H, k=6)\na = ax.scatter(H[:, 0], H[:, 1], c = z_hat, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spec.spectral_clustering(X, k = i)\n    a = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spec.spectral_clustering(H, k = i)\n    a = ax.scatter(H[:, 0], H[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\n\n\nTesting\n\n\nL = np.diag([4,5,22,2,3])\n# print(L)\nevalue, evector = np.linalg.eig(L)\nprint(\"evalue\")\nprint(evalue)\nprint(\"evector\")\nprint(evector)\nk = L.shape[1] \nidx = evalue.argsort()[:k][::-1] \nevalue = evalue[idx]\nevector = evector[:, idx]\n\nprint(\"evalue after change\")\nprint(evalue)\nprint(\"evector after change\")\nprint(evector)\n\nindex = evector.shape[1]\nprint(\"col\")\nprint(evector[:,index-2])\n\nevalue\n[ 4.  5. 22.  2.  3.]\nevector\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\nevalue after change\n[22.  5.  4.  3.  2.]\nevector after change\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]]\ncol\n[0. 0. 0. 0. 1.]"
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Choosing Features\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \n                  'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen',\n                  'Stage_Adult, 1 Egg Stage', 'Clutch Completion_No',\n                  'Clutch Completion_Yes', 'Sex_FEMALE', 'Sex_MALE']\n\nWhat we do here is that we choose the feature combination based on which combination have the highest score using Logistic Regression from sklearn. We try every single combination exhaustively and compute it’s score, and record the combination and score pair in a dictionary. Then we print out the combination with the highest score for future use.\n\nPG.select_combin_Logistic(df=X_train, y=y_train, all_qual_cols=all_qual_cols, all_quant_cols=all_quant_cols)\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nprint(max([-10, 5, 3], key=abs))\nprint(f\"The combination of columns that achieves the highest scores is: {max(PG.feature_score_pair, key = PG.feature_score_pair.get)}\")\nprint(max(PG.feature_score_pair.values()))\n\n-10\nThe combination of columns that achieves the highest scores is: ('Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)')\n1.0\n\n\n\n\ncols = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', \n        'Culmen Length (mm)', 'Culmen Depth (mm)']\nprint(f\"the best column combination where one column is qualitative and two columns are quantitative is:\\n{cols}\")\n\nthe best column combination where one column is qualitative and two columns are quantitative is:\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\n\n\nModel Choices\nLet’s read in our test data and use the same prepare_data() function we have to make our data ready for use.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\nX_test, y_test = PG.prepare_data(test)\n\nDespite we already have picked a highest scoring combination according to Logistic Regression from sklearn, We still want to experiment with different column combinations just to see how things goes with different classifiers. Hence, we keep some combinations that we tried here, and one could commend out other combinations to run the code with the column combination cols that they want to try.\n\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ncols = ['Flipper Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)'] \ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Body Mass (g)']\n\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Body Mass (g)', \n        'Sex_FEMALE', 'Sex_MALE']\n\ncols = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen',\n        'Culmen Length (mm)', 'Culmen Depth (mm)'] \n\n\n\nUsing Support Vector Classification from scikit-learn\nHere, we are using out-of-the-box implementations from scikit-learn, so we just do a very standard work flow with fitting, scoring using cross-validation on testing data, and plotting using a function that’s provided from the reference we linked at the start of the post. Let’s try the combination of Culmen Length, Dulmen Depth, and Clutch Completion for this one. Then we will stick with the highest scoring combination.\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', \n        'Clutch Completion_No', 'Clutch Completion_Yes'] \n\n\nmySvc = SVC(kernel=\"linear\", gamma=\"auto\", shrinking=False)\n\n\nmySvc.fit(X_train[cols],y_train)\n\nSVC(gamma='auto', kernel='linear', shrinking=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma='auto', kernel='linear', shrinking=False)\n\n\n\ncross_v_score = cross_val_score(mySvc, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \n\nthe cross validation scores are: [0.91666667 1.         1.         0.90909091 1.         0.72727273]\n\n\nScores are really high! And the picture below looks good! It seems the classifier has succesfully classified the penguins! The blue dots are classified as blue, green dots classified in the green region, and so on.\n\n# plt.rcParams[\"figure.figsize\"] = (4,4)\nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(mySvc, X_test[cols], y_test)\n# PG.plot_regions(mySvc, X_train[cols], y_train)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n\n\n\n\n\n\n\nUsing Random Forest Classifier from scikit-learn\nAgain, we are going to fit, score using cross-validation, and plot a nice graph.\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)',\n        'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nrf = RandomForestClassifier(random_state=0)\nrf = rf.fit(X_train[cols],y_train)\nrf.score(X_train[cols],y_train)\n\n1.0\n\n\n\ncross_v_score = cross_val_score(rf, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \n\nthe cross validation scores are: [0.91666667 1.         1.         1.         0.90909091 0.90909091]\n\n\nScores are high, but the plot below looks not bad! Our classifier is doing a great job at tellling apart the penguins! It seems all penguins are classified correctly!\n\nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(rf, X_test[cols], y_test)\n# PG.plot_regions(rf, X_train[cols], y_train)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n\n\n\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', \n        'Clutch Completion_No', 'Clutch Completion_Yes'] \nrf = RandomForestClassifier(random_state=0)\nrf = rf.fit(X_train[cols],y_train)\nrf.score(X_train[cols],y_train)\ncross_v_score = cross_val_score(rf, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(rf, X_test[cols], y_test)\n# PG.plot_regions(rf, X_train[cols], y_train)\n\nthe cross validation scores are: [0.83333333 1.         1.         1.         0.90909091 0.72727273]\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n\n\n\n\n\nBut this time, everything looks good!\n\n\nUsing Decision Tree from scikit-learn\nAgain, we fit, score, and plot.\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)',\n        'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nclf = DecisionTreeClassifier(random_state=0)\nclf = clf.fit(X_train[cols], y_train)\nclf.score(X_train[cols], y_train)\n\n1.0\n\n\n\nprint(f\"The clf.score() on testing data is: {clf.score(X_test[cols], y_test)}\")\n\nThe clf.score() on testing data is: 0.9852941176470589\n\n\n\ncross_v_score = cross_val_score(clf, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \n\nthe cross validation scores are: [0.91666667 0.91666667 1.         1.         0.90909091 0.90909091]\n\n\nWe have pretty high scores, but the plot looks slightly weird with the green rectangle region between blue and red that is predicted Chinstrap, but we doing actually have penguins there! Decision Tree is not good at classifying the penguins using these columns!\n\nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(clf, X_test[cols], y_test)\n# PG.plot_regions(clf, X_train[cols], y_train)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n\n\n\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', \n        'Clutch Completion_No', 'Clutch Completion_Yes'] \nclf = DecisionTreeClassifier(random_state=0)\nclf = clf.fit(X_train[cols], y_train)\nclf.score(X_train[cols], y_train)\nprint(f\"The clf.score() on testing data is: {clf.score(X_test[cols], y_test)}\")\ncross_v_score = cross_val_score(clf, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(clf, X_test[cols], y_test)\n# PG.plot_regions(clf, X_train[cols], y_train)\n\nThe clf.score() on testing data is: 0.9411764705882353\nthe cross validation scores are: [0.83333333 0.91666667 1.         0.72727273 0.90909091 0.72727273]\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n\n\n\n\n\nThis looks more like it! With this column conbination, our classifier is doing great at telling apart the penguins!\n\n\nBonus: personally hand-picked columns with Logistic Regression from sklearn\nAfter performing the exploration during the first half of this blog post, we could develop a rough idea about which variables could “contribute more” to the model for the goal of classification. Hence, based on previous analysis, I think the Culmen Depth of penguins, measured in mm, is quite central to guessing which species is the penguin under question.\nAlso, we have seen that different species of penguins also tend to have different Body Mass. Island that the penguin lives on is also another telling factor.\n\n# Hand picked columns based on the exploration we did before\ncols = ['Culmen Depth (mm)', 'Body Mass (g)',\n        'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test) \n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.7941176470588235\n\n\n\nprint(f\"The LR.score() on testing data is: {LR.score(X_test[cols], y_test)}\")\n\nThe LR.score() on testing data is: 0.7941176470588235\n\n\n\ncross_v_score = cross_val_score(LR, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \n\nthe cross validation scores are: [0.83333333 0.83333333 0.81818182 0.81818182 0.72727273 0.81818182]\n\n\nThe cross validation scores are not super high compared to other models we have seen in this blog, but the picture below looks good! The model did a especially good job at telling apart Adelie and Gentoo, and a less-good job at telling apart Gentoo and Chinstrap.\n\n# plt.rcParams[\"figure.figsize\"] = (4,4)\nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(LR, X_test[cols], y_test)\n# PG.plot_regions(LR, X_train[cols], y_train)\n\n['Culmen Depth (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n\n\nSince we selected this highest scoring combination using logistic regression, we might as well to see how it performs exactly!\n\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)',\n        'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test) \ncross_v_score = cross_val_score(LR, X_test[cols], y_test, cv = 6)\nprint(f\"the cross validation scores are: {cross_v_score}\") \nprint(cols)\nplt.rcParams['figure.dpi'] = 156\nPG.plot_regions(LR, X_test[cols], y_test)\n\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/home/xianzhiwang/miniforge3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nthe cross validation scores are: [1.         1.         1.         1.         0.90909091 0.90909091]\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']"
  }
]