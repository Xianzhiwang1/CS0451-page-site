[
  {
    "objectID": "posts/my-blog-post-04-linear-regress/index.html",
    "href": "posts/my-blog-post-04-linear-regress/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nIn this blog post I am going to discuss Linear Regression.\nBefore we start the implementation, we first record the following code snippet that will help us to automatically load our source code when we are in the editing phase.\n\n%load_ext autoreload\n%autoreload 2\n\nFirst, let’s import some libraries, then we perform our fit_gradient and fit_analytic on the following simple data set with only one features to visualize our linear regression.\n\nimport numpy as np\nnp.random.seed(42)\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.dpi'] = 78 \nplt.rcParams['savefig.dpi'] = 156 \nplt.rcParams['figure.figsize'] = [3, 4]\nfrom linear_regression import LinearRegression \n\n\n\nFitting Linear Regression using gradient descent; a.k.a. fit_gradient\nIn fit_gradient, the key step is to compute the gradient using a descent algorithm so that we could solve the following problem: \\[ \\hat{w} = \\arg \\min_{w} L(w). \\] Equivalently, we could unpact this equation: \\[ \\hat{w} = \\sum_{i=1}^{n} \\ell(\\hat{y}_i, y_i) = \\argmin_{w} \\sum_{i=1}^{n} ( \\langle w, x_i \\rangle - y_i)^2.\\] Recall that our loss function is of the form $ (, y) = (-y)^2 $ since we are using ordinary least square regression.\nWe start by taking derivative with respect to \\(w.\\) Using chain rule for matrices, we obtain the following expression: \\[ \\nabla L(w) = 2 X^{T}(X\\cdot w -y).\\] Then, we use gradient descent to find the \\(w\\) that is “good enough.” We achieve this by the following iteration: \\[ w^{(t+1)} \\leftarrow w^{(t)} - 2 \\cdot \\alpha \\cdot X^{T} (X \\cdot w^{(t)} - y).\\]\n\n# We start by generate a small data set.\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nWe are able to generate data and visualize this problem when p_features = 1. Graphically, we are trying to draw a line “of best fit” through the data points in the sense of OLS, which stands for Ordinary Least Squares. The line we draw just means given the feature x, we find the corresponding predicted y using the line, which will be close to the original y, if we have done a good job.\n\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y)\n\nAfter importing linear_regression.py, we could call the fit_gradient method that implements the gradient descent algorithm for us, as illustrated in the above cell. In the following cell, we plot the “line of best fit” using the weights LR1.w that we obtained after running fit_gradient.\n\nplt.scatter(x, y)\nplt.plot(x, X_@LR1.w, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\n\n\nFitting Linear Regression using a analytic formula; a.k.a. fit_analytic\nSimilarly to fit_gradient, we also have a method called fit_analytic, which uses a formula to compute the weights w exactly, and this is implemented using the followiing equation: \\[ \\hat{w} = (X^T X)^{-1} X^T y, \\] where \\(\\hat{w}\\) denotes the weights we obtained after calling the function fit_analytic. Note that in order for this formula to make sense, we need X to be a invertible matrix. Now, with the math part out of the way, let’s see this in action using the following code:\n\nLR = LinearRegression()\nX_ = LR.pad(x)\nLR.fit_analytic(X_,y)\nplt.scatter(x, y)\nplt.plot(x, X_@LR.w, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\n\n\nMore than one feature\nNow we use the following function to create both testing and validation data. At this stage, we could experiment with more features.\nWhen the number of features is one, p_features = 1, we could plot the artificial training data set and the validation data set. We lose this luxury when we have 2 or more features.\n\nn_train = 100\nn_val = 100\np_features = 1 \nnoise = 0.2\n\n# create some data\nLR = LinearRegression()\nX_train, y_train, X_val, y_val = LR.LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow we experiment with the number of features being n_train - 1, which quite a lot features.\n\n\nn_train = 100\nn_val = 100\np_features = n_train - 1 \nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR.LR_data(n_train, n_val, p_features, noise)\n\n[0.63670274 0.80630268 0.35513237 0.91438401 1.16299646 0.25471538\n 0.41929398 0.33128811 0.20562377 1.01101763 1.03533991 0.52161055\n 1.06408515 0.97373593 0.90195551 0.66708621 1.0553234  0.9038803\n 0.61806445 0.98863527 0.28500621 0.71033293 0.3895933  1.16001778\n 0.40914841 0.56370937 1.11308954 0.7595861  0.25039412 0.32132671\n 0.37317078 0.34757871 0.35170662 0.21077583 1.07547584 1.13214022\n 0.24784227 0.50182363 0.50983977 1.10205703 0.44692467 0.34827051\n 0.50807989 0.57485158 1.11593703 0.30167099 1.03100803 0.43282352\n 0.97354337 0.82979848 0.22099813 0.44731208 0.81564771 0.59497909\n 0.29111052 0.53750411 0.79428553 0.58251937 1.15713839 0.31408165\n 0.59708775 0.81603557 0.9006841  1.06787209 0.8839062  0.48405233\n 0.98376161 1.12966603 1.16950196 0.73324542 0.49236504 0.34219561\n 0.40279833 0.3268641  0.55424444 0.69369508 0.64131131 0.37501194\n 0.52423439 0.78499193 0.34587613 0.82398686 0.88436386 1.0896536\n 0.92725116 0.54739758 0.80205799 1.12189711 0.32002073 1.0173303\n 0.97576977 0.55629244 0.55567143 0.71001839 0.98502912 0.55223488\n 0.9632969  0.97102891 0.70019774 0.76183673]\n\n\nHere’s the snippets within the fit_gradient function that makes the same code work for different number of features:\nfeatures = X_.shape[1]\nself.w = np.random.rand(features)\n\nfrom linear_regression import LinearRegression \nLR = LinearRegression()\nX_train_ = LR.pad(X_train)\nX_val_ = LR.pad(X_val)\nLR.fit_analytic(X_train_, y_train) # I used the analytical formula as my default fit method\nprint(f\"Training score = {LR.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val_, y_val).round(4)}\")\n\nTraining score = 1.0\nValidation score = -28.9927\n\n\n\n# The estimated weight vector w is\nprint(LR.w)\nprint(f\"Training Loss = {LR.Big_L(X_train_, y_train).round(2)}\")\nprint(f\"Validation Loss = {LR.Big_L(X_val_, y_val).round(2)}\")\n\n[ -6.98654837  -5.11475728   8.57443208   5.07362523  -4.87413305\n   1.56026393   0.72491913   6.30882348   2.53666501  -4.92871486\n  -0.34034587   1.59120555  -2.528114    -2.78795259   5.15328034\n   4.27929691   4.4974397    0.49911716   5.42071211   3.08095628\n   0.73021938   3.88593999  -1.4181669    9.13986037   9.40915052\n  -0.31260307  -2.27939577  -2.28677311   2.2816238    0.13271794\n  -6.15093261  -2.76268557   0.66617876  -1.72896585  -3.63483613\n   1.79880844   0.06002997   1.63561401   3.63579881   2.83111064\n  -0.01094197   2.07372372   1.00556763  -7.67616558   7.44297791\n   5.04437298   4.30471381   0.77607148   4.81262528   4.81503507\n   0.21150729   1.84560229  -2.0094033   10.22625218   6.35957177\n   2.24820699  -3.27148277   4.4020253   -3.85757669  -1.47929074\n  -1.47511316   3.41494346   1.51279448 -10.41771156   1.96379535\n  -1.16326796  -3.74895009   1.55522698   1.72993542  -3.15904913\n  -2.15681793  -2.46951596  -3.10286333  -2.54008191  10.34623135\n   8.09693242   2.4651751   -0.81581193   0.91847796  -1.97853034\n   1.02250488  -8.2642525    4.05232048  -4.72529527   8.2724321\n   8.59203294   7.42419234  -3.79828989   3.81754082  -3.36718803\n  -1.58881632   4.23229791   1.07026613  -4.52381751  -0.6137537\n   3.19538746  -5.06610057   2.97739053   3.43978605 -10.01714519]\nTraining Loss = 0.0\nValidation Loss = 127.55\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train_, y_train)\nprint(LR2.w)\nprint(f\"Training score = {LR2.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val_, y_val).round(4)}\")\n\n# plt.plot(LR2.score_history)\n# labels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].plot(LR2.score_history)\naxarr[1].plot(LR2.loss_history)\nlabs = axarr[0].set(title = \"Score History\", xlabel = \"Iteration\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Loss History\", xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.tight_layout()\n\n[-0.20674413  0.12084117  0.57888575  0.10695928  0.3733441   1.53368573\n  0.32909533  2.19548408  0.35743443  0.84050323 -0.35876475  0.02140112\n  0.82346882  0.35922347  0.82526183  1.52828547  0.89941016 -0.50593451\n  0.15769917  0.17178843  1.04108492  0.22669827 -0.36448125  1.15371632\n  1.06980962  0.14919761  0.1489406   1.26635475  0.57680149  0.86882031\n  1.92894308  0.15563199  1.27118457  1.15114506  0.37997635  0.24154142\n  0.34591701  2.02346507  1.19253636  1.54006147  0.7682607   0.10483456\n  1.22761859  0.89395795  0.08145398 -0.2297801   1.18426365 -0.08806117\n  1.68563933  1.60796234 -0.3256804  -0.11730305  0.41853193  1.5334774\n -1.01472514  0.32646328 -0.14053296  1.00740866  1.04385105  0.98986568\n  0.05197086 -0.16415203  0.77044747  0.26846784  0.82958642  0.59342176\n  0.2997246  -0.181763    1.20827012  0.58855747 -0.13635831  1.0821288\n  0.55542882  0.9743686   0.83504526 -0.01698154  0.64567474  1.09707975\n  1.38820026  0.61133771  0.34974516  1.17627146  0.64519163  0.50528819\n  1.2513157   0.69183193  1.86799821  0.77918796  0.08140117  0.95929145\n  0.08892961  0.27624929  1.13100438  1.60412538  0.47177505 -0.40085373\n  0.51731535  2.13281993  0.33661442  1.73720408]\nTraining score = 0.5443\nValidation score = -2.2102\n\n\n\n\n\n\n\nLASSO Regularization\nIn this last section, we use a modified loss function of the following expression: \\[ L(w) = \\lVert X \\cdot w -y \\rVert ^2_2 + \\sum_{j=1}^{p-1} \\alpha \\cdot | w_j | \\]\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\n\nn_train = 30 \nn_val = 30\np_features = 1 \nnoise = 0.2\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR.LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\nL.score(X_val, y_val)\n\n0.557327771070066\n\n\n\nLR4 = LinearRegression()\nLR4.lasso_score(n_train, n_val, noise)\nLR4.lin_regress_score(n_train, n_val, noise)\n\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.416e-02, tolerance: 3.952e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.083e-02, tolerance: 5.877e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.936e-02, tolerance: 5.085e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.962e-02, tolerance: 5.838e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.074e-02, tolerance: 5.375e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.018e-02, tolerance: 5.204e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.358e-01, tolerance: 8.331e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.265e-02, tolerance: 6.840e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.160e-01, tolerance: 8.020e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\nKeyboardInterrupt: \n\n\n\nfig, axarr = plt.subplots(1, 2, sharex = False, sharey = False)\naxarr[0].plot(LR4.lasso_score_history)\naxarr[1].plot(LR4.fit_gradient_score_history)\nlabs = axarr[0].set(title = \"LASSO Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Linear regression Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nplt.tight_layout()\n\n\n\n\n\nprint(LR4.lasso_score_history)\nprint(LR4.fit_gradient_score_history)\n\n[0.6620134086527454, 0.7926652719544164, 0.7636216364918889, 0.7909619588951335, 0.8645992981586434, 0.8035728463544629, 0.8876763854143038, 0.8825528989471548, 0.8999645997553866, 0.9204144321545072, 0.9036502575751295, 0.9238042101891167, 0.9130491832790616, 0.9564958075480152, 0.9123523996274471, 0.9481736724093577, 0.9408155633323741, 0.9531214026635366, 0.9565368685279597, 0.9207839048724966, 0.9408003015258696, 0.9392737935629251, 0.946157051544213, 0.956997646406508, 0.9421334731115686, 0.9597226602720572, 0.9463221520441731, 0.9538853236888134, 0.9680544210629696, 0.9624507331053962, 0.9666915821265389, 0.9572390152046465, 0.9486495579876864, 0.9676815675127243, 0.9524032529564634, 0.9675283275131255, 0.9472936616302995, 0.970347778989878, 0.9665539116544122, 0.9738129521335204, 0.9555171068252105, 0.9612870258755014, 0.9671522961768058, 0.9659453362023974, 0.945242931456526, 0.8923291792428418, 0.968511774255436, 0.9803940514695835, 0.9532468095489877, 0.9279866466777582, 0.9756748126605774, 0.9580956960799869, 0.9671130026776822, 0.9659167437143412, 0.972199368588313, 0.9655237631740734, 0.9777107989445485, 0.939582636050352, 0.9628917193706108, 0.9555918611832371, 0.9653594464973367, 0.9368380439800654, 0.9493618553592368, 0.9642932042674554, 0.9731180682164032, 0.9597145953032044, 0.9626469367253617, 0.9449114006158158, 0.9465105575413985, 0.9286954644985084, 0.9404265023540571, 0.9527045186691424, 0.9254418882208905, 0.916390283956204, 0.9725195770294937, 0.9613623768600648, 0.936822976733226, 0.9494894512649069, 0.91851813304406, 0.9373279771194486, 0.9271088767614978, 0.9232823603751628, 0.9578438671593265, 0.9631761278031633, 0.847254419173977, 0.890709938607362, 0.9123779677748304, 0.9371227104511238, 0.7758552317935522, 0.8492576877283446, 0.7532906138001103, 0.7307697614489437, 0.6901337517790225, 0.7643319710264794, 0.834985390762478, 0.7863595972193863, 0.6805404357370728, 0.7685003827789387, 0.8191368447201366, 0.8907633169113939, 0.7775398701688743, 0.7774518524409693, 0.591737349413717, 0.7609165316722868, 0.7695149522844223, 0.6306362529649474, 0.47172760507303324, 0.6073980100796071, 0.7290145643482713, 0.7951102890286174, 0.6384081372534829, 0.7202307341791766, 0.6466076072658353, 0.6317188763137664, 0.4993856623005043, 0.4728207467368597, 0.5857548529626517, 0.4969374491478511, 0.7203254279648479, 0.45870768539600415, 0.5941603678664984, 0.5717429286347755, 0.5318891568177826, 0.4244814354489278, 0.6517289032444603, 0.5058336149374157, 0.5874799181033838, 0.4059717459780787, 0.5447854882863031, 0.30762060992211515, 0.21928544408566464, 0.39789691834735885, 0.411548461984635, 0.2548094462297046, 0.5575117793968875, 0.3933449432426236, 0.3926270588447518, 0.09873090908764459, 0.3821478149081805, 0.4640049165204918, 0.25979735931998216]\n[-0.8249617908118685, 0.8121432205380077, 0.4639197616445587, 0.36531810260796205, 0.6617616804177566, 0.7729015288733523, 0.7361030432966176, 0.8555200641823753, 0.9045627506203497, 0.8284163485104935, 0.8541791435457786, 0.911447944171333, 0.9215357108555404, 0.8667100960202782, 0.9119013163057852, 0.8847459864105149, 0.8483191440470397, 0.903013626397807, 0.9383893723783573, 0.8667160057400336, 0.9528839210881687, 0.9047967398946285, 0.9420396841451046, 0.9057525293628661, 0.9038854988476893, 0.943686262613224, 0.8765514949474006, 0.9535605712193543, 0.9342807667989528, 0.9428033432347753, 0.9470360188346756, 0.8621041736116027, 0.9434064427812061, 0.9466002252347184, 0.9569869552589783, 0.8978494097791345, 0.9182118929044777, 0.9046343476447372, 0.9304381146955548, 0.8944719454830504, 0.9479359752444244, 0.9320961999145093, 0.9481595010301163, 0.926002155617684, 0.9300137189892773, 0.9349528805100566, 0.9205291720605397, 0.912497844855618, 0.9517022815527717, 0.9636762826285734, 0.9399693310316238, 0.9237235111024888, 0.8960262898168136, 0.9138858091514634, 0.8936177825726954, 0.9399443612622592, 0.9392554866835118, 0.9305235179922433, 0.947957120295502, 0.9390288559927166, 0.9125582845223968, 0.9124849338102969, 0.9599514785517649, 0.9203624631280505, 0.8852503952860189, 0.9240596590416974, 0.9569672185938854, 0.9067830750591351, 0.8817457950416756, 0.9463673740193816, 0.8944389490948083, 0.9250141383260846, 0.8827914285646317, 0.9268852044225158, 0.935221607147371, 0.9151770680566815, 0.9361822524115295, 0.9248858182294418, 0.8566830985903884, 0.9049300919450286, 0.8931381492296803, 0.9193676339468935, 0.9394313323992707, 0.8834254680571255, 0.8943931203691251, 0.9031269244676963, 0.9286902372165995, 0.8966371764073809, 0.927615513613402, 0.9076991520331388, 0.9321716948039015, 0.9108336900768844, 0.8846584144670369, 0.9325712472464127, 0.8673002727136925, 0.9297756000050073, 0.8995589719262822, 0.8615778944708705, 0.8592620486876347, 0.908066081487866, 0.8998773088372259, 0.8970108969997423, 0.8820871729943264, 0.8901255227582149, 0.8671497448064913, 0.8817337838178306, 0.9292240909628517, 0.8797672629787175, 0.9002705737760063, 0.8689735106087924, 0.9007801999522664]"
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Intro under Construction.\nIn this blog post I am going to discuss kernel logistic regression for binary classification.\n\n\nImplementation\n\n\nUnder Construction.\n\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries.\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.01)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nXX = np.array([\n[-100.84739307, 100.71154296],\n [ 111.46814927, -0.28580296],\n [ -100.5192833,   -100.94984582],\n [ 10.73327397,  10.17310931],\n [ 0.33197143,  0.43375035],\n [ 1.62726102, -0.54736954]\n])\nyy = np.array([0, 0, 1, 0, 1, 1])\nprint(yy)\nplt.scatter(XX[:,0], XX[:,1], c = yy)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n[0 0 1 0 1 1]\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n# LR = LogisticRegression()\n# LR.fit(X, y)\n# plot_decision_regions(X, y, clf = LR)\n# title = plt.gca().set(title = f\"Accuracy = {(LR.predict(X) == y).mean()}\",\n#                       xlabel = \"Feature 1\", \n#                       ylabel = \"Feature 2\")\n\nWe work on nonlinear patterns\n\n\\[ \\hat{w} = \\arg \\min_{w} L(w) \\]\n\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom kernel_logistic import KLR \n\n\nKLR = KLR(rbf_kernel )\nKLR.fit(XX, yy)\n\n\n\nprint(XX)\n\n[[-100.84739307  100.71154296]\n [ 111.46814927   -0.28580296]\n [-100.5192833  -100.94984582]\n [  10.73327397   10.17310931]\n [   0.33197143    0.43375035]\n [   1.62726102   -0.54736954]]\n\n\n\n\n\nunder construction\n\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nprint(KLR.predict(XX))\nprint(\"OMG\")\nprint(yy)\n\n[0 0 1 0 1 1]\nOMG\n[0 0 1 0 1 1]\n\n\n\n\ntry on a bigger example\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(123)\nnp.seterr(all=\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nX, y = make_moons(80, shuffle = True, noise = 0.3)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nKLR.fit(X, y)\n\n\nplot_decision_regions(X, y, clf = KLR)\nyourpredict = KLR.predict(X)\ntitle = plt.gca().set(title = f\"Accuracy = {(yourpredict == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\nprint(KLR.predict(X))\nprint(\"OMG\")\nprint(y)\n\n[0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1\n 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1\n 0 1 1 1 1 0]\nOMG\n[1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1\n 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1\n 0 1 1 1 0 1]\n\n\n\n\n\n\nprint(KLR.v)\n\n[ -622.23496279  1335.13521946   681.20950851 -1284.34485486\n  -525.72085209   333.19420805]"
  },
  {
    "objectID": "posts/my-blog-post-01/index.html",
    "href": "posts/my-blog-post-01/index.html",
    "title": "My first Blog post on Perceptron",
    "section": "",
    "text": "Introduction.\nIn this blog post, we implement the perceptron algorithm, which is oftentimes the first machine learning algorithm a student encounters in a machine learning class (which is at least true in my case). We write code in Python for this implementation, and our goal is to classify binary labeled artificial data.\n\n\nImplementation\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries that we need.\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nIn the following code cell, we will generate a linearly separable dataset of binary-labeled 2D points. The make_blobs function essentially takes \\(n\\) samples, a number of features, and classes, and spits out a dataset of points with a data set with the given size, and label the data points using the classes. Visually, we see two clusters of points of two different color. In this special case where I set the seed, those two clusters seems linearly separable, which just means we could draw a straight line that completely seprates them. If we go to higher dimensions, then we need precise mathematical definitions, but we don’t need to worry about that right now.\n\nnp.random.seed(42)\nn=100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features=p_features-1, centers=[ (-1.7,-1.7, -1.7), (1.7,1.7, 1.7) ])\n\nfig=plt.scatter(X[:,0], X[:,1], c=y)\nxlab=plt.xlabel(\"feature 1\")\nylab=plt.ylabel(\"feature 2\")\n\n\n\n\n\n\nThe Perceptron Algorithm\nOur goal is to find the separating line using the perceptron algorithm. The algorithm takes in our feature matrix X and our vector of labels y. As detailed in the source code (link at the start of the blog), the algorithm performs the following steps: * Initialize the weights vector w * Iterate through the data points (randomly), updating the weights w until either a user-specified maximum number of iteration is reached. * record the accuracy score in self.history.\nState in mathematical terms, we would like to apply the perceptron algorithm to find the hyperplane that separates those data points, given that they are separable (so perceptron algorithm will converge). A key equation in the perceptron algorithm that defines the update is the following:\n\n\n\\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\]\nAnd this will provide us with the step to update w in each iteration.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, maxiter=10000)\n\nprint(p.history[-10:])\nprint(p.w_)\n# print(X)\n\n# print(w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n[ 50.44366343  39.1028252   21.79442178 -40.        ]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(p.score(X,y))\n\n1.0\n\n\n\nprint((1<2)*2)\nprint((1>2)*2)\n\n2\n0\n\n\n\nprint(p.predict(X))\nprint(\"\\n\\n\")\nprint(y)\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\nprint(p.w_)\nprint(np.size(p.w_))\n\n[39.71808898 55.01564117 -7.        ]\n3\n\n\n\n\nTime complexity for update step\nRecall our equation for the update step: \\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\] This involves taking an inner product $ ^{(t)}, _i , $ which has time complexity \\(O(p)\\) where \\(p\\) is a constant denoting the number of features. The other operations are addition, multiplication, taking the simple (step) function, which have constant time complexity \\(O(1)\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/my-blog-post-02-gradient-descent/index.html",
    "href": "posts/my-blog-post-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "do the gradient descent\n\nloss = logis.empirical_risk(X_, y, logis.logistic_loss, LR.w_)\nprint(loss)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n0.2578411254252761\n\n\n\n\n\n\nmyScore = LR.score(X_,y)\nprint(myScore)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Score = {myScore}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.score_history)\n\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Score History\")\nplt.tight_layout()\n\n0.7421588745747238\n\n\n\n\n\n\nprint(y)\nprint(LR.predict(X_))\n\n[0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1\n 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1\n 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1]\n[0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1\n 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1\n 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1\n 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1\n 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1]\n\n\n\n# print((1<0)*1)\n\n\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n\nIllustration\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.05) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.05)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Flabbergasted CSCI 0451 Blog",
    "section": "",
    "text": "My blog post on Linear Regression\n\n\n\n\n\n\nMar 20, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Kernel Logistic Regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimplementing gradient descent\n\n\n\n\n\n\nFeb 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy first Blog post on Perceptron\n\n\n\n\n\n\nFeb 15, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Xianzhi Wang’s Flabbergasted Blog for CS0451 Machine Learning class."
  }
]