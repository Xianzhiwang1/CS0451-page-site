[
  {
    "objectID": "posts/my-blog-post-04-linear-regress/index.html",
    "href": "posts/my-blog-post-04-linear-regress/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Intro under Construction.\nIn this blog post I am going to discuss kernel logistic regression for binary classification.\n\n\nImplementation\n\n\nUnder Construction.\n\n\n%load_ext autoreload\n%autoreload 2\n\nFirst, let’s import some libraries.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n        print(w)\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\n# axarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\n# labs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n[0.49052914 0.75551445]\n\n\n\n\n\nHere’s some math equations that makes all this work:\n\n\\[ \\hat{w} = \\arg \\min_{w} L(w) \\]\n\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\n\n# from sklearn.metrics.pairwise import rbf_kernel\nfrom linear_regression import LinearRegression \nLR = LinearRegression()\n# LR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n# print(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\n# print(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\n\nfrom linear_regression import LinearRegression \nLR = LinearRegression()\nX_ = LR.pad(x)\nLR.fit_gradient(X_,y)\n\nnew 25.2472427255503\nprev 25.2472427255503\n***\nnew 38.90755321504917\nprev 38.90755321504917\n***\nnew 59.999166005779045\nprev 59.999166005779045\n***\nnew 92.54698621365628\nprev 92.54698621365628\n***\nnew 142.7639134947236\nprev 142.7639134947236\n***\nnew 220.23651549196953\nprev 220.23651549196953\n***\nnew 339.754994173443\nprev 339.754994173443\n***\nnew 524.1366966636361\nprev 524.1366966636361\n***\nnew 808.5821551804639\nprev 808.5821551804639\n***\nnew 1247.395246180656\nprev 1247.395246180656\n***\nnew 1924.3504104044525\nprev 1924.3504104044525\n***\nnew 2968.6861482655963\nprev 2968.6861482655963\n***\nnew 4579.778128984155\nprev 4579.778128984155\n***\nnew 7065.202291385354\nprev 7065.202291385354\n***\nnew 10899.454618129786\nprev 10899.454618129786\n***\nnew 16814.53786146495\nprev 16814.53786146495\n***\nnew 25939.70926538657\nprev 25939.70926538657\n***\nnew 40017.0687273466\nprev 40017.0687273466\n***\nnew 61734.14565258266\nprev 61734.14565258266\n***\nnew 95236.97914670255\nprev 95236.97914670255\n***\nnew 146921.64444091456\nprev 146921.64444091456\n***\nnew 226655.33702422006\nprev 226655.33702422006\n***\nnew 349660.13345068326\nprev 349660.13345068326\n***\nnew 539419.0603688526\nprev 539419.0603688526\n***\nnew 832159.2736871026\nprev 832159.2736871026\n***\nnew 1283768.2382049405\nprev 1283768.2382049405\n***\nnew 1980463.2857386605\nprev 1980463.2857386605\n***\nnew 3055251.4927800125\nprev 3055251.4927800125\n***\nnew 4713322.257147215\nprev 4713322.257147215\n***\nnew 7271220.307793934\nprev 7271220.307793934\n***\nnew 11217277.724709583\nprev 11217277.724709583\n***\nnew 17304842.134736776\nprev 17304842.134736776\n***\nnew 26696099.415325377\nprev 26696099.415325377\n***\nnew 41183948.31018647\nprev 41183948.31018647\n***\nnew 63534285.366139464\nprev 63534285.366139464\n***\nnew 98014046.31201002\nprev 98014046.31201002\n***\nnew 151205813.03607067\nprev 151205813.03607067\n***\nnew 233264503.97852477\nprev 233264503.97852477\n***\nnew 359856064.55068576\nprev 359856064.55068576\n***\nnew 555148275.8209507\nprev 555148275.8209507\n***\nnew 856424661.1538373\nprev 856424661.1538373\n***\nnew 1321202338.506452\nprev 1321202338.506452\n***\nnew 2038212698.0359857\nprev 2038212698.0359857\n***\nnew 3144341242.3348846\nprev 3144341242.3348846\n***\nnew 4850760599.114629\nprev 4850760599.114629\n***\nnew 7483245798.2360735\nprev 7483245798.2360735\n***\nnew 11544368461.935413\nprev 11544368461.935413\n***\nnew 17809443492.60097\nprev 17809443492.60097\n***\nnew 27474545581.41958\nprev 27474545581.41958\n***\nnew 42384853587.31782\nprev 42384853587.31782\n***\nnew 65386916347.51856\nprev 65386916347.51856\n***\n[[-2.74999632e+09]\n [-5.10556418e+09]]\n\n\n\nprint(LR.history)\n\n[25.2472427255503, 25.2472427255503, 38.90755321504917, 38.90755321504917, 59.999166005779045, 59.999166005779045, 92.54698621365628, 92.54698621365628, 142.7639134947236, 142.7639134947236, 220.23651549196953, 220.23651549196953, 339.754994173443, 339.754994173443, 524.1366966636361, 524.1366966636361, 808.5821551804639, 808.5821551804639, 1247.395246180656, 1247.395246180656, 1924.3504104044525, 1924.3504104044525, 2968.6861482655963, 2968.6861482655963, 4579.778128984155, 4579.778128984155, 7065.202291385354, 7065.202291385354, 10899.454618129786, 10899.454618129786, 16814.53786146495, 16814.53786146495, 25939.70926538657, 25939.70926538657, 40017.0687273466, 40017.0687273466, 61734.14565258266, 61734.14565258266, 95236.97914670255, 95236.97914670255, 146921.64444091456, 146921.64444091456, 226655.33702422006, 226655.33702422006, 349660.13345068326, 349660.13345068326, 539419.0603688526, 539419.0603688526, 832159.2736871026, 832159.2736871026, 1283768.2382049405, 1283768.2382049405, 1980463.2857386605, 1980463.2857386605, 3055251.4927800125, 3055251.4927800125, 4713322.257147215, 4713322.257147215, 7271220.307793934, 7271220.307793934, 11217277.724709583, 11217277.724709583, 17304842.134736776, 17304842.134736776, 26696099.415325377, 26696099.415325377, 41183948.31018647, 41183948.31018647, 63534285.366139464, 63534285.366139464, 98014046.31201002, 98014046.31201002, 151205813.03607067, 151205813.03607067, 233264503.97852477, 233264503.97852477, 359856064.55068576, 359856064.55068576, 555148275.8209507, 555148275.8209507, 856424661.1538373, 856424661.1538373, 1321202338.506452, 1321202338.506452, 2038212698.0359857, 2038212698.0359857, 3144341242.3348846, 3144341242.3348846, 4850760599.114629, 4850760599.114629, 7483245798.2360735, 7483245798.2360735, 11544368461.935413, 11544368461.935413, 17809443492.60097, 17809443492.60097, 27474545581.41958, 27474545581.41958, 42384853587.31782, 42384853587.31782, 65386916347.51856, 65386916347.51856]\n\n\n\nplt.scatter(x, y)\nplt.plot(x, X@LR.w, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n# labels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)\n\n\n\n\n\n\n\n\nunder construction fit analytic\n\nX = LR.pad(x)\nLR.fit_analytic(x,y)\nplt.scatter(x, y)\nplt.plot(x, X@LR.w_analytic, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n# labels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nprint(LR.w_analytic)\n\n[[ 0.74529173]\n [-0.53399651]]"
  },
  {
    "objectID": "posts/my-blog-post-03-kernel/index.html",
    "href": "posts/my-blog-post-03-kernel/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Intro under Construction.\nIn this blog post I am going to discuss kernel logistic regression for binary classification.\n\n\nImplementation\n\n\nUnder Construction.\n\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries.\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.01)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nXX = np.array([\n[-100.84739307, 100.71154296],\n [ 111.46814927, -0.28580296],\n [ -100.5192833,   -100.94984582],\n [ 10.73327397,  10.17310931],\n [ 0.33197143,  0.43375035],\n [ 1.62726102, -0.54736954]\n])\nyy = np.array([0, 0, 1, 0, 1, 1])\nprint(yy)\nplt.scatter(XX[:,0], XX[:,1], c = yy)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n[0 0 1 0 1 1]\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n# LR = LogisticRegression()\n# LR.fit(X, y)\n# plot_decision_regions(X, y, clf = LR)\n# title = plt.gca().set(title = f\"Accuracy = {(LR.predict(X) == y).mean()}\",\n#                       xlabel = \"Feature 1\", \n#                       ylabel = \"Feature 2\")\n\nWe work on nonlinear patterns\n\n\\[ \\hat{w} = \\arg \\min_{w} L(w) \\]\n\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom kernel_logistic import KLR \n\n\nKLR = KLR(rbf_kernel )\nKLR.fit(XX, yy)\n\n\n\nprint(XX)\n\n[[-100.84739307  100.71154296]\n [ 111.46814927   -0.28580296]\n [-100.5192833  -100.94984582]\n [  10.73327397   10.17310931]\n [   0.33197143    0.43375035]\n [   1.62726102   -0.54736954]]\n\n\n\n\n\nunder construction\n\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nprint(KLR.predict(XX))\nprint(\"OMG\")\nprint(yy)\n\n[0 0 1 0 1 1]\nOMG\n[0 0 1 0 1 1]\n\n\n\n\ntry on a bigger example\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(123)\nnp.seterr(all=\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nX, y = make_moons(80, shuffle = True, noise = 0.3)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nKLR.fit(X, y)\n\n\nplot_decision_regions(X, y, clf = KLR)\nyourpredict = KLR.predict(X)\ntitle = plt.gca().set(title = f\"Accuracy = {(yourpredict == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\nprint(KLR.predict(X))\nprint(\"OMG\")\nprint(y)\n\n[0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1\n 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1\n 0 1 1 1 1 0]\nOMG\n[1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1\n 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1\n 0 1 1 1 0 1]\n\n\n\n\n\n\nprint(KLR.v)\n\n[ -622.23496279  1335.13521946   681.20950851 -1284.34485486\n  -525.72085209   333.19420805]"
  },
  {
    "objectID": "posts/my-blog-post-01/index.html",
    "href": "posts/my-blog-post-01/index.html",
    "title": "My first Blog post on Perceptron",
    "section": "",
    "text": "Under Construction.\n\n\nImplementation\n\n\nUnder Construction.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\nnp.random.seed(12345)\nn=100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features=p_features-1, centers=[ (-1.7,-1.7, -1.7), (1.7,1.7, 1.7) ])\n\n# fig=plt.scatter(X[:,0], X[:,1], c=y)\n# xlab=plt.xlabel(\"feature 1\")\n# ylab=plt.ylabel(\"feature 2\")\n\n\nWe would like to apply the perceptron algorithm to find the hyperplane that separates those data points, given that they are separable (so perceptron algorithm will converge). A key equation in the perceptron algorithm that defines the update is the following:\n\n\\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\]\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, maxiter=10000)\n\nprint(p.history[-10:])\nprint(p.w_)\n# print(X)\n\n# print(w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n[ 50.44366343  39.1028252   21.79442178 -40.        ]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(p.score(X,y))\n\n1.0\n\n\n\nprint((1<2)*2)\nprint((1>2)*2)\n\n2\n0\n\n\n\nprint(p.predict(X))\nprint(\"\\n\\n\")\nprint(y)\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\nprint(p.w_)\nprint(np.size(p.w_))\n\n[39.71808898 55.01564117 -7.        ]\n3\n\n\n\n\n\nunder construction"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/my-blog-post-02-gradient-descent/index.html",
    "href": "posts/my-blog-post-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "do the gradient descent\n\nloss = logis.empirical_risk(X_, y, logis.logistic_loss, LR.w_)\nprint(loss)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n0.2578411254252761\n\n\n\n\n\n\nmyScore = LR.score(X_,y)\nprint(myScore)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Score = {myScore}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.score_history)\n\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Score History\")\nplt.tight_layout()\n\n0.7421588745747238\n\n\n\n\n\n\nprint(y)\nprint(LR.predict(X_))\n\n[0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1\n 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1\n 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1]\n[0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1\n 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1\n 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1\n 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1\n 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1]\n\n\n\n# print((1<0)*1)\n\n\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n\nIllustration\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.05) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.05)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Flabbergasted CSCI 0451 Blog",
    "section": "",
    "text": "My blog post on Linear Regression\n\n\n\n\n\n\nMar 9, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Kernel Logistic Regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimplementing gradient descent\n\n\n\n\n\n\nFeb 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy first Blog post on Perceptron\n\n\n\n\n\n\nFeb 15, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Xianzhi Wang’s Flabbergasted Blog for CS0451 Machine Learning class."
  }
]