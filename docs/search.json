[
  {
    "objectID": "posts/my-blog-post-03-kernel/index.html",
    "href": "posts/my-blog-post-03-kernel/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Intro under Construction.\nIn this blog post I am going to discuss kernel logistic regression for binary classification.\n\n\nImplementation\n\n\nUnder Construction.\n\n\n%load_ext autoreload\n%autoreload 2\n\nFirst, let’s import some libraries.\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nLR = LogisticRegression()\n\nLR.fit(X, y)\n\nplot_decision_regions(X, y, clf = LR)\n\ntitle = plt.gca().set(title = f\"Accuracy = {(LR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nWe work on nonlinear patterns\n\n\\[ \\hat{w} = \\arg \\min_{w} L(w) \\]\n\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom kernel_logistic import KLR \n\n\n\n\n\nKLR = KLR(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\n\nOMG\nOMG\nOMG\n\n\n\n\nprint(KLR.v)\n\n[0.44590271 0.27362667 0.9971245  0.4261813  0.45138702 0.16362382\n 0.79480955 0.69368223 0.22076961 0.08238105 0.6804993  0.65451121\n 0.27325953 0.95086356 0.15105789 0.4323348  0.94361592 0.41972732\n 0.63852595 0.3975944  0.2742152  0.98397765 0.40933401 0.8940992\n 0.22995461 0.2131047  0.03113408 0.65166683 0.36852634 0.86435825\n 0.47320991 0.96819343 0.18552552 0.86862317 0.77659685 0.77092184\n 0.84478323 0.76102399 0.62622032 0.13124488 0.03252618 0.92084785\n 0.61665031 0.79653729 0.48152235 0.11730819 0.12518579 0.68556529\n 0.43030589 0.20052473 0.49159455 0.06420894 0.5819714  0.2689934\n 0.7975591  0.31036196 0.45522015 0.01162054 0.07244689 0.39249356\n 0.47993883 0.60002055 0.29166258 0.69498189 0.8601224  0.77985099\n 0.03961883 0.48050695 0.10493018 0.24204502 0.98666259 0.14249554\n 0.49888815 0.61815573 0.70246497 0.55964868 0.00977085 0.32646131\n 0.51771164 0.0878665  0.35062693 0.03320311 0.0785785  0.39692328\n 0.13271575 0.56754085 0.68946497 0.8005867  0.20015024 0.16748258\n 0.10456784 0.63643025 0.70647573 0.03158614 0.93621225 0.05197128\n 0.54129634 0.70906052 0.87096912 0.71408693 0.80172808 0.33945019\n 0.81482511 0.08011485 0.89481666 0.54759238 0.81729777 0.45231828\n 0.6435777  0.52640266 0.73158952 0.08162998 0.06035208 0.24710323\n 0.15954468 0.87178357 0.21921399 0.97586526 0.33689579 0.18211792\n 0.78969851 0.65870778 0.49819572 0.55536355 0.71920178 0.22845474\n 0.99633392 0.97479316 0.65032569 0.19954245 0.68022824 0.07219841\n 0.0306525  0.25768289 0.46262296 0.86827251 0.72716907 0.74270652\n 0.42549333 0.34593499 0.37103876 0.98764956 0.04010919 0.8670315\n 0.57867541 0.43861542 0.72525766 0.48666894 0.87342324 0.90070186\n 0.42172093 0.2768278  0.59235033 0.91236335 0.21066219 0.62296658\n 0.63156022 0.73311302 0.13156769 0.71582496 0.90903252 0.17968311\n 0.23754332 0.97139509 0.18097695 0.85438509 0.49227786 0.24723107\n 0.8707499  0.44530526 0.51481735 0.35923337 0.59295085 0.16352387\n 0.39108154 0.96941232 0.25813343 0.65673666 0.32519006 0.77347313\n 0.13087366 0.96982105 0.45378954 0.23605046 0.07349675 0.16975791\n 0.51977395 0.33700318 0.82888337 0.43088752 0.24871427 0.61714499\n 0.70677722 0.16704191 0.16761922 0.03667143 0.73640202 0.66380453\n 0.47463088 0.84417045]\n\n\n\n\n\nunder construction\n\nplot_decision_regions(X, y, clf = KLR)\nKLR.predict(X)\ntitle = plt.gca().set(title = f\"Accuracy = {(KLR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n/Users/xianzhiwang/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/mlxtend/plotting/decision_regions.py:269: UserWarning: No contour levels were found within the data range.\n  ax.contour(xx, yy, Z, cset.levels, **contour_kwargs)\n\n\n\n\n\n\nprint(KLR.predict(X))\nprint(\"OMG\")\nprint(y)\n\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\nOMG\n[1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1\n 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1\n 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1\n 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0\n 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0]"
  },
  {
    "objectID": "posts/my-blog-post-01/index.html",
    "href": "posts/my-blog-post-01/index.html",
    "title": "My first Blog post on Perceptron",
    "section": "",
    "text": "Under Construction.\n\n\nImplementation\n\n\nUnder Construction.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\nnp.random.seed(12345)\nn=100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features=p_features-1, centers=[ (-1.7,-1.7, -1.7), (1.7,1.7, 1.7) ])\n\n# fig=plt.scatter(X[:,0], X[:,1], c=y)\n# xlab=plt.xlabel(\"feature 1\")\n# ylab=plt.ylabel(\"feature 2\")\n\n\nWe would like to apply the perceptron algorithm to find the hyperplane that separates those data points, given that they are separable (so perceptron algorithm will converge). A key equation in the perceptron algorithm that defines the update is the following:\n\n\\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\]\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, maxiter=10000)\n\nprint(p.history[-10:])\nprint(p.w_)\n# print(X)\n\n# print(w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n[ 50.44366343  39.1028252   21.79442178 -40.        ]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(p.score(X,y))\n\n1.0\n\n\n\nprint((1<2)*2)\nprint((1>2)*2)\n\n2\n0\n\n\n\nprint(p.predict(X))\nprint(\"\\n\\n\")\nprint(y)\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\nprint(p.w_)\nprint(np.size(p.w_))\n\n[39.71808898 55.01564117 -7.        ]\n3\n\n\n\n\n\nunder construction"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/my-blog-post-02-gradient-descent/index.html",
    "href": "posts/my-blog-post-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "do the gradient descent\n\nloss = logis.empirical_risk(X_, y, logis.logistic_loss, LR.w_)\nprint(loss)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n0.2578411254252761\n\n\n\n\n\n\nmyScore = LR.score(X_,y)\nprint(myScore)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Score = {myScore}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.score_history)\n\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Score History\")\nplt.tight_layout()\n\n0.7421588745747238\n\n\n\n\n\n\nprint(y)\nprint(LR.predict(X_))\n\n[0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1\n 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1\n 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1]\n[0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1\n 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1\n 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1\n 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1\n 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1]\n\n\n\n# print((1<0)*1)\n\n\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n\nIllustration\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.05) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.05)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Flabbergasted CSCI 0451 Blog",
    "section": "",
    "text": "My blog post on Kernel Logistic Regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimplementing gradient descent\n\n\n\n\n\n\nFeb 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy first Blog post on Perceptron\n\n\n\n\n\n\nFeb 15, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Xianzhi Wang’s Flabbergasted Blog for CS0451 Machine Learning class."
  }
]