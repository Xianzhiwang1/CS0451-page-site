[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Xianzhi Wang’s Flabbergasted Blog for CS0451 Machine Learning class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Flabbergasted CSCI 0451 Blog",
    "section": "",
    "text": "My blog post on Palmer Penguins classification\n\n\n\n\n\n\nApr 22, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Image Compression with SVD and Spectral Community Detection\n\n\n\n\n\n\nApr 5, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy Blog post on Auditing Allocative Bias\n\n\n\n\n\n\nMar 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Linear Regression\n\n\n\n\n\n\nMar 20, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Kernel Logistic Regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimplementing gradient descent\n\n\n\n\n\n\nFeb 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy first Blog post on Perceptron\n\n\n\n\n\n\nFeb 15, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/my-blog-post-01/index.html",
    "href": "posts/my-blog-post-01/index.html",
    "title": "My first Blog post on Perceptron",
    "section": "",
    "text": "Introduction.\nIn this blog post, we implement the perceptron algorithm, which is oftentimes the first machine learning algorithm a student encounters in a machine learning class (which is at least true in my case). We write code in Python for this implementation, and our goal is to classify binary labeled artificial data.\n\n\nImplementation\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries that we need.\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nIn the following code cell, we will generate a linearly separable dataset of binary-labeled 2D points. The make_blobs function essentially takes \\(n\\) samples, a number of features, and classes, and spits out a dataset of points with a data set with the given size, and label the data points using the classes. Visually, we see two clusters of points of two different color. In this special case where I set the seed, those two clusters seems linearly separable, which just means we could draw a straight line that completely seprates them. If we go to higher dimensions, then we need precise mathematical definitions, but we don’t need to worry about that right now.\n\nnp.random.seed(42)\nn=100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features=p_features-1, centers=[ (-1.7,-1.7, -1.7), (1.7,1.7, 1.7) ])\n\nfig=plt.scatter(X[:,0], X[:,1], c=y)\nxlab=plt.xlabel(\"feature 1\")\nylab=plt.ylabel(\"feature 2\")\n\n\n\n\n\n\nThe Perceptron Algorithm\nOur goal is to find the separating line using the perceptron algorithm. The algorithm takes in our feature matrix X and our vector of labels y. As detailed in the source code (link at the start of the blog), the algorithm performs the following steps: * Initialize the weights vector w * Iterate through the data points (randomly), updating the weights w until either a user-specified maximum number of iteration is reached. * record the accuracy score in self.history.\nState in mathematical terms, we would like to apply the perceptron algorithm to find the hyperplane that separates those data points, given that they are separable (so perceptron algorithm will converge). A key equation in the perceptron algorithm that defines the update is the following:\n\n\n\\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\]\nAnd this will provide us with the step to update w in each iteration.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, maxiter=10000)\n\nprint(p.history[-10:])\nprint(p.w_)\n# print(X)\n\n# print(w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n[ 50.44366343  39.1028252   21.79442178 -40.        ]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(p.score(X,y))\n\n1.0\n\n\n\nprint((1<2)*2)\nprint((1>2)*2)\n\n2\n0\n\n\n\nprint(p.predict(X))\nprint(\"\\n\\n\")\nprint(y)\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\nprint(p.w_)\nprint(np.size(p.w_))\n\n[39.71808898 55.01564117 -7.        ]\n3\n\n\n\n\nTime complexity for update step\nRecall our equation for the update step: \\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\] This involves taking an inner product $ ^{(t)}, _i , $ which has time complexity \\(O(p)\\) where \\(p\\) is a constant denoting the number of features. The other operations are addition, multiplication, taking the simple (step) function, which have constant time complexity \\(O(1)\\)."
  },
  {
    "objectID": "posts/my-blog-post-02-gradient-descent/index.html",
    "href": "posts/my-blog-post-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Reference\nHere is a link to the main reference we are using when crafting this blog post.\n\n\nIntroduction\nLet’s recall what problem we are investigating. We are working on the empirical risk minimization problem, which involves finding a weight vector w, that satisfy the following general form: #### \\[ \\hat{w} = \\arg \\min_{w} L(w). \\]\nAlso, remember from our previous blog post that our data includes a feature matrix X, which is a \\(n\\times p\\) matrix with entries being real numbers. The number \\(n\\) represents the number of distinct observations, and we have \\(n\\) rows in X. \\(p\\) is the number of features. Our data also have a y, which is called target vector and lives in \\(\\mathbb{R}^n\\). The target vector gives a label for each observation.\nWe also need some formulas that’s computed using pen and paper by our friends in the math department. First, we remember this piece of notation $ f_w(x) = w, x $ and we could obtain the following: #### \\[ \\nabla L(w) = \\nabla ( \\frac{1}{n} \\sum_{i=1}^{n} \\ell [ f_w(x_i), y_i ] ). \\] And remember $ = w, x_i $ (another piece of notation!), the logistic loss we are using is #### \\[ \\ell(\\hat{y}, y) = -y \\log \\sigma (\\hat{y}) - (1-y) \\log(1-\\sigma(\\hat{y})), \\] where $ () $ denotes the logistical sigmoid function. as demonstrated in the link under the Reference heading above, we have #### \\[ \\frac{d \\ell(\\hat{y},y)}{d \\hat{y}} = \\sigma (\\hat{y}) -y. \\] Therefore, with some effort, one can do this computation and obtain the following formula: #### \\[ \\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^{n} (\\sigma(\\hat{y_i}) - y_i) x_i, \\] and this will help us to implement the gradient of the empirical risk for logistic regression in python code using numpy library.\n\n%load_ext autoreload\n%autoreload 2\n\nWe start by importing the relavant libraries and creating some data points using the make_blobs function that we imported from sklearn.datasets. We would like to create some non-separable data, which means graphically in 2 dimension, we cannot draw a straight line to separate the data points of the two different classes (as indicated by the color). Notice that the horizontal axis is Feature 1, and the vertical axis is Feature 2.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (18, 6)\n\nimport numpy as np\nnp.random.seed(42)\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAfter we generated the data, which includes a feature matrix X, which is a \\(n\\times p\\) matrix with entries being real numbers. The number \\(n\\) represents the number of distinct observations, and we have \\(n\\) rows in X. \\(p\\) is the number of features. Our data also have a y, which is called target vector and lives in \\(\\mathbb{R}^n\\). The target vector gives a label, value, or outcome for each observation. # Implementing Regular Gradient Descent We start by calling the function fit, which implements regular gradient descent, and we call it regular to distinguish it from Stochastic gradient descent, which we implement next. We are interested in finding the value for w_, and fit function uses the following key equation to iteratively update w_ until we have a “good enough” w_ or we reach the user-specified maximum number of iterations allowed, whichever comes first: #### \\[ w^{(t+1)} \\leftarrow w^{(t)} - \\alpha \\cdot \\nabla L(w^{(t)}), \\] where \\(\\nabla L(w)\\) is given by the following equation: ##### \\[ \\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell(f_{w}(x_i), y_i)\\]\n\nfrom solutions_logistic import LogisticRegression \nLR = LogisticRegression()\nX_ = LR.pad(X)\n\n# inspect the fitted value of w\nLR.fit(X, y, alpha = 0.01, max_epochs = 2000)\nprint(LR.w_)\n\n[ 1.59965852  1.45281308 -0.20047656]\n\n\nAfter calling the function fit, we obtain the weight vector w_, but are they doing what they are supposed to do? How big is the loss for this perticular case? We could visualize this result by plotting the line that hopefully separates the data points in a intuitive way. See the picture on the left. Now we would like to find out about how the empirical loss evolves as the number of iteration goes up. Let’s plot this in the picture on the right.\n\nnp.random.seed(42)\n# pick a random weight vector and calculate the loss\nw = .5 - np.random.rand(p_features)\n# fig = plt.scatter(X_[:,0], X_[:,1], c = y)\n# xlab = plt.xlabel(\"Feature 1\")\n# ylab = plt.ylabel(\"Feature 2\")\n# f1 = np.linspace(-3, 3, 101)\n# p = plt.plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n# title = plt.gca().set_title(f\"Loss = {LR.last_loss}\")\n\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\n\nAccuracy of regular gradint descent\nAgain, we draw the scatter plot and the fitted line on the left, and on the right, we plot the evolution of the accuracy score as the number of iteration increases.\n\nmyScore = LR.score(X_,y)\n\nfig, axarr = plt.subplots(1, 2)\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Score = {myScore}\")\nf1 = np.linspace(-3, 3, 101)\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\naxarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nWe could also print out the vector y and the predicted vector given by the function predict(). In this way, we could have a look “under the hood” and obtain a rough sense how good is our prediction.\n\nprint(y)\nprint(LR.predict(X_))\n\n[0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1\n 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0\n 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0\n 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0]\n[0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 0\n 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0\n 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0\n 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0]\n\n\n\n\nImplementing Stochastic Gradient Descent\nHere, by “Stochastic” we just mean we introduce a certain amount of randomness to our gradient descent step. The modification from the regular gradient descent is as follows. We pick a random subset \\(S \\subset [n]\\) and we let ##### \\[ \\nabla_S L(w) = \\frac{1}{|S|} \\sum_{i \\in S} \\nabla \\ell(f_{w}(x_i), y_i).\\] And the rest is business as usual. We deem our weights w as “good enough” when: either the user-specified maximum number of iteration is reached, or the current empirical risk function is “close enough” to the one from the previous iteration. With the mathematics technicality out of the way, let’s visualize the scatter plot, the best-fit-line, and the evolution of the empirical risk, and the evolution of the accuracy score all in one go.\n\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = 1) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 3)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\naxarr[2].plot(LR.score_history)\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\n\n\nIllustration\nHaving seen how regular gradient descent and stochastic gradient descent perform, we could add a momentum feature to the stochastic gradient descent. Then we have the choice of selecting momentum = True when we call the function fit_stochastic. Hence, we could compare the three versions of gradient descent and plot their respective empirical risk (loss) evolution in one picture, where the horizontal axis is number of iterations, and the vertical axis is empirical risk.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 100, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Here is a link to the source code for this Kernel Logistic Regression blog post."
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html#try-on-a-tiny-example",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html#try-on-a-tiny-example",
    "title": "Kernel Logistic Regression",
    "section": "Try on a tiny example",
    "text": "Try on a tiny example\nTo start us off, we manually create a tiny data set, and we call the fit function that we implemented in the source code (link at the start of the blog). This will help us to see clearly where our implementation works or not, and using a tiny test data set also helps for debugging purposes.\n\nXX = np.array([\n[-50.84739307, 50.71154296],\n [ 11.46814927, -9.28580296],\n [ -40.5192833,   -70.94984582],\n [ 10.73327397,  10.17310931],\n [ 10.33197143,  0.43375035],\n\n [ -1.62726102, -0.54736954],\n[-7.84739307, 5.71154296],\n [ -21.46814927, -19.28580296],\n [ -10.5192833,   -50.94984582],\n [ 7.73327397,  0.17310931],\n])\nyy = np.array([1, 1, 0, 0, 0, 0, 1, 0, 1, 1])\nprint(yy)\nplt.scatter(XX[:,0], XX[:,1], c = yy)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n[1 1 0 0 0 0 1 0 1 1]\n\n\n\n\n\nIt seems that those \\(10\\) data points does not display a linear pattern, which is what we want. Now we create an instance of the KLR class and test our fit function to see if it could classify according to nonlinear patterns.\n\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom kernel_logistic import KLR \nKLR = KLR(rbf_kernel )\nKLR.fit(XX, yy)\n\n\nprint(XX)\n\n[[-50.84739307  50.71154296]\n [ 11.46814927  -9.28580296]\n [-40.5192833  -70.94984582]\n [ 10.73327397  10.17310931]\n [ 10.33197143   0.43375035]\n [ -1.62726102  -0.54736954]\n [ -7.84739307   5.71154296]\n [-21.46814927 -19.28580296]\n [-10.5192833  -50.94984582]\n [  7.73327397   0.17310931]]\n\n\n\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nprint(KLR.predict(XX))\nprint(yy)\n\n[1 1 0 0 0 0 1 0 1 1]\n[1 1 0 0 0 0 1 0 1 1]"
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html#try-on-a-bigger-example",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html#try-on-a-bigger-example",
    "title": "Kernel Logistic Regression",
    "section": "Try on a bigger example",
    "text": "Try on a bigger example\n\n\nnp.random.seed(42)\nnp.seterr(all=\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nX, y = make_moons(80, shuffle = True, noise = 0.3)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nKLR.fit(X, y)\n\n\nplot_decision_regions(X, y, clf = KLR)\nyourpredict = KLR.predict(X)\ntitle = plt.gca().set(title = f\"Accuracy = {(yourpredict == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\nprint(KLR.predict(X))\nprint(\"OMG\")\nprint(y)\n\n[0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1\n 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0\n 1 0 0 1 0 0]\nOMG\n[0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1\n 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0\n 1 0 1 1 0 1]\n\n\n\n\n\n\nprint(KLR.v)\n\n[-1179.96550569 -2790.28343168 -1350.34402312 -1016.12721979\n  1317.09792385 -1810.11426626  2546.49702776 -1698.14631029\n -1811.95450782   -44.84604051 -1875.0500244  -2414.73171276\n   -83.16865128  2737.37445494  -511.39291408  -172.10314639\n  1841.53416798  -289.68517734   278.73676319  2314.88794609\n  2308.33636841 -2010.14390628    52.42682597  -197.05415376\n -1082.98594611 -2013.7726828  -1497.96098375   914.94570061\n  1977.66154624 -1608.7151421  -2103.83992013  -932.57756199\n -7436.76036863 -2198.75261428  2026.86616783  -111.48818587\n   688.7887425    655.92511244  2896.7561339   2584.46281901\n  3653.83460661   -25.48845316   748.56648432   411.28381018\n  1839.04630941  1188.73342418 -1059.77106821  2617.22188799\n  2692.81087034  5302.19223408 -1199.64601213  1180.74030999\n  2375.01140945  -912.06492168  2152.82530682 -3287.86563739\n -1511.48609759  1701.60310544   890.20852102 -6049.43487866\n   919.50031634  2040.3175221  -1453.43703857  2059.97544263\n -2653.45287258  -448.11834792  1556.05840695  1715.29365951\n -2402.68814431 -1838.28676088  3978.58062434  -848.76206469\n -3127.12581133   285.4039421    536.22820077  1495.01840979\n -3051.0533366   4639.95344253  1529.93340297 -2551.4328797 ]"
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html#lets-try-a-different-data-set",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html#lets-try-a-different-data-set",
    "title": "Kernel Logistic Regression",
    "section": "Let’s try a different data set",
    "text": "Let’s try a different data set\n\n\nX, y = make_moons(80, shuffle = True, noise = 0.1)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nKLR.fit(X, y)\n\n# plot the graph\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\nplot_decision_regions(X, y, clf = KLR)\nyourpredict = KLR.predict(X)\ntitle = plt.gca().set(title = f\"Accuracy = {(yourpredict == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\nprint(KLR.predict(X))\nprint(\"OMG\")\nprint(y)\n\n[0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1\n 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n 1 0 0 1 0 0]\nOMG\n[0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n 1 0 0 1 0 0]"
  },
  {
    "objectID": "posts/my-blog-post-04-linear-regress/index.html",
    "href": "posts/my-blog-post-04-linear-regress/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nIn this blog post I am going to discuss Linear Regression.\nBefore we start the implementation, we first record the following code snippet that will help us to automatically load our source code when we are in the editing phase.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries, then we perform our fit_gradient and fit_analytic on the following simple data set with only one features to visualize our linear regression.\n\nimport numpy as np\nnp.random.seed(42)\nfrom matplotlib import pyplot as plt\nimport matplotlib\nplt.rcParams[\"figure.figsize\"] = (6,2)\nplt.rcParams['figure.dpi'] = 78 \nplt.rcParams['savefig.dpi'] = 156 \nfrom linear_regression import LinearRegression \n\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\n\nFitting Linear Regression using gradient descent; a.k.a. fit_gradient\nIn fit_gradient, the key step is to compute the gradient using a descent algorithm so that we could solve the following problem: \\[ \\hat{w} = \\arg \\min_{w} L(w). \\] Equivalently, we could unpact this equation: \\[ \\hat{w} = \\sum_{i=1}^{n} \\ell(\\hat{y}_i, y_i) = \\argmin_{w} \\sum_{i=1}^{n} ( \\langle w, x_i \\rangle - y_i)^2.\\] Recall that our loss function is of the form $ (, y) = (-y)^2 $ since we are using ordinary least square regression.\nWe start by taking derivative with respect to \\(w.\\) Using chain rule for matrices, we obtain the following expression: \\[ \\nabla L(w) = 2 X^{T}(X\\cdot w -y).\\] Then, we use gradient descent to find the \\(w\\) that is “good enough.” We achieve this by the following iteration: \\[ w^{(t+1)} \\leftarrow w^{(t)} - 2 \\cdot \\alpha \\cdot X^{T} (X \\cdot w^{(t)} - y).\\]\nWe use the following code block to generate a small data set for testing our linear regression implementation.\n\n# We start by generate a small data set.\nw0 = -0.5\nw1 =  0.7\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.figure(figsize=(4,4))\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nWe are able to generate data and visualize this problem when p_features = 1. Graphically, we are trying to draw a line “of best fit” through the data points in the sense of OLS, which stands for Ordinary Least Squares. The line we draw just means given the feature x, we find the corresponding predicted y using the line, which will be close to the original y, if we have done a good job.\nAfter importing linear_regression.py, we could call the fit_gradient method that implements the gradient descent algorithm for us, as illustrated in the above cell. In the following cell, we plot the “line of best fit” using the weights LR1.w that we obtained after running fit_gradient. Also, we print out the weights vector w that we fited, which seems to be doing a good job judging from the pictures below, plotted together with fit_analytic.\n\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\nprint(LR1.w)\n\n[[ 0.62865065]\n [-0.46565231]]\n\n\n\n\nUsing LASSO\n\nL.fit(x,y)\nL.score(x,y)\nprint(\"*\")\nL_w = np.hstack([L.coef_, L.intercept_])\nprint(L_w)\n\n*\n[ 0.6426091  -0.47312394]\n\n\n\n\nFitting Linear Regression using a analytic formula; a.k.a. fit_analytic\nSimilarly to fit_gradient, we also have a method called fit_analytic, which uses a formula to compute the weights w exactly, and this is implemented using the followiing equation: \\[ \\hat{w} = (X^T X)^{-1} X^T y, \\] where \\(\\hat{w}\\) denotes the weights we obtained after calling the function fit_analytic. Note that in order for this formula to make sense, we need X to be a invertible matrix. Now, with the math part out of the way, let’s see this in action in the following block.\n\n\n# # plot it\n# LR1 = LinearRegression()\n# X_ = LR1.pad(x)\n# LR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\n\n# fig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n# axarr[0].scatter(x,y)\n# axarr[0].plot(x, X_@LR1.w, color = \"black\")\n\n# LR2 = LinearRegression()\n# X_ = LR2.pad(x)\n# LR.fit_analytic(X_,y)\n\n# axarr[1].scatter(x,y)\n# axarr[1].plot(x, X_@LR.w, color = \"black\")\n\n# labs = axarr[0].set(title=\"Best-fit-line by implementing gradient descent\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n# labs = axarr[1].set(title=\"Best-fit-line by implementing the analytic formula\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n# plt.tight_layout()\n\n\nmatplotlib.rc('font', size=6)\n# gradient\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\n\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\naxarr[0].scatter(x,y)\naxarr[0].plot(x, X_@LR1.w, color = \"black\")\n\n# Analytic \nLR2 = LinearRegression()\nX_ = LR2.pad(x)\nLR2.fit_analytic(X_,y)\n\naxarr[1].scatter(x,y)\naxarr[1].plot(x, X_@LR2.w, color = \"black\")\n\n# LASSO\naxarr[2].scatter(x,y)\naxarr[2].plot(x, X_@L_w, color = \"black\")\n\nlabs = axarr[0].set(title=\"Best-fit-line by implementing gradient descent\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[1].set(title=\"Best-fit-line by implementing the analytic formula\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[2].set(title=\"Best-fit-line by implementing LASSO\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\nplt.tight_layout()\n\n\n\n\n\n\nMore than one feature\nNow we use the following function to create both testing and validation data. At this stage, we could experiment with more features. We use the following code to create artificial data sets that has any number of features that we specify.\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n        # print(w)\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = LR.pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = LR.pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nWhen the number of features is one, p_features = 1, we could plot the artificial training data set and the validation data set. We lose this luxury when we have 2 or more features. Let’s plot it.\n\nn_train = 100\nn_val = 100\np_features = 1 \nnoise = 0.2\n\n# create some data\nLR = LinearRegression()\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow we experiment with the number of features being n_train - 1, which quite a lot features.\n\n\nn_train = 100\nn_val = 100\np_features = n_train - 1 \nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nHere’s the snippets within the fit_gradient function that makes the same code work for different number of features:\nfeatures = X_.shape[1]\nself.w = np.random.rand(features)\n\nfrom linear_regression import LinearRegression \nLR = LinearRegression()\nX_train_ = LR.pad(X_train)\nX_val_ = LR.pad(X_val)\nLR.fit_analytic(X_train_, y_train) # I used the analytical formula as my default fit method\nprint(f\"Training score = {LR.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val_, y_val).round(4)}\")\n\nTraining score = 0.01\nValidation score = -0.181\n\n\n\n# The estimated weight vector w is\nprint(LR.w)\nprint(f\"Training Loss = {LR.Big_L(X_train_, y_train).round(2)}\")\nprint(f\"Validation Loss = {LR.Big_L(X_val_, y_val).round(2)}\")\n\n[ -0.7845107    4.049007     2.68182817  -1.31911216  -2.39285946\n  -2.00219241  -0.32103052  -2.10906148  -0.08934903  -1.6195237\n  -4.40257633   6.84778921  10.54836286   1.02721444   1.91099441\n   1.94444705  -0.83956425   5.73672698  -1.69742181   8.11975219\n   0.04339881  -0.36221101   3.1120061    0.73878394  -4.07877963\n  -5.94430671  -2.51971207   2.85726601   2.76885314   0.0866742\n   2.86227124  -5.17200329   2.62611145  -2.08071332   4.86673258\n  -0.24489573   0.11580583  -2.32144177   2.49627168   1.74563486\n  -1.30731774   2.82037016   1.26051291   4.35286933   6.37892501\n   3.34078878   2.73431785  -4.86134304   3.14629798   0.6460801\n  -2.40697429   4.76225123  -2.21624246  -0.31357243  -1.86829949\n   1.42916226   0.04242573   2.22149979  -4.46148373  -0.30154216\n   2.19761681   6.99332214   2.81030572   3.48025664   0.5052169\n   3.05263047  -1.36039978   9.26417952  -8.75795323  -1.31272234\n   3.79085029   1.48871219   4.1549847    1.90384903   2.5648586\n  -0.41181245  -1.59049434   0.87541055   0.65089538   1.41283414\n   2.61994704   4.81443311   5.07854556   3.29959339   2.07762799\n   1.49678962   2.23728672   5.01518015   1.35015492   3.04524847\n   2.86229126   2.11538612   6.75562266  -1.07232827  -2.61438704\n  -1.90309452   1.15174284   2.48664442  -2.04358778 -18.94977335]\nTraining Loss = 0.0\nValidation Loss = 96.7\n\n\n\nLR5 = LinearRegression()\n\nLR5.fit_gradient(X_train_, y_train, 0.0001, 1000)\nprint(f\"Training score = {LR5.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR5.score(X_val_, y_val).round(4)}\")\n\n# plt.plot(LR2.score_history)\n# labels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].plot(LR5.score_history)\naxarr[1].plot(LR5.loss_history)\nlabs = axarr[0].set(title = \"Score History\", xlabel = \"Iteration\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Loss History\", xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.tight_layout()\n\nTraining score = -5.9295\nValidation score = -6.2086\n\n\n\n\n\n\nLASSO Regularization\nIn this last section, we use a modified loss function of the following expression: \\[ L(w) = \\lVert X \\cdot w -y \\rVert ^2_2 + \\sum_{j=1}^{p-1} \\alpha \\cdot | w_j | \\]\n\nL2 = Lasso(alpha = 0.01)\n\n\n\nn_train = 30 \nn_val = 30\np_features = 1 \nnoise = 0.2\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2.score(X_val, y_val)\n\n0.5877132844037414\n\n\n\nLR4 = LinearRegression()\nLR4.lasso_score(n_train, n_val, noise)\nLR4.lin_regress_score(n_train, n_val, noise)\nLR4.lin_regress_score_analytic(n_train, n_val, noise)\n\nTypeError: fit_gradient() missing 2 required positional arguments: 'alpha' and 'max_epochs'\n\n\n\n# from matplotlib.pyplot import figure\n# figure(figsize=(8, 6), dpi = 156)\nfig, axarr = plt.subplots(1, 3, sharex = False, sharey = False)\naxarr[0].plot(LR4.lasso_score_history)\naxarr[1].plot(LR4.fit_gradient_score_history)\naxarr[2].plot(LR4.fit_analytic_score_history)\nlabs = axarr[0].set(title = \"LASSO Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Linear regression fit_gradient Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[2].set(title = \"Linear regression fit_analytic Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nplt.tight_layout()\n\n\n\n\n\n# print(LR4.lasso_score_history)\n# print(\"***\")\n# print(LR4.fit_gradient_score_history)\n# print(\"***\")\n# print(LR4.fit_analytic_score_history)"
  },
  {
    "objectID": "posts/my-blog-post-05-audit-bias/index.html",
    "href": "posts/my-blog-post-05-audit-bias/index.html",
    "title": "My Blog post on Auditing Allocative Bias",
    "section": "",
    "text": "Reference\nHere is a link to the main guide and reference when we write this blog post.\nAnother reference is this paper that documents which variable means what in the PUMS data set we are going to use.\n\n\nIntroduction.\n\n\nImplementation\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries that we need.\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nIn the following code cell, we will generate a linearly separable dataset of binary-labeled 2D points. The make_blobs function essentially takes \\(n\\) samples, a number of features, and classes, and spits out a dataset of points with a data set with the given size, and label the data points using the classes. Visually, we see two clusters of points of two different color. In this special case where I set the seed, those two clusters seems linearly separable, which just means we could draw a straight line that completely seprates them. If we go to higher dimensions, then we need precise mathematical definitions, but we don’t need to worry about that right now.\n\n\nGetting the data using folktables\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\n\nSTATE = \"IN\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000042\n      3\n      1\n      2000\n      2\n      18\n      1013097\n      46\n      20\n      ...\n      7\n      6\n      82\n      44\n      6\n      76\n      83\n      44\n      45\n      44\n    \n    \n      1\n      P\n      2018GQ0000053\n      3\n      1\n      2306\n      2\n      18\n      1013097\n      19\n      48\n      ...\n      16\n      19\n      37\n      23\n      2\n      19\n      2\n      2\n      40\n      18\n    \n    \n      2\n      P\n      2018GQ0000074\n      3\n      1\n      2000\n      2\n      18\n      1013097\n      88\n      20\n      ...\n      166\n      158\n      160\n      90\n      87\n      84\n      88\n      90\n      13\n      166\n    \n    \n      3\n      P\n      2018GQ0000118\n      3\n      1\n      401\n      2\n      18\n      1013097\n      72\n      20\n      ...\n      11\n      10\n      11\n      11\n      71\n      11\n      70\n      11\n      74\n      135\n    \n    \n      4\n      P\n      2018GQ0000319\n      3\n      1\n      200\n      2\n      18\n      1013097\n      97\n      22\n      ...\n      15\n      170\n      93\n      181\n      175\n      92\n      174\n      16\n      96\n      95\n    \n  \n\n5 rows × 286 columns\n\n\n\n\n\nData wrangling, applying Logistic Regression\nWe recall the equation for a linear regression first: \\[ y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n, \\] where, \\(\\beta_i\\)’s are coefficients, \\(y\\) is the depedent variable, and the \\(X_i\\)’s are regressors (independent variables). Now, we recall logistic function (or sigmoid function), which is \\[ f(x) = \\frac{1}{1+e^{-x}}, \\] and when we put those two piece together, we obtain the formula for logistic regression: \\[ y = \\frac{1}{1+e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n}}, \\]\nHere are some of the variables that are important for our analysis: * PINCP is total personal income. * ESR is employment status coded as a dummy variable (1 if employed, 0 if not) * SEX is binary sex, coded 1 for male, and 2 for female. * RAC1P is race (1for White Alone, 2 for Black/African American alone, 3 and above for other self-identified racial groups) * DEAR, DEYE, and DERM refers to disability status relating to ear, eye, etc. * AGEP is Age, represented as integers. * SCHL is educational attainment, coded as integers. * MAR is Marital status, coded using integers. * RELP is Relationship. * COW is class of worker, coded using integers. * OCCP is occupation. * POBP is place of birth. * WKHP is usual hours worked per week in the past 12 months.\n\nLet’s consider the following task: we are going to\n\n\nTrain a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including race, and\n\n\nPerform a bias audit of our algorithm to determine whether it displays racial bias.\n\n\nFirst, let’s be more specific:\n\n\nmy_features=['PINCP', 'ESR', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P']\n# new_df = acs_data[my_features]\n# new_df['INCOME'] = np.where(new_df['PINCP'] >= 70000, 1, 0)\n\n\n# new_df.loc[new_df['PINCP'] >= 70000]\n# new_df.loc[new_df['ESR'] == 1]\n\n\nfeatures_to_use = [f for f in my_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nprint(features_to_use)\n\n['PINCP', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX']\n\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    # group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nWorking with pd.DataFrame, apply logistic regression in Python with scikit-learn.\n\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n# df.loc[df['group'] == 2]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      PINCP\n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      group\n      label\n    \n  \n  \n    \n      0\n      0.0\n      31.0\n      20.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n    \n    \n      1\n      32000.0\n      50.0\n      18.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      2\n      0.0\n      2.0\n      0.0\n      5.0\n      2.0\n      2.0\n      5.0\n      1.0\n      1.0\n      0.0\n      4.0\n      1.0\n      2.0\n      2.0\n      0.0\n      1.0\n      1\n      False\n    \n    \n      3\n      0.0\n      14.0\n      9.0\n      5.0\n      2.0\n      2.0\n      7.0\n      1.0\n      3.0\n      0.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n    \n    \n      4\n      38000.0\n      58.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n  \n\n\n\n\n\nBasic Discriptives\nUsing this data frame, we first answer the following questions:\n\nHow many individuals are in the data?\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\nOf these individuals, how many are in each of the groups?\nIn each group, what proportion of individuals have target label equal to 1?\nCheck for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex. This might be a good opportunity to use a visualization such as a bar chart, e.g. via the seaborn package.\n\n\nprint(f\"The Number of Rows is: {df.shape[0]}\")\nprint(f\"The Number of Columns is: {df.shape[1]}\")\nprint(f\"The Number of individual who are employed is: {df.loc[df['label'] == True].shape[0]}\")\nprint(f\"The Percentage of individuals who are employed is: {24858/54144}\")\n###\nprint(f\"The Number of person who identify as black is: {df.loc[df['group']==2].shape[0]}\")\nprint(f\"The Number of person who identify as white is: {df.loc[df['group']==1].shape[0]}\")\nprint(f\"The Number of person who identify as black and is currently employed: {df.loc[(df['group']==2) & (df['label']==True)].shape[0]}\")\nprint(f\"The Number of person who identify as white and is currently employed: {df.loc[(df['group']==1) & (df['label']==True)].shape[0]}\")\n###\nprint(f\"The Number of person who identify as other racial groups is: {df.loc[df['group']>= 3].shape[0]}\")\nprint(f\"The Number of person who identify as other racial groups and is currently employed: {df.loc[(df['group']>= 3) & (df['label']==True)].shape[0]}\")\n###\nprint(f\"The Percentage of person who identify as black and is also employed is: {1374/3626}\")\nprint(f\"The Percentage of person who identify as white and is also employed is: {22200/47332}\")\nprint(f\"The Percentage of person who identify as other racial groups and is also employed is: {1284/3186}\")\n\nThe Number of Rows is: 54144\nThe Number of Columns is: 18\nThe Number of individual who are employed is: 24858\nThe Percentage of individuals who are employed is: 0.4591090425531915\nThe Number of person who identify as black is: 3626\nThe Number of person who identify as white is: 47332\nThe Number of person who identify as black and is currently employed: 1374\nThe Number of person who identify as white and is currently employed: 22200\nThe Number of person who identify as other racial groups is: 3186\nThe Number of person who identify as other racial groups and is currently employed: 1284\nThe Percentage of person who identify as black and is also employed is: 0.3789299503585218\nThe Percentage of person who identify as white and is also employed is: 0.4690272965435646\nThe Percentage of person who identify as other racial groups and is also employed is: 0.4030131826741996\n\n\n\nprint( df.groupby(\"SEX\").size() )\n\nSEX\n1.0    26578\n2.0    27566\ndtype: int64\n\n\nWe observe that there are \\(26578\\) males and \\(27566\\) females in the data set. Using the .groupby function, we see that we can more efficiently obtain the information we needed than the methods demonstrated in the above cell blocks.\n\nprint( df.groupby(\"group\").size() )\n\ngroup\n1    47332\n2     3626\n3       83\n4        1\n5       29\n6      942\n7       17\n8      906\n9     1208\ndtype: int64\n\n\nSince group 1 denotes white individuals, and group 2 denotes black individuals, we could read off of the previous block of code that there are \\(47332\\) white individuals and \\(3636\\) black individuals in Indiana in 2018.\n\nprint(df.groupby(\"label\").size())\n\nlabel\nFalse    29286\nTrue     24858\ndtype: int64\n\n\nWe see that in Indiana in 2018, there’s \\(29286\\) persons who are unemployed, and there’s \\(24858\\) persons who are employed. Again, using .groupby is much more efficient than what we did previously. The total number of persons in the data set is \\(54144.\\) The (average) percentage of individuals who are employed is about \\(46\\) percent.\n\nprint(df.groupby(['group', 'label']).size())\nprint(\"the average employment rate (of all people in all groups, in Indiana, in 2018) is: \" , df[\"label\"].mean())\n\ngroup  label\n1      False    25132\n       True     22200\n2      False     2252\n       True      1374\n3      False       50\n       True        33\n4      True         1\n5      False       17\n       True        12\n6      False      492\n       True       450\n7      False        6\n       True        11\n8      False      516\n       True       390\n9      False      821\n       True       387\ndtype: int64\nthe average employment rate (of all people in all groups, in Indiana, in 2018) is:  0.4591090425531915\n\n\nWe see that in group 1, for white individuals in Indiana, in 2018, the number of unemployed individual is \\(25132\\), and the number of employed individual is \\(22200\\). Similarly, for black individuals in Indiana, in 2018, the number of unemployed individual is \\(2252\\), and the number of employed individual is \\(1374\\). However, it might be more clear to see the employment rate for better comparison. The following line of code shows the employment rate for each group. The employment rate for white individual in IN in 2018 is \\(47\\) percent. The employment for black individual in IN in 2018 is \\(37.9\\) percent. Hence, we see that the employment rate is higher for a white individual than a black individual statistically.\n\nprint( df.groupby(\"group\")[\"label\"].mean() )\n\ngroup\n1    0.469027\n2    0.378930\n3    0.397590\n4    1.000000\n5    0.413793\n6    0.477707\n7    0.647059\n8    0.430464\n9    0.320364\nName: label, dtype: float64\n\n\nMore efficiently, we could use the following line of code and read off the employment rate for persons based on their race and gender binary. We see that for people identifying as white male, the employment rate is \\(50.7\\) percent. For people identifying as white female, the employment rate is \\(43.2\\) percent. Similarly, we see that for people identifying as black male, the employment rate is \\(35.7\\) percent. For people identifying as black female, the employment rate is \\(40\\) percent.\n\ndf.groupby([\"group\",\"SEX\"])[\"label\"].mean()\n\ngroup  SEX\n1      1.0    0.507493\n       2.0    0.431978\n2      1.0    0.357579\n       2.0    0.400000\n3      1.0    0.348837\n       2.0    0.450000\n4      2.0    1.000000\n5      1.0    0.352941\n       2.0    0.500000\n6      1.0    0.510067\n       2.0    0.448485\n7      1.0    0.666667\n       2.0    0.625000\n8      1.0    0.475877\n       2.0    0.384444\n9      1.0    0.312178\n       2.0    0.328000\nName: label, dtype: float64\n\n\nThe below graph shows the number of female in each racial group and male in each racial group for the PUMS data of Indiana in 2018. Recall that group 1, shown in blue here, denotes white individuals, and group 2, shown in orange here, denotes black individuals. The rest of the groups denotes several other racial groups, and the detailed encoding could be accessed on PUMS website. The main takaway here is that the population in Indiana in 2018 is predominantly white.\n\ncounts = df.groupby([\"group\", \"SEX\"]).size().reset_index(name = \"n\")\nsns.barplot(data = counts, x = \"SEX\", y = \"n\", hue = \"group\")\n\n<AxesSubplot:xlabel='SEX', ylabel='n'>\n\n\n\n\n\nIn the following cell, we show a bar graph of average employment rate for individuals in different racial and gender binary categories. We should ignore group 4, since previously, as we are tallying the number of individuals in each racial group, we see that there is only one person in group 4, and that person is also recorded as employed, so group 4 female has a employment rate of \\(100\\) percent is because there’s only one person who is also employed.\n\n\npercentages = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index()\nsns.barplot(data = percentages, x = \"SEX\", y = \"label\", hue = \"group\")\n\n<AxesSubplot:xlabel='SEX', ylabel='label'>\n\n\n\n\n\n\n\n\nMoving on to training my model\nAfter consideration, we decide to go with logistic regression. We build our model, and we fit our model on our training data, which is stored in variable X_train, and y_train.\n\n\n# model = make_pipeline(StandardScaler(), LogisticRegression())\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(random_state=0, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0, solver='liblinear')\n\n\n\ny_hat = model.predict(X_test)\n\n\nprint(f\" The overall accuracy in predicting whether someone is employed in 2018 in Indiana is: {(y_hat == y_test).mean()}\", \"\\n\",\n    f\" The accuracy for white individuals is {(y_hat == y_test)[group_test == 1].mean()}\", \"\\n\", \n    f\" The accuracy for black individuals is {(y_hat == y_test)[group_test == 2].mean()}\")\n\n The overall accuracy in predicting whether someone is employed in 2018 in Indiana is: 0.8331855791962175 \n  The accuracy for white individuals is 0.8338424983027835 \n  The accuracy for black individuals is 0.8265086206896551\n\n\n\nTrain my model\nFirst let us fit our model.\n\nregress = model.fit(X_train, y_train)\n\nNow we score our model on the test sets, and we could read off the overall test score as \\(0.833\\).\n\nregress.score(X_test, y_test)\n\n0.8331855791962175\n\n\n\nmodel.score(X_test, y_test)\n\n0.8331855791962175\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))\n\n              precision    recall  f1-score   support\n\n       False       0.82      0.89      0.85      7277\n        True       0.86      0.77      0.81      6259\n\n    accuracy                           0.83     13536\n   macro avg       0.84      0.83      0.83     13536\nweighted avg       0.83      0.83      0.83     13536\n\n\n\n\n\n\nAudit the model\n\nOverall Measures\n\nWhat is the overall accuracy of your model?\nWhat is the positive predictive value (PPV) of your model?\nWhat are the overall false negative and false positive rates (FNR and FPR) for your model? ### By-Group Measures\nWhat is the accuracy of your model on each subgroup?\nWhat is the PPV of your model on each subgroup?\nWhat are the FNR and FPR on each subgroup? ### Bias Measures\nSee Chouldechova (2017) for definitions of these terms. For calibration, you can think of the score as having only two values, 0 and 1.\nIs your model approximately calibrated?\nDoes your model satisfy approximate error rate balance?\nDoes your model satisfy statistical parity?\n\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\nprint(features)\nprint(group)\nprint(label)\n\n(67680, 16)\n(67680,)\n(67680,)\n[[1.70e+03 2.00e+01 1.90e+01 ... 2.00e+00 2.00e+00 2.00e+00]\n [9.80e+03 4.80e+01 1.30e+01 ... 2.00e+00 1.00e+00 2.00e+00]\n [3.50e+03 2.00e+01 1.90e+01 ... 2.00e+00 2.00e+00 1.00e+00]\n ...\n [2.11e+04 6.70e+01 1.60e+01 ... 1.00e+00 2.00e+00 1.00e+00]\n [8.40e+03 6.70e+01 1.90e+01 ... 2.00e+00 2.00e+00 2.00e+00]\n [5.50e+04 6.70e+01 2.10e+01 ... 2.00e+00 2.00e+00 1.00e+00]]\n[9 1 1 ... 1 1 1]\n[False False False ... False False  True]\n\n\nRecall that: * upper-left corner is TN, which stands for True negative * lower-left corner is FN, False negative * upper-right corner is FP, which stands for False positive * lower-right corner is TP, True positive\nWe also care about the FPR, which stands for the false positive rate, which is top-right corner of the confusion matrix (after we normalize). FNR is false negative rate.\n\nmy_matr = confusion_matrix(y_test, model.predict(X_test))\nfig, ax = plt.subplots(figsize=(4,4))\nax.imshow(my_matr)\nax.xaxis.set(ticks=(0,1), ticklabels=('Predicted False', 'Predicted True'))\nax.yaxis.set(ticks=(0,1), ticklabels=('Actually False', 'Actually True'))\nax.set_ylim(1.5, -0.5)\n\nfor i in range(2):\n    for j in range(2):\n        ax.text(j,i, my_matr[i,j], ha='center', va='center', color='black')\n\n\n\n\n\n\nmy_matr = confusion_matrix(y_test, model.predict(X_test), normalize=\"true\")\nfig, ax = plt.subplots(figsize=(4,4))\nax.imshow(my_matr)\nax.xaxis.set(ticks=(0,1), ticklabels=('Predicted False', 'Predicted True'))\nax.yaxis.set(ticks=(0,1), ticklabels=('Actually False', 'Actually True'))\nax.set_ylim(1.5, -0.5)\n\nfor i in range(2):\n    for j in range(2):\n        ax.text(j,i, my_matr[i,j].round(4), ha='center', va='center', color='black')\n\n\n\n\nThe positive predictive value (PPV) is obtained by using this formula: \\[PPV = \\frac{TP}{TP+FP},\\] where \\(TP\\) denotes True Positive, and \\(FP\\) denotes True Negative. Hence, we need the value from lower-right corner (TP) of the confusion matrix divided by the value from lower right corner (TP) plus upper-right corner (FP).\n\n\nConcluding Discussion"
  },
  {
    "objectID": "posts/my-blog-post-06-unsupervised-learning/index.html",
    "href": "posts/my-blog-post-06-unsupervised-learning/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "# %reload_ext autoreload\n\n\nLink to Source Code\nHereis a link to the source code for this post.\n\n\nLink to reference for this blog post\nHere is a link to the main reference we use as we implement this post.\n\n\nPart I: Image Compression with the Singular Value Decomposition\nWe use the image of a cat which can be accessed here for free download: www.pexels.com. I have already downloaded a copy of an image of a tabby cat, and I have stored it in the same directory as this jupyter notebook.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nimport PIL\nfrom PIL import Image\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n# url = \"https://images.pexels.com/photos/1170986/pexels-photo-1170986.jpeg?cs=srgb&dl=pexels-evg-kowalievska-1170986.jpg&fm=jpg\"\n# myimg = read_image(url)\n\n\n# open the image from working directory\nimg = Image.open(\"./tabby_cat.png\")\nprint(f\"format: {img.format}\")\nprint(f\"size: {img.size}\")\nprint(f\"mode: {img.mode}\")\n# convert PIL images into numpy arrays.\nmyimg = np.asarray(img)\n\nformat: JPEG\nsize: (1771, 2657)\nmode: RGB\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(myimg)\n\naxarr[0].imshow(myimg)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\nprint(grey_img.shape[0])\nprint(grey_img.shape[1])\n\n2657\n1771\n\n\n\nfrom hidden_images import svd\nsvd1 = svd()\nk = 18 \nA_ = svd1.reconstruct(grey_img, k)\nsvd1.compare_images(grey_img, A_)\n\n\n\n\n\nsvd1.experiment(grey_img) \n\n\n\n\n\n\nPart II: Spectral Community Detection\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nWe need to return a vector of binary labels to split the graph.\n\nclubs = nx.get_node_attributes(G, \"club\")\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\n\nImplementing Laplacian Spectral Clustering\n\nfrom sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\nX, y = make_blobs(n_samples=100, n_features=2, \n                                centers=2, random_state=1)\n\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(42)\n\nn = 500\nX, y = make_circles(n_samples=n, shuffle=True, noise=0.07, random_state=None, factor = 0.5)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\nk = 10\nnbrs = NearestNeighbors(n_neighbors=k).fit(X)\nA = nbrs.kneighbors_graph().toarray()\n\n# symmetrize the matrix\nA = A + A.T\nA[A > 1] = 1\n\n\nimport networkx as nx\nfrom hidden_spectral import spectral \nspec  = spectral()\nspec.plot_graph(X, A)\n\n\n\n\n\n# fig, axarr = plt.subplots(1, 2, figsize = (8, 4))\ny_bad = np.random.randint(0, 2, n)\n\n# plot_graph(X, A, z = y, ax = axarr[0])\n# plot_graph(X, A, z = y_bad, ax = axarr[1])\n\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import pairwise_distances\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ngood labels cut = 22.0\nbad labels cut = 3000.0\ngood labels cut = 22.0\nbad labels cut = 3000.0\n\n\n\n\nUse a theorem from linear algebra\n\\(z\\) should be the eigenvector with the second smallest eigenvalue of the matrix \\[L = D^{-1}\\left[ D-A \\right]. \\] This matrix \\(L\\) is called the normalized Laplacian.\n\nfrom hidden_spectral import spectral \nspec = spectral()\nfig, ax = plt.subplots(figsize = (4, 4))\nz_ = spec.second_laplacian_eigenvector(A=A)\nspec.plot_graph(X, A, z=z_, ax = ax, show_edge_cuts = False)\n\n\n\n\n\nz = z_ > 0\nfig, ax = plt.subplots(figsize = (4, 4))\nspec.plot_graph(X, A, z, show_edge_cuts = True, ax = ax)\n\n\n\n\n\nfrom sklearn.datasets import make_moons\n\nH, z = make_moons(n_samples=100, random_state=1, noise = .1)\nfig, ax = plt.subplots(figsize = (4, 4))\na = ax.scatter(H[:, 0], H[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfig, ax = plt.subplots(figsize = (4, 4))\nz_hat = spec.spectral_clustering(H, k=6)\na = ax.scatter(H[:, 0], H[:, 1], c = z_hat, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spec.spectral_clustering(X, k = i)\n    a = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spec.spectral_clustering(H, k = i)\n    a = ax.scatter(H[:, 0], H[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\n\n\nTesting\n\n\nL = np.diag([4,5,22,2,3])\n# print(L)\nevalue, evector = np.linalg.eig(L)\nprint(\"evalue\")\nprint(evalue)\nprint(\"evector\")\nprint(evector)\nk = L.shape[1] \nidx = evalue.argsort()[:k][::-1] \nevalue = evalue[idx]\nevector = evector[:, idx]\n\nprint(\"evalue after change\")\nprint(evalue)\nprint(\"evector after change\")\nprint(evector)\n\nindex = evector.shape[1]\nprint(\"col\")\nprint(evector[:,index-2])\n\nevalue\n[ 4.  5. 22.  2.  3.]\nevector\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\nevalue after change\n[22.  5.  4.  3.  2.]\nevector after change\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]]\ncol\n[0. 0. 0. 0. 1.]"
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Here is a link to the source code for this Penguin Classification blog post."
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html#try-on-a-bigger-example",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html#try-on-a-bigger-example",
    "title": "Classifying Palmer Penguins",
    "section": "Try on a bigger example",
    "text": "Try on a bigger example"
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html#lets-try-a-different-data-set",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html#lets-try-a-different-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s try a different data set",
    "text": "Let’s try a different data set"
  }
]