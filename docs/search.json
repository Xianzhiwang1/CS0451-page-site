[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Xianzhi Wang’s Flabbergasted Blog for CS0451 Machine Learning class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Flabbergasted CSCI 0451 Blog",
    "section": "",
    "text": "My blog post on Palmer Penguins classification\n\n\n\n\n\n\nApr 22, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Image Compression with SVD and Spectral Community Detection\n\n\n\n\n\n\nApr 5, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy Blog post on Auditing Allocative Bias\n\n\n\n\n\n\nMar 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Linear Regression\n\n\n\n\n\n\nMar 20, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy blog post on Kernel Logistic Regression\n\n\n\n\n\n\nMar 2, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimplementing gradient descent\n\n\n\n\n\n\nFeb 24, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMy first Blog post on Perceptron\n\n\n\n\n\n\nFeb 15, 2023\n\n\nXianzhi Wang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/my-blog-post-01/index.html",
    "href": "posts/my-blog-post-01/index.html",
    "title": "My first Blog post on Perceptron",
    "section": "",
    "text": "Introduction.\nIn this blog post, we implement the perceptron algorithm, which is oftentimes the first machine learning algorithm a student encounters in a machine learning class (which is at least true in my case). We write code in Python for this implementation, and our goal is to classify binary labeled artificial data.\n\n\nImplementation\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries that we need.\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nIn the following code cell, we will generate a linearly separable dataset of binary-labeled 2D points. The make_blobs function essentially takes \\(n\\) samples, a number of features, and classes, and spits out a dataset of points with a data set with the given size, and label the data points using the classes. Visually, we see two clusters of points of two different color. In this special case where I set the seed, those two clusters seems linearly separable, which just means we could draw a straight line that completely seprates them. If we go to higher dimensions, then we need precise mathematical definitions, but we don’t need to worry about that right now.\n\nnp.random.seed(42)\nn=100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features=p_features-1, centers=[ (-1.7,-1.7, -1.7), (1.7,1.7, 1.7) ])\n\nfig=plt.scatter(X[:,0], X[:,1], c=y)\nxlab=plt.xlabel(\"feature 1\")\nylab=plt.ylabel(\"feature 2\")\n\n\n\n\n\n\nThe Perceptron Algorithm\nOur goal is to find the separating line using the perceptron algorithm. The algorithm takes in our feature matrix X and our vector of labels y. As detailed in the source code (link at the start of the blog), the algorithm performs the following steps: * Initialize the weights vector w * Iterate through the data points (randomly), updating the weights w until either a user-specified maximum number of iteration is reached. * record the accuracy score in self.history.\nState in mathematical terms, we would like to apply the perceptron algorithm to find the hyperplane that separates those data points, given that they are separable (so perceptron algorithm will converge). A key equation in the perceptron algorithm that defines the update is the following:\n\n\n\\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\]\nAnd this will provide us with the step to update w in each iteration.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, maxiter=10000)\n\nprint(p.history[-10:])\nprint(p.w_)\n# print(X)\n\n# print(w)\n\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n[ 50.44366343  39.1028252   21.79442178 -40.        ]\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(p.score(X,y))\n\n1.0\n\n\n\nprint((1<2)*2)\nprint((1>2)*2)\n\n2\n0\n\n\n\nprint(p.predict(X))\nprint(\"\\n\\n\")\nprint(y)\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\n[1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0]\n\n\n\nprint(p.w_)\nprint(np.size(p.w_))\n\n[39.71808898 55.01564117 -7.        ]\n3\n\n\n\n\nTime complexity for update step\nRecall our equation for the update step: \\[ \\tilde{w}^{(t+1)} = \\tilde{w}^{t} + \\mathbb{1} (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{x}_i \\rangle < 0)\\tilde{y}_i\\tilde{x}_i.\\] This involves taking an inner product $ ^{(t)}, _i , $ which has time complexity \\(O(p)\\) where \\(p\\) is a constant denoting the number of features. The other operations are addition, multiplication, taking the simple (step) function, which have constant time complexity \\(O(1)\\)."
  },
  {
    "objectID": "posts/my-blog-post-02-gradient-descent/index.html",
    "href": "posts/my-blog-post-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Reference\nHere is a link to the main reference we are using when crafting this blog post.\n\n\nIntroduction\nLet’s recall what problem we are investigating. We are working on the empirical risk minimization problem, which involves finding a weight vector w, that satisfy the following general form: \\[\n\\hat{w} = \\arg \\min_{w} L(w).\n\\] where\n\\[\nL(w) =  \\frac{1}{n} \\sum_{i=1}^{n} \\ell [ f_w(x_i), y_i ]  \n\\] is our loss function. In a previous blog post, we took \\(\\ell(\\cdot, \\cdot)\\) to be the 0-1 loss, but this time, we are going to use a different function called logistic loss, and it is detailed below. First, let’s recall what is matrix X and what are we doing.\nRemember from our previous blog post that our data includes a feature matrix X, which is a \\(n\\times p\\) matrix with entries being real numbers. The feature matrix X is a bunch of rows stacked together, and each row is going to represent a data point in our data set. Hence, since we have \\(n\\) data points in our data set, we have \\(n\\) rows in our feature matrix X. Since we record in each data point \\(p\\) many features that constitutes this data point, our feature matrix X has \\(p\\) columns. In other words, the number \\(n\\) represents the number of distinct observations, corresponding to \\(n\\) rows in X. \\(p\\) will always denote the number of features in this blog post. Our data also have a y, which is called target vector and lives in \\(\\mathbb{R}^n\\). The target vector gives a label for each observation. Hence, we have X, which contains a lot of information, and we want to predict y.\nWe also need some formulas that’s computed using pen and paper by our friends in the math department. First, we remember this piece of notation \\[ f_w(x) := \\langle w, x \\rangle \\] and we could obtain the following: \\[ \\nabla L(w) = \\nabla ( \\frac{1}{n} \\sum_{i=1}^{n} \\ell [ f_w(x_i), y_i ] ). \\] And remember $ = w, x_i $ (another piece of notation!), the logistic loss we are using is \\[ \\ell(\\hat{y}, y) = -y \\log \\sigma (\\hat{y}) - (1-y) \\log(1-\\sigma(\\hat{y})), \\] where $ () $ denotes the logistical sigmoid function. as demonstrated in the link under the Reference heading above, we have \\[ \\frac{d \\ell(\\hat{y},y)}{d \\hat{y}} = \\sigma (\\hat{y}) -y. \\] Therefore, with some effort, one can do this computation and obtain the following formula: \\[ \\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^{n} (\\sigma(\\hat{y_i}) - y_i) x_i, \\] and this will help us to implement the gradient of the empirical risk for logistic regression in python code using numpy library.\nRecall that in single variable calculus, gradient is just the derivative of a function. In multivariable calculus, since we have more than one variable, we take derivative with respect to each variable and put them in a vector to get our gradient. In formulas, let \\(f(z_1, z_2, \\cdots, z_p): \\mathbb{R}^p \\mapsto \\mathbb{R}\\) be our function, and the gradient of \\(f\\), denoted by \\(\\nabla f\\) is given by \\[ \\nabla f(z_1, z_2, \\cdots, z_p) := \\begin{bmatrix}\n&\\frac{\\partial f}{\\partial z_1}\\\\\n&\\frac{\\partial f}{\\partial z_2}\\\\\n&\\vdots\\\\\n&\\frac{\\partial f}{\\partial z_p}\\\\\n\\end{bmatrix}\n\\] Hence, given a vector \\[\\mathbf{z} = \\begin{bmatrix}\n&z_1\\\\\n&z_2\\\\\n&\\vdots\\\\\n&z_p\n\\end{bmatrix}\n\\] we know that \\(\\nabla f(\\mathbf{z})\\) is also going to be a vector of the same dimesion, we could put them in the same equations. Hence, the following Batch Gradient Descent Algorithm makes sense.\n\n\nBatch Gradient Descent Algorithm:\nfor function \\(f\\), starting point \\(z^{(0)}\\), and learning rate \\(\\alpha\\), we perform the following update step many many times: \\[\nz^{(t+1)} \\leftarrow z^{(t)} - \\alpha \\nabla f(z^{(t)})\n\\] Return the final value \\(z^{(t)}\\).\nIn code, we need a precise way to decide when to stop after performing the update many times. One way is to stop when we reached the maximum_number_of_iteration or max_iter that is specified by the user, or until convergence, in the sense that \\(\\nabla f(z^{(t)})\\) is close to 0. Also, there are math theorems guarantee that \\(z^{(0)}, z^{(1)}, \\cdots, z^{(t)}\\) converges to \\(z^{*}\\) under suitable conditions. Now, with the math background out of the way, let’s see this in code.\n\n%load_ext autoreload\n%autoreload 2\n\nWe start by importing the relavant libraries and creating some data points using the make_blobs function that we imported from sklearn.datasets. We would like to create some non-separable data, which means graphically in 2 dimension, we cannot draw a straight line to separate the data points of the two different classes (as indicated by the color). Notice that the horizontal axis is Feature 1, and the vertical axis is Feature 2.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (4,4)\n\nimport numpy as np\nnp.random.seed(42)\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nRecall that we have feature matrix X, which is a \\(n\\times p\\) matrix with entries being real numbers. The number \\(n\\) represents the number of distinct observations, and we have \\(n\\) rows in X. \\(p\\) is the number of features. Our data also have a y, which is called target vector and lives in \\(\\mathbb{R}^n\\). The target vector gives a label, value, or outcome for each observation. In the solutions_logistic.py, we implemented the gradient descent using the following update step.\n\\[ w^{(t+1)} \\leftarrow w^{(t)}  - \\alpha \\cdot \\nabla L(w^{(t)}), \\]\nwhere \\(\\nabla L(w)\\) is given by the following equation: \\[ \\nabla L(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell(f_{w}(x_i), y_i)\\] Now let’s import our implementation and create plots.\n\nfrom solutions_logistic import LogisticRegression \nLR = LogisticRegression()\nX_ = LR.pad(X)\n\n# inspect the fitted value of w\nLR.fit(X, y, alpha = 0.01, max_epochs = 2000)\nprint(LR.w_)\n\n[ 1.59965852  1.45281308 -0.20047656]\n\n\nAfter calling the function fit, we obtain the weight vector w_, but are they doing what they are supposed to do? How big is the loss for this perticular case? We could visualize this result by plotting the line that hopefully separates the data points in a intuitive way. See the picture on the left. Now we would like to find out about how the empirical loss evolves as the number of iteration goes up. Let’s plot this in the picture on the right.\n\nnp.random.seed(42)\n# pick a random weight vector and calculate the loss\nw = .5 - np.random.rand(p_features)\n# fig = plt.scatter(X_[:,0], X_[:,1], c = y)\n# xlab = plt.xlabel(\"Feature 1\")\n# ylab = plt.ylabel(\"Feature 2\")\n# f1 = np.linspace(-3, 3, 101)\n# p = plt.plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n# title = plt.gca().set_title(f\"Loss = {LR.last_loss}\")\n\n\nplt.rcParams[\"figure.figsize\"] = (18,6)\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nFrom the plot on the left, we see that our gradient descent algorithm is doing a good job at finding the line that separates the data. From the plot on the right, we see that as the number of itermations on the x-axis increases, the empirical risk goes down. The loss is around \\(0.15\\) to \\(0.20\\), and this is a reasonable number since our data is not linear separable, as we can see from the picture.\n\n\nAccuracy of regular gradint descent\nAgain, we draw the scatter plot and the fitted line on the left, and on the right, we plot the evolution of the accuracy score as the number of iteration increases.\n\nmyScore = LR.score(X_,y)\n\nfig, axarr = plt.subplots(1, 2)\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Score = {myScore}\")\nf1 = np.linspace(-3, 3, 101)\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\naxarr[1].plot(LR.score_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nWe could also print out the vector y and the predicted vector given by the function predict(). In this way, we could have a look “under the hood” and obtain a rough sense how good is our prediction.\n\nprint(f\"our actual labels: {y}\")\nprint(f\"our predicted labels: {LR.predict(X_)}\")\n\nour actual labels: [0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1\n 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0\n 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0\n 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0]\nour predicted labels: [0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0\n 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0\n 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0]\n\n\n\n\nStochastic Gradient Descent\nHere, by “Stochastic” we just mean we introduce a certain amount of randomness to our gradient descent step. The modification from the regular gradient descent is as follows. We pick a random subset \\(S \\subset [n]\\) and we let \\[ \\nabla_S L(w) = \\frac{1}{|S|} \\sum_{i \\in S} \\nabla \\ell(f_{w}(x_i), y_i).\\] And the rest is business as usual. We deem our weights w as “good enough” when: either the user-specified maximum number of iteration is reached, or the current empirical risk function is “close enough” to the one from the previous iteration. With the mathematics technicality out of the way, let’s visualize the scatter plot, the best-fit-line, and the evolution of the empirical risk, and the evolution of the accuracy score all in one go.\n\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = 1) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 3)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\naxarr[2].plot(LR.score_history)\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nStochastic gradient uses random batches of the data to compute the gradient, and in this case it performs similar to regular gradient descent. We see that as Iteration number increases, the empirical risk decreased and the accuracy score increased. Sometimes, there are kinks in the curve, which means more iterations is not always better. However, this time, there’s no kinks, and we see that our stochastic gradient did a good job at minimizing empirical risk as iteration increases.\n\nComparison of Gradient Descent, Stochastic Gradient Descent, and Stochastic Gradient with Momentum\nHaving seen how regular gradient descent and stochastic gradient descent perform, we could add a momentum feature to the stochastic gradient descent. Then we have the choice of selecting momentum = True when we call the function fit_stochastic. Hence, we could compare the three versions of gradient descent and plot their respective empirical risk (loss) evolution in one picture, where the horizontal axis is number of iterations, and the vertical axis is empirical risk. Also, let’s try having 5 features in our artificial data set for this comparison.\n\n# 5 features\np_features = 5 \nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = True, \n                  batch_size = 100, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  momentum = False, \n                  batch_size = 100, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe have just ploted the loss history over iteration number of the 3 methods we implemented. As we see in this plot, regular gradient descent is the worst at minimizing the empirical risk, and it also takes the longest to converge. Stochastic gradient did a better job than regular gradient descent at minimzing the empirical risk, and it converges faster. Stochastic gradient descent with momentum is clearly the best here, since it converged before hitting 100 iterations, and it did a good job at minimizing empirical risk, outperforms the regular stochastic after \\(10\\) iterations.\n\n\nChoosing an Alpha too big\nAgain, we take a look at regular gradient descent, and this time, we experiment with different values of alpha that are big.\n\n# back to 2 features\np_features = 2 \nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nWhen we choose a big alpha, such as alpha = 10, we see that the empirical risk is minimized in a slower way, and our curve on the right is less steep. Still our algorithm managed to find a line that reasonably separates the data.\n\nLR.fit(X, y, alpha = 90, max_epochs = 1000)\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.last_loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.w_[2] - f1*LR.w_[0])/LR.w_[1], color = \"black\")\n\naxarr[1].plot(LR.loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\nThis time, we let alpha=90, which is creating some strange behaviors if we look at the plot on the right. Again, x-axis is the iteration number, and the y-axis is the empirical risk that we are trying to minimize. However, instead of monotonically decreasing empirical risk as iteration increase, we see that empirical risk jumps up and goes down many times. Also, the separating line in the left plot is also slightly off compared to before. In this case, alpha is too big, and when we take a step in the correct direction, which is \\(-\\nabla f\\), we overshot and empirical risk goes up instead of down. Hence we have this periodic behavior.\n\n\nExperimenting with batch size that affects convergence speed.\n\nLR.fit_stochastic(X, y, \n                  max_epochs = 5000, \n                  momentum = False, \n                  batch_size = 15, \n                  alpha = 1) \n\nloss = LR.stochastic_loss_history[-1]\n\nfig, axarr = plt.subplots(1, 3)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR.omega_[2] - f1*LR.omega_[0])/LR.omega_[1], color = \"black\")\n\naxarr[1].plot(LR.stochastic_loss_history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n\naxarr[2].plot(LR.score_history)\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Accuracy Score\")\nplt.tight_layout()\n\n\n\n\nWe see that as batch size gets smaller, we see Stochastic gradient might not do as well as before, since there’s kinks in the accuracy score history. The score hits it’s highest point, and then actually descreases slowly as iteration increases. Also, the separating line in the graph on the left is slightly off compared to the ones we had before, and we see that the loss is at \\(0.32\\) this time, which is higher than before. In the plot that’s below, we use different batch size and compare convergence behavior as iteration incrases.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 15, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient batch: 15\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 30, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient batch: 30\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 10000, \n                  momentum = False, \n                  batch_size = 200, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient batch: 200\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 10000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nWe see that the Stochastic gradient with batch size \\(200\\) is performing relatively okay, using about \\(1000\\) iterations to converge, which is close to the performance of regular gradient descent. We see that smaller batch size actually performs better here, and the one with batch size 15 has converged with \\(100\\) iterations, and it did a good job at minimizing empirical risk. The one with batch size \\(30\\) also did well, converging using about \\(300\\) iterations, definitely faster than the one with batch size 200.\n\n\nDoes Momentum speeds up convergence and performs better than just Stochastic gradient?\n\n# 10 features\n# make the data\np_features = 10 \nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 5000, \n                  momentum = True, \n                  batch_size = 90, \n                  alpha = 0.1) \n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 5000, \n                  momentum = False, \n                  batch_size = 90, \n                  alpha = 0.1)\n\nnum_steps = len(LR.stochastic_loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.stochastic_loss_history, label = \"stochastic gradient\")\nxlab = plt.xlabel(\"Iteration number\")\nylab = plt.ylabel(\"Empirical Risk\")\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIn this case, with a data set having \\(10\\) features and a batch size of \\(90\\), we see that momentum clearly outperforms regular stochastic gradient descent. At some number of iteration, momentum achieves smaller empirical risk, and it also takes less number of iterations to converge than regular stochastic. And we conclude our blog post here."
  },
  {
    "objectID": "posts/my-blog-post-03-kernel-logistic/index.html",
    "href": "posts/my-blog-post-03-kernel-logistic/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Reference for this blog post\nHere is a link to the main reference we are using when creating this blog post.\n\n\nIntroduction.\nIn this blog post I am going to discuss kernel logistic regression for binary classification. Recall that in previous blog posts, the visualization of the data set with \\(2\\) features looks graphically like it could be reasonably separated by a line. With more features, we could no longer visualize in \\(2\\) dimension, but the idea is the same, we use linear separation, like a hyperplane. However, what if the data is dominated by other patterns that are not suitable for a straight line (or a straight plane if you will) separation? Hence, we need to use some other methods, and hence let us use kernel logistic regression.\nRecall the empirical risk minimization problem in previous blog posts, which is finding the weight vector w that minimize the loss function \\(L(w)\\). \\[ \\hat{w} = \\arg \\min_{w} L(w), \\] where the loss function \\(L(w)\\) is of the following form: \\[ L(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell( \\langle w, x_i \\rangle, y_i ), \\] and $(, y ) $ is the logistic loss from the previous blog post. However, previously, we only studied linear decision boundaries, and now we want to study nonlinear patterns. Hence, instead of feature vector \\(x_i\\), we need modified feature vector \\(\\kappa(x_i)\\) that does the following: \\[\n\\kappa(x_i) =\n\\begin{bmatrix}\n&k(x_1, x_i)\\\\\n&k(x_2, x_i)\\\\\n&\\vdots\\\\\n&k(x_n, x_i)\\\\\n\\end{bmatrix}\n\\] where \\(k: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) is called a kernel function. Hence our empirical risk function (loss function) is now \\[ L_k(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell( \\langle w, \\kappa(x_i) \\rangle, y_i ), \\] where the subscript \\(k\\) in \\(L_k(w)\\) indicates we are using a kernel function \\(k\\).\n\n\n%load_ext autoreload\n%autoreload 2\n\nFirst, let’s import some libraries, and let’s create an artificial data set that has nonlinear patterns.\n\n\n# from sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons, make_circles, make_blobs, make_biclusters, make_classification\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (6,3)\nplt.rcParams['figure.dpi'] = 117 \nplt.rcParams['savefig.dpi'] = 78 \nimport numpy as np\nnp.random.seed(42)\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\nnp.seterr(all=\"ignore\")\n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nFirst, we use make_moons from sklearn.datasets to generate some artificial data. How to deal with the nonlinear pattern we see in the plot below (graph on the left)? it seems a linear separator like a straight line will not do a satisfactory job here. Hence, we need kernel logistic regression, which we implemented in kernel_logistic.py, and we are going to import it into this notebook.\n\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom kernel_logistic import KLR \nX_moon, y_moon = make_moons(200, shuffle = True, noise = 0.1)\n\nAfter seeing this data set below, we would guess that a linear classification would not do a great job at classifying the patterns we see here with our eyes. Hence, without further ado, let’s see how our KLR, kernel logistic regression handle this binary classification problem. Let’s choose a gamma = 1.8 and fit our model.\n\nfrom kernel_logistic import KLR \nKLR5 = KLR(rbf_kernel, gamma = 1.8)\nKLR5.fit(X_moon, y_moon)\n\n\nKLR5.my_plot(X_moon, y_moon, \"make_moons\")\n\n\n\n\nWe see that we achieved a training score of \\(0.985\\). Not bad at all! Also, just looking at the graph on the right, we see that the decision boundary looks spot on, and we could tell it is a good classification by just looking at it. The blue squares land correctly in the blue region, and the orange triangles land correctly in the orange region. Still, let’s create a fresh synthetic data that our Kernel Logistic Regression has not seen before, and test it on that.\n\nX_moon2, y_moon2 = make_moons(200, shuffle = True, noise = 0.1)\n\nBelow, we see a testing score of \\(0.935\\), which is not bad at all considering our model has not seen this perticular synthetic data before!\n\nKLR5.my_plot(X_moon2, y_moon2, \"make_moons\")\n\n\n\n\nNot bad at all! We see that KLR did a decent job fitting the non-linear decision boundary, and achieved an accuracy score above \\(0.9\\). The data is quite neatly separated by this curvy decision boundary our KLR generated. Now, let’s explore the parameter gamma and also experiment with data that has different amount of noise in it.\n\n\nChoosing gamma and Try on a tiny example\nTo start us off, we manually create a tiny data set, and we call the fit function that we implemented in the source code (link at the start of the blog). We start with a big value for gamma: gamma = 100, and in this tiny data set, we could see clearly what’s going on with the decision boundary, and a tiny data set could also help with debugging purposes.\n\nXX = np.array([\n[-40.84739307, 30.71154296],\n [ 11.46814927, -9.28580296],\n [ -40.5192833,   -70.94984582],\n [ 10.73327397,  10.17310931],\n [ 10.33197143,  0.43375035],\n\n [ -1.62726102, -0.54736954],\n[-7.84739307, 5.71154296],\n [ -21.46814927, -19.28580296],\n [ -10.5192833,   -50.94984582],\n [ 7.73327397,  0.17310931],\n])\nyy = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\nplt.rcParams[\"figure.figsize\"] = (3,3)\nplt.scatter(XX[:,0], XX[:,1], c = yy)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nIt seems that those \\(10\\) data points does not display a linear pattern, which is what we want. Now we create an instance of the KLR class with gamma = 100 and test our fit function to see if it could classify these \\(10\\) points with \\(100 \\%\\) accuracy.\n\nfrom kernel_logistic import KLR \nKLR = KLR(rbf_kernel, gamma = 100)\nKLR.fit(XX, yy)\n\nWe see that our classifier makes a little circular boundary around each orange triangular data points, while the rest of the region is all classified as blue for the blue squares. gamma essentially controls how “wiggly” our decision boundary is allowed to be. If gamma is small, our decision boundary would be less curvy, and if gamma is too big, we tend to overfit and produce the picture below.\n\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAlso, we do achieve \\(100\\%\\) accuracy here, since our accuracy is \\(1.0\\). But if you insist on pop open the hood and inspect what’s underneath, let’s print out the actual label yy and our predicted labels, and see that they are indeed the same.\n\nprint(f\"Actual labels: {yy}\")\nprint(f\"Predicted labels: {KLR.predict(XX)}\")\n\nActual labels: [1 1 1 1 1 0 0 0 0 0]\nPredicted labels: [1 1 1 1 1 0 0 0 0 0]\n\n\nNow, with the same tiny data set, we use a very small gamma and see how the decision boundary would look like. Hence, let gamma = 0.05.\n\nfrom kernel_logistic import KLR \nKLR = KLR(rbf_kernel, gamma = 0.05)\nKLR.fit(XX, yy)\nplot_decision_regions(XX, yy, clf = KLR)\nmypredict = KLR.predict(XX)\ntitle = plt.gca().set(title = f\"Accuracy = {(mypredict == yy).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nwe see that our decision boundary is less “smooth” and looks more “jagged”, so choosing a good gamma would decide the right complexity for our decision boundary, hence the parameter gamma is quite important to this binary classification problem.\n\n\nChoosing different noise level and Try on a synthetic data set with make_circles\nThis time, we set the size of the data set to contain \\(200\\) data points, and we call mak_circle to generate our data. We set noise = 0.8, so our data set is very noisy, and in the visualization below (the plot on the right), we see that we cannot really distinguish the circles by eye, so the data set is indeed very noisy and chaotic.\n\nX_cir, y_cir = make_circles(200, shuffle = True, noise = 0.8)\n\nLet’s set gamma = 1.8 and train our model by calling the .fit() function.\n\nfrom kernel_logistic import KLR \nKLR6 = KLR(rbf_kernel, gamma = 1.8)\nKLR6.fit(X_cir, y_cir)\n\nBelow, in the plot on the left, we see that with very noisy data, our Kernel Logistic Regression only scored \\(0.63\\) in accuracy, and judging by looking at the picture, we feel that our classifier only did a slightly above average job at classifying the data points. Hence, noisy data does negatively affect the performace of our Kernel Logistic Regression.\n\nplt.rcParams[\"figure.figsize\"] = (8,4)\nKLR6.my_plot(X_cir, y_cir, \"make_circles\")\n\n\n\n\nStill, let’s test our trained model on a fresh copy of synthetic data generated using make_circles. This time, the testing accuracy is only \\(0.435\\), which is less than \\(0.5\\). Hence, we might be better off just guess randomly instead of applying our training model in this case!\n\nX_cir2, y_cir2 = make_circles(200, shuffle = True, noise = 0.8)\nKLR6.my_plot(X_cir2, y_cir2, \"make_circles\")\n\n\n\n\nNow, we try with a data set with very little noise by setting noise = 0.05, and let the rest of the parameter stay the same. Hence, we keep gamma = 1.8, number of data points \\(200\\), etc.\n\nX_cir3, y_cir3 = make_circles(200, shuffle = True, noise = 0.05)\n\n\nfrom kernel_logistic import KLR \nKLR2 = KLR(rbf_kernel, gamma = 1.8)\nKLR2.fit(X_cir3, y_cir3)\n\n\n\nfig, axarr = plt.subplots(1,2)\naxarr[0].scatter(X_cir3[:,0], X_cir3[:,1], c = y_cir3)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = \"artificial data created by make_circles\")\n\n\naxarr[1].plot()\nplot_decision_regions(X_cir3, y_cir3, clf = KLR2)\nyourpredict = KLR2.predict(X_cir3)\naxarr[1].set(title = f\"Accuracy = {(yourpredict == y_cir3).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.tight_layout()\n\n\n\n\nThis time, since our data has pretty clear patterns, and has very little noise, our Kernel Logistic Regression did a great job at classifying the data points and engineering the decision boundary. We see that decision region plot on the right looks spot on, and the accuracy score is around \\(0.95\\), which is pretty high compared to the score we had before when the data was noisy.\n\n\nTry other problem geometries\nlet’s start with a data set generated by make_blobs function, with a reasonable amount of standard deviations (which is a proxy for noise in this case) in it.\n\nX, y = make_blobs(n_samples = 200, centers = 2, n_features = 2, cluster_std=3, random_state=42)\n\nLet us set gamma = 2.45 and see how well we do in this case.\n\nfrom kernel_logistic import KLR\nKLR3 = KLR(rbf_kernel, gamma = 2.45)\nKLR3.fit(X, y)\n\n\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8,4)\nKLR3.my_plot(X,y, \"make_blobs\")\n\n\n\n\n\nWe see that we achieved a very high accuracy score of \\(0.995\\). We could also call the .predict() function to find out the predicted value and compare that to the actual value of y, and see that they indeed match up.\n\nprint(f\"Actual labels: {y}\")\nprint(f\"Predicted labels: {KLR3.predict(X)}\")\n\nActual labels: [1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0\n 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0]\nPredicted labels: [1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0\n 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0]\n\n\nIn the next example, we use make_classification to make some sythetic data with \\(4\\) features. Since we have more than \\(2\\) features, we could only plot \\(2\\) dimensions at a time to visualize how our classification performs. Let’s generate the data set first.\n\nX_class, y_class = make_classification(n_samples = 200, \n                           n_classes = 2, \n                           n_redundant= 0,\n                           n_informative= 2,\n                           n_features = 4, \n                           n_clusters_per_class = 1, \n                           class_sep = 1.5,\n                           random_state=42)\n\nNow, we set gamma = 2.45 and we train our Kernel Logistic Regression on this data set.\n\nfrom kernel_logistic import KLR\nKLR4 = KLR(rbf_kernel, gamma = 2.45)\nKLR4.fit(X_class, y_class)\n\nNext, we call our costom function .myprint() that generate the plots for us that only focuses on the first \\(2\\) dimensions, since we can only plot \\(2\\) dimension at a time in 2D.\n\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (6,3)\nvalue = 1 \nwidth = 1 \nKLR4.myprint(X_class, y_class, \"make_classification\", value, width)\n\n\n\n\nNow, let’s also take a peak at how our Kernel Logistic Regression is doing in dimension \\(3\\) and \\(4\\). We see that the decision boundary looks pretty good! Notice that feature is labeled 0, 1, 2, 3, so we are looking at feature 2 and 3 here!\n\nplt.rcParams[\"figure.figsize\"] = (3,3)\nplot_decision_regions(X_class, y_class, clf = KLR4, \n                    feature_index=[2,3],\n                    filler_feature_values={0: value, 1: value},\n                    filler_feature_ranges={0: width, 1: width})\nyourpredict = KLR4.predict(X_class)\ntitle = plt.gca().set(title = f\"Accuracy = {(yourpredict == y_class).mean()}\",\n                xlabel = \"Feature 2\", \n                ylabel = \"Feature 3\")"
  },
  {
    "objectID": "posts/my-blog-post-04-linear-regress/index.html",
    "href": "posts/my-blog-post-04-linear-regress/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nIn this blog post let us take a look at Linear Regression. Linear regression, especially OLS (ordinary least squares) regression, is the bread and butter of many fields like economics and statistics. When we first learn about OLS regression, often times it was in some other setting, at least for yours truly. I have learned this in a economics class on regression analysis. However, we could easily formulate the same concept in a way that’s closer to the style of machine learning, and specifically classification tasks. For OLS linear regression, the loss function is \\[\n\\ell (\\hat{y}, y) = (\\hat{y} - y)^2,\n\\] where \\(\\hat{y} = \\langle w, x \\rangle\\) is the predicted labels (which the inner product of \\(w\\), the weights, and \\(x\\), our data point), and \\(y\\) is the true label. Hence, when we are trying to minimize loss, we are trying to minimize the squared difference between our prediction and the true label. The empirical risk minimization problem is \\[\n\\hat{w} = \\argmin_{w} L(w) \\\\\n\\] where \\[\nL(w) = \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\lvert \\lvert Xw - y \\rvert \\rvert ^2_2.\\\\\n\\] Hence, least square is simply referring to minimizing the sum of squares for the loss function.\nBefore we start the implementation, we first record the following code snippet that will help us to automatically load our source code when we are constantly editing the .ipynb and .py files.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries, then we perform our fit_gradient and fit_analytic on the following simple data set with only one features to visualize our linear regression.\n\nimport numpy as np\nnp.random.seed(42)\nfrom matplotlib import pyplot as plt\nimport matplotlib\nplt.rcParams[\"figure.figsize\"] = (18,6)\nplt.rcParams['figure.dpi'] = 156 \nplt.rcParams['savefig.dpi'] = 156 \nfrom linear_regression import LinearRegression \n\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\n\nFitting Linear Regression using gradient descent; a.k.a. fit_gradient\nIn fit_gradient, the key step is to compute the gradient using a descent algorithm so that we could solve the following problem: \\[ \\hat{w} = \\arg \\min_{w} L(w). \\] Equivalently, we could unpact this equation: \\[ \\hat{w} = \\sum_{i=1}^{n} \\ell(\\hat{y}_i, y_i) = \\argmin_{w} \\sum_{i=1}^{n} ( \\langle w, x_i \\rangle - y_i)^2.\\] Recall that our loss function is of the form $ (, y) = (-y)^2 $ since we are using ordinary least square regression.\nWe start by taking derivative with respect to \\(w.\\) Using chain rule for matrices, we obtain the following expression: \\[ \\nabla L(w) = 2 X^{T}(X\\cdot w -y).\\] Then, we use gradient descent to find the \\(w\\) that is “good enough.” We achieve this by the following iteration: \\[ w^{(t+1)} \\leftarrow w^{(t)} - 2 \\cdot \\alpha \\cdot X^{T} (X \\cdot w^{(t)} - y).\\]\nWe use the following code block to generate a small data set for testing our linear regression implementation. Let’s plot our data, and in this 2-Dim case we could see the linear pattern with our eyes, and it’s quite intuitive that the best-fit-line obtained by our algorithm should also “fit” the data visually as if we were to draw it by hand on the plot of this sythetic data set.\n\n# We start by generate a small data set.\nw0 = -0.5\nw1 =  0.7\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.figure(figsize=(6,6))\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nWe are able to generate data and visualize this problem when p_features = 1. Graphically, we are trying to draw a line “of best fit” through the data points in the sense of OLS, which stands for Ordinary Least Squares. The line we draw just means given the feature x, we find the corresponding predicted y using the line, which will be close to the original y, if we have done a good job.\nAfter importing linear_regression.py, we could call the fit_gradient method that implements the gradient descent algorithm for us, as illustrated in the above cell. In the following cell, we plot the “line of best fit” using the weights LR1.w that we obtained after running fit_gradient. Also, we print out the weights vector w that we fited, which seems to be doing a good job judging from the pictures below, plotted together with Lasso and fit_analytic.\n\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\nprint(f\"the weights that we obtained after calling fit_gradient are: {LR1.w}\")\n\nthe weights that we obtained after calling fit_gradient are: [[ 0.62864267]\n [-0.46564827]]\n\n\n\n\nUsing LASSO from scikit-learn\nRoughly, LASSO algorithm is OLS regression plus a regulariation term, so the loss function looks like this: \\[\nL(w) = \\lvert \\lvert Xw - y \\rvert \\rvert ^2_2 + \\lvert \\lvert w' \\rvert \\rvert_1.\\\\\n\\] Where \\[\n\\lvert \\lvert w' \\rvert \\rvert_1 = \\sum_{j=1}^{p-1} |w_j|.\\\\\n\\] Hence, we could think of this regularization term as some sort of penalty term when we have problems with overparameterization. In this post, we are not going to dive deep into the details of LASSO regression, so we are just going to use the implementation from scikit-learn. Again, we plot the result of fitting this regression together with the fit_gradient and fit_analytic in the big visualization below.\n\nL.fit(x,y)\nL.score(x,y)\nprint(\"*\")\nL_w = np.hstack([L.coef_, L.intercept_])\nprint(f\"the weights that we obtained after calling fit_gradient are: {L_w}\")\n\n*\nthe weights that we obtained after calling fit_gradient are: [ 0.6426091  -0.47312394]\n\n\n\n\nFitting Linear Regression using a analytic formula; a.k.a. fit_analytic\nSimilarly to fit_gradient, we also have a method called fit_analytic, which uses a formula to compute the weights w exactly, and this is implemented using the followiing equation: \\[ \\hat{w} = (X^T X)^{-1} X^T y, \\] where \\(\\hat{w}\\) denotes the weights we obtained after calling the function fit_analytic. Note that in order for this formula to make sense, we need X to be a invertible matrix. Now, with the math part out of the way, let’s see this in action in the following block, where we plot the three regressions we introduced so far together in one plot.\n\nmatplotlib.rc('font', size=6)\n# gradient\nLR1 = LinearRegression()\nX_ = LR1.pad(x)\nLR1.fit_gradient(X_, y, alpha=0.0001, max_epochs=1e4)\n\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\naxarr[0].scatter(x,y)\naxarr[0].plot(x, X_@LR1.w, color = \"black\")\n\n# Analytic \nLR2 = LinearRegression()\nX_ = LR2.pad(x)\nLR2.fit_analytic(X_,y)\n\naxarr[1].scatter(x,y)\naxarr[1].plot(x, X_@LR2.w, color = \"black\")\n\n# LASSO\naxarr[2].scatter(x,y)\naxarr[2].plot(x, X_@L_w, color = \"black\")\n\nlabs = axarr[0].set(title=\"Best-fit-line by implementing gradient descent\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[1].set(title=\"Best-fit-line by implementing the analytic formula\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[2].set(title=\"Best-fit-line by implementing LASSO\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\nplt.tight_layout()\n\n\n\n\n\n\nMore than one feature\nNow we use the following function to create both testing and validation data. At this stage, we could experiment with more features. We use the following code to create artificial data sets that has any number of features that we specify.\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n        # print(w)\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = LR.pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = LR.pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nWhen the number of features is one, p_features = 1, we could plot the artificial training data set and the validation data set. We lose this luxury when we have 2 or more features. Let’s plot the data set we are going to use.\n\nn_train = 100\nn_val = 100\np_features = 1 \nnoise = 0.2\n\n# create some data\nLR = LinearRegression()\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow we experiment with the number of features being n_train - 1, which is quite a lot features. Are we going to have a high training score? Let’s find out!\n\n\nn_train = 100\nn_val = 100\np_features = n_train - 1 \nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nHere’s the snippets within the fit_gradient function that makes the same code work for different number of features:\nfeatures = X_.shape[1]\nself.w = np.random.rand(features)\n\nfrom linear_regression import LinearRegression \nLR = LinearRegression()\nX_train_ = LR.pad(X_train)\nX_val_ = LR.pad(X_val)\nLR.fit_analytic(X_train_, y_train) # I used the analytical formula as my default fit method\nprint(f\"Training score = {LR.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val_, y_val).round(4)}\")\n\nTraining score = 0.0333\nValidation score = -0.2332\n\n\nWe see that our training score is very close to zero, hence very low.\n\nprint(f\"The estimated weight vector w is: {LR.w}\")\nprint(f\"Training Loss = {LR.Big_L(X_train_, y_train).round(2)}\")\nprint(f\"Validation Loss = {LR.Big_L(X_val_, y_val).round(2)}\")\n\nThe estimated weight vector w is: [-1.60048517 -3.1978511   0.80566383  1.10203529  1.36187644 -0.47123621\n  1.06288478 -0.49759679  0.88232925 -3.21484205  1.00067548 -0.85983013\n  0.36202332  0.61615846  3.23278271  0.49621812  1.51714629 -1.97442441\n  2.91490824 -0.56270441  1.34873889 -4.33324786 -3.39289948 -1.86798015\n -4.24483306 -3.9345999  -0.38623783  0.77311176  2.2738731  15.68998649]\nTraining Loss = 0.0\nValidation Loss = 20.17\n\n\nLet’s plot the score history and loss history to get a better idea of what’s going on here. We see that we get pretty low scores, and our loss history doesn’t look very good. Hence, our OLS regression is not super good when the number of features gets to large.\n\nLR5 = LinearRegression()\n\nLR5.fit_gradient(X_train_, y_train, 0.0001, 1000)\nprint(f\"Training score = {LR5.score(X_train_, y_train).round(4)}\")\nprint(f\"Validation score = {LR5.score(X_val_, y_val).round(4)}\")\n\n# plt.plot(LR2.score_history)\n# labels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].plot(LR5.score_history)\naxarr[1].plot(LR5.loss_history)\nlabs = axarr[0].set(title = \"Score History\", xlabel = \"Iteration\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Loss History\", xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.tight_layout()\n\nTraining score = -5.9295\nValidation score = -6.2086\n\n\n\n\n\n\nLASSO Regularization\nIn this last section, let us recall that LASSO uses a modified loss function of the following expression: \\[ L(w) = \\lVert X \\cdot w -y \\rVert ^2_2 + \\sum_{j=1}^{p-1} \\alpha \\cdot | w_j |. \\] And hopefully, LASSO will be a better option when number of features gets too big.\n\nL2 = Lasso(alpha = 0.01)\n\n\n\nn_train = 30 \nn_val = 30\np_features = 1 \nnoise = 0.2\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL2.fit(X_train, y_train)\nL2.score(X_val, y_val)\n\n0.5877132844037414\n\n\nHey, this score is not bad!\n\nLR4 = LinearRegression()\nLR4.lasso_score(n_train, n_val, noise)\nLR4.lin_regress_score(n_train, n_val, noise)\nLR4.lin_regress_score_analytic(n_train, n_val, noise)\n\nLet’s use a custom function that we implemented in linear_regression.py to increase the number of features one by one, and for each number of feature, we generate a new sythetic data set and fit the \\(3\\) regressions we introduced so far. Then we plot the Score history for all three regressions against number of features, so y-axis is score, and x-axis is number of features. In this way, we could see clearly how fit_gradient, fit_analytic, and LASSO perform as the number of features increases to up to \\(n\\), which is the number of data points.\n\n# from matplotlib.pyplot import figure\n# figure(figsize=(8, 6), dpi = 156)\nfig, axarr = plt.subplots(1, 3, sharex = False, sharey = False)\naxarr[0].plot(LR4.lasso_score_history)\naxarr[1].plot(LR4.fit_gradient_score_history)\naxarr[2].plot(LR4.fit_analytic_score_history)\nlabs = axarr[0].set(title = \"LASSO Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"Linear regression fit_gradient Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nlabs = axarr[2].set(title = \"Linear regression fit_analytic Score History\", xlabel = \"number of features\", ylabel = \"Score\")\nplt.tight_layout()\n\n\n\n\nWe see, it turns out that all three are not doing super well as the number of feature increases. The score goes up and down wildly, and especially for fit_analyitic, it seems that the score just fall off the cliff at one point and then bounced back somehow. Hence, as the number of features increases too much, we tend to have problems with the three regressions we introduced in this post."
  },
  {
    "objectID": "posts/my-blog-post-05-audit-bias/index.html",
    "href": "posts/my-blog-post-05-audit-bias/index.html",
    "title": "My Blog post on Auditing Allocative Bias",
    "section": "",
    "text": "Reference\nHere is a link to the main guide and reference when we write this blog post.\nAnother reference is this paper that documents which variable means what in the PUMS data set we are going to use.\n\n\nIntroduction.\n\n\nImplementation\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nFirst, let’s import some libraries that we need.\n\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nIn the following code cell, we will generate a linearly separable dataset of binary-labeled 2D points. The make_blobs function essentially takes \\(n\\) samples, a number of features, and classes, and spits out a dataset of points with a data set with the given size, and label the data points using the classes. Visually, we see two clusters of points of two different color. In this special case where I set the seed, those two clusters seems linearly separable, which just means we could draw a straight line that completely seprates them. If we go to higher dimensions, then we need precise mathematical definitions, but we don’t need to worry about that right now.\n\n\nGetting the data using folktables\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\n\nSTATE = \"IN\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000042\n      3\n      1\n      2000\n      2\n      18\n      1013097\n      46\n      20\n      ...\n      7\n      6\n      82\n      44\n      6\n      76\n      83\n      44\n      45\n      44\n    \n    \n      1\n      P\n      2018GQ0000053\n      3\n      1\n      2306\n      2\n      18\n      1013097\n      19\n      48\n      ...\n      16\n      19\n      37\n      23\n      2\n      19\n      2\n      2\n      40\n      18\n    \n    \n      2\n      P\n      2018GQ0000074\n      3\n      1\n      2000\n      2\n      18\n      1013097\n      88\n      20\n      ...\n      166\n      158\n      160\n      90\n      87\n      84\n      88\n      90\n      13\n      166\n    \n    \n      3\n      P\n      2018GQ0000118\n      3\n      1\n      401\n      2\n      18\n      1013097\n      72\n      20\n      ...\n      11\n      10\n      11\n      11\n      71\n      11\n      70\n      11\n      74\n      135\n    \n    \n      4\n      P\n      2018GQ0000319\n      3\n      1\n      200\n      2\n      18\n      1013097\n      97\n      22\n      ...\n      15\n      170\n      93\n      181\n      175\n      92\n      174\n      16\n      96\n      95\n    \n  \n\n5 rows × 286 columns\n\n\n\n\n\nData wrangling, applying Logistic Regression\nWe recall the equation for a linear regression first: \\[ y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n, \\] where, \\(\\beta_i\\)’s are coefficients, \\(y\\) is the depedent variable, and the \\(X_i\\)’s are regressors (independent variables). Now, we recall logistic function (or sigmoid function), which is \\[ f(x) = \\frac{1}{1+e^{-x}}, \\] and when we put those two piece together, we obtain the formula for logistic regression: \\[ y = \\frac{1}{1+e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n}}, \\]\nHere are some of the variables that are important for our analysis: * PINCP is total personal income. * ESR is employment status coded as a dummy variable (1 if employed, 0 if not) * SEX is binary sex, coded 1 for male, and 2 for female. * RAC1P is race (1for White Alone, 2 for Black/African American alone, 3 and above for other self-identified racial groups) * DEAR, DEYE, and DERM refers to disability status relating to ear, eye, etc. * AGEP is Age, represented as integers. * SCHL is educational attainment, coded as integers. * MAR is Marital status, coded using integers. * RELP is Relationship. * COW is class of worker, coded using integers. * OCCP is occupation. * POBP is place of birth. * WKHP is usual hours worked per week in the past 12 months.\n\nLet’s consider the following task: we are going to\n\n\nTrain a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including race, and\n\n\nPerform a bias audit of our algorithm to determine whether it displays racial bias.\n\n\nFirst, let’s be more specific:\n\n\nmy_features=['PINCP', 'ESR', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P']\n# new_df = acs_data[my_features]\n# new_df['INCOME'] = np.where(new_df['PINCP'] >= 70000, 1, 0)\n\n\n# new_df.loc[new_df['PINCP'] >= 70000]\n# new_df.loc[new_df['ESR'] == 1]\n\n\nfeatures_to_use = [f for f in my_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nprint(features_to_use)\n\n['PINCP', 'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX']\n\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    # group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nWorking with pd.DataFrame, apply logistic regression in Python with scikit-learn.\n\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\n# df.loc[df['group'] == 2]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      PINCP\n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      group\n      label\n    \n  \n  \n    \n      0\n      0.0\n      31.0\n      20.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n    \n    \n      1\n      32000.0\n      50.0\n      18.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      2\n      0.0\n      2.0\n      0.0\n      5.0\n      2.0\n      2.0\n      5.0\n      1.0\n      1.0\n      0.0\n      4.0\n      1.0\n      2.0\n      2.0\n      0.0\n      1.0\n      1\n      False\n    \n    \n      3\n      0.0\n      14.0\n      9.0\n      5.0\n      2.0\n      2.0\n      7.0\n      1.0\n      3.0\n      0.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n    \n    \n      4\n      38000.0\n      58.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n  \n\n\n\n\n\nBasic Discriptives\nUsing this data frame, we first answer the following questions:\n\nHow many individuals are in the data?\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\nOf these individuals, how many are in each of the groups?\nIn each group, what proportion of individuals have target label equal to 1?\nCheck for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex. This might be a good opportunity to use a visualization such as a bar chart, e.g. via the seaborn package.\n\n\nprint(f\"The Number of Rows is: {df.shape[0]}\")\nprint(f\"The Number of Columns is: {df.shape[1]}\")\nprint(f\"The Number of individual who are employed is: {df.loc[df['label'] == True].shape[0]}\")\nprint(f\"The Percentage of individuals who are employed is: {24858/54144}\")\n###\nprint(f\"The Number of person who identify as black is: {df.loc[df['group']==2].shape[0]}\")\nprint(f\"The Number of person who identify as white is: {df.loc[df['group']==1].shape[0]}\")\nprint(f\"The Number of person who identify as black and is currently employed: {df.loc[(df['group']==2) & (df['label']==True)].shape[0]}\")\nprint(f\"The Number of person who identify as white and is currently employed: {df.loc[(df['group']==1) & (df['label']==True)].shape[0]}\")\n###\nprint(f\"The Number of person who identify as other racial groups is: {df.loc[df['group']>= 3].shape[0]}\")\nprint(f\"The Number of person who identify as other racial groups and is currently employed: {df.loc[(df['group']>= 3) & (df['label']==True)].shape[0]}\")\n###\nprint(f\"The Percentage of person who identify as black and is also employed is: {1374/3626}\")\nprint(f\"The Percentage of person who identify as white and is also employed is: {22200/47332}\")\nprint(f\"The Percentage of person who identify as other racial groups and is also employed is: {1284/3186}\")\n\nThe Number of Rows is: 54144\nThe Number of Columns is: 18\nThe Number of individual who are employed is: 24858\nThe Percentage of individuals who are employed is: 0.4591090425531915\nThe Number of person who identify as black is: 3626\nThe Number of person who identify as white is: 47332\nThe Number of person who identify as black and is currently employed: 1374\nThe Number of person who identify as white and is currently employed: 22200\nThe Number of person who identify as other racial groups is: 3186\nThe Number of person who identify as other racial groups and is currently employed: 1284\nThe Percentage of person who identify as black and is also employed is: 0.3789299503585218\nThe Percentage of person who identify as white and is also employed is: 0.4690272965435646\nThe Percentage of person who identify as other racial groups and is also employed is: 0.4030131826741996\n\n\n\nprint( df.groupby(\"SEX\").size() )\n\nSEX\n1.0    26578\n2.0    27566\ndtype: int64\n\n\nWe observe that there are \\(26578\\) males and \\(27566\\) females in the data set. Using the .groupby function, we see that we can more efficiently obtain the information we needed than the methods demonstrated in the above cell blocks.\n\nprint( df.groupby(\"group\").size() )\n\ngroup\n1    47332\n2     3626\n3       83\n4        1\n5       29\n6      942\n7       17\n8      906\n9     1208\ndtype: int64\n\n\nSince group 1 denotes white individuals, and group 2 denotes black individuals, we could read off of the previous block of code that there are \\(47332\\) white individuals and \\(3636\\) black individuals in Indiana in 2018.\n\nprint(df.groupby(\"label\").size())\n\nlabel\nFalse    29286\nTrue     24858\ndtype: int64\n\n\nWe see that in Indiana in 2018, there’s \\(29286\\) persons who are unemployed, and there’s \\(24858\\) persons who are employed. Again, using .groupby is much more efficient than what we did previously. The total number of persons in the data set is \\(54144.\\) The (average) percentage of individuals who are employed is about \\(46\\) percent.\n\nprint(df.groupby(['group', 'label']).size())\nprint(\"the average employment rate (of all people in all groups, in Indiana, in 2018) is: \" , df[\"label\"].mean())\n\ngroup  label\n1      False    25132\n       True     22200\n2      False     2252\n       True      1374\n3      False       50\n       True        33\n4      True         1\n5      False       17\n       True        12\n6      False      492\n       True       450\n7      False        6\n       True        11\n8      False      516\n       True       390\n9      False      821\n       True       387\ndtype: int64\nthe average employment rate (of all people in all groups, in Indiana, in 2018) is:  0.4591090425531915\n\n\nWe see that in group 1, for white individuals in Indiana, in 2018, the number of unemployed individual is \\(25132\\), and the number of employed individual is \\(22200\\). Similarly, for black individuals in Indiana, in 2018, the number of unemployed individual is \\(2252\\), and the number of employed individual is \\(1374\\). However, it might be more clear to see the employment rate for better comparison. The following line of code shows the employment rate for each group. The employment rate for white individual in IN in 2018 is \\(47\\) percent. The employment for black individual in IN in 2018 is \\(37.9\\) percent. Hence, we see that the employment rate is higher for a white individual than a black individual statistically.\n\nprint( df.groupby(\"group\")[\"label\"].mean() )\n\ngroup\n1    0.469027\n2    0.378930\n3    0.397590\n4    1.000000\n5    0.413793\n6    0.477707\n7    0.647059\n8    0.430464\n9    0.320364\nName: label, dtype: float64\n\n\nMore efficiently, we could use the following line of code and read off the employment rate for persons based on their race and gender binary. We see that for people identifying as white male, the employment rate is \\(50.7\\) percent. For people identifying as white female, the employment rate is \\(43.2\\) percent. Similarly, we see that for people identifying as black male, the employment rate is \\(35.7\\) percent. For people identifying as black female, the employment rate is \\(40\\) percent.\n\ndf.groupby([\"group\",\"SEX\"])[\"label\"].mean()\n\ngroup  SEX\n1      1.0    0.507493\n       2.0    0.431978\n2      1.0    0.357579\n       2.0    0.400000\n3      1.0    0.348837\n       2.0    0.450000\n4      2.0    1.000000\n5      1.0    0.352941\n       2.0    0.500000\n6      1.0    0.510067\n       2.0    0.448485\n7      1.0    0.666667\n       2.0    0.625000\n8      1.0    0.475877\n       2.0    0.384444\n9      1.0    0.312178\n       2.0    0.328000\nName: label, dtype: float64\n\n\nThe below graph shows the number of female in each racial group and male in each racial group for the PUMS data of Indiana in 2018. Recall that group 1, shown in blue here, denotes white individuals, and group 2, shown in orange here, denotes black individuals. The rest of the groups denotes several other racial groups, and the detailed encoding could be accessed on PUMS website. The main takaway here is that the population in Indiana in 2018 is predominantly white.\n\ncounts = df.groupby([\"group\", \"SEX\"]).size().reset_index(name = \"n\")\nsns.barplot(data = counts, x = \"SEX\", y = \"n\", hue = \"group\")\n\n<AxesSubplot:xlabel='SEX', ylabel='n'>\n\n\n\n\n\nIn the following cell, we show a bar graph of average employment rate for individuals in different racial and gender binary categories. We should ignore group 4, since previously, as we are tallying the number of individuals in each racial group, we see that there is only one person in group 4, and that person is also recorded as employed, so group 4 female has a employment rate of \\(100\\) percent is because there’s only one person who is also employed.\n\n\npercentages = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index()\nsns.barplot(data = percentages, x = \"SEX\", y = \"label\", hue = \"group\")\n\n<AxesSubplot:xlabel='SEX', ylabel='label'>\n\n\n\n\n\n\n\n\nMoving on to training my model\nAfter consideration, we decide to go with logistic regression. We build our model, and we fit our model on our training data, which is stored in variable X_train, and y_train.\n\n\n# model = make_pipeline(StandardScaler(), LogisticRegression())\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nmodel.fit(X_train, y_train)\n\nLogisticRegression(random_state=0, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0, solver='liblinear')\n\n\n\ny_hat = model.predict(X_test)\n\n\nprint(f\" The overall accuracy in predicting whether someone is employed in 2018 in Indiana is: {(y_hat == y_test).mean()}\", \"\\n\",\n    f\" The accuracy for white individuals is {(y_hat == y_test)[group_test == 1].mean()}\", \"\\n\", \n    f\" The accuracy for black individuals is {(y_hat == y_test)[group_test == 2].mean()}\")\n\n The overall accuracy in predicting whether someone is employed in 2018 in Indiana is: 0.8331855791962175 \n  The accuracy for white individuals is 0.8338424983027835 \n  The accuracy for black individuals is 0.8265086206896551\n\n\n\nTrain my model\nFirst let us fit our model.\n\nregress = model.fit(X_train, y_train)\n\nNow we score our model on the test sets, and we could read off the overall test score as \\(0.833\\).\n\nregress.score(X_test, y_test)\n\n0.8331855791962175\n\n\n\nmodel.score(X_test, y_test)\n\n0.8331855791962175\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_hat))\n\n              precision    recall  f1-score   support\n\n       False       0.82      0.89      0.85      7277\n        True       0.86      0.77      0.81      6259\n\n    accuracy                           0.83     13536\n   macro avg       0.84      0.83      0.83     13536\nweighted avg       0.83      0.83      0.83     13536\n\n\n\n\n\n\nAudit the model\n\nOverall Measures\n\nWhat is the overall accuracy of your model?\nWhat is the positive predictive value (PPV) of your model?\nWhat are the overall false negative and false positive rates (FNR and FPR) for your model? ### By-Group Measures\nWhat is the accuracy of your model on each subgroup?\nWhat is the PPV of your model on each subgroup?\nWhat are the FNR and FPR on each subgroup? ### Bias Measures\nSee Chouldechova (2017) for definitions of these terms. For calibration, you can think of the score as having only two values, 0 and 1.\nIs your model approximately calibrated?\nDoes your model satisfy approximate error rate balance?\nDoes your model satisfy statistical parity?\n\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\nprint(features)\nprint(group)\nprint(label)\n\n(67680, 16)\n(67680,)\n(67680,)\n[[1.70e+03 2.00e+01 1.90e+01 ... 2.00e+00 2.00e+00 2.00e+00]\n [9.80e+03 4.80e+01 1.30e+01 ... 2.00e+00 1.00e+00 2.00e+00]\n [3.50e+03 2.00e+01 1.90e+01 ... 2.00e+00 2.00e+00 1.00e+00]\n ...\n [2.11e+04 6.70e+01 1.60e+01 ... 1.00e+00 2.00e+00 1.00e+00]\n [8.40e+03 6.70e+01 1.90e+01 ... 2.00e+00 2.00e+00 2.00e+00]\n [5.50e+04 6.70e+01 2.10e+01 ... 2.00e+00 2.00e+00 1.00e+00]]\n[9 1 1 ... 1 1 1]\n[False False False ... False False  True]\n\n\nRecall that: * upper-left corner is TN, which stands for True negative * lower-left corner is FN, False negative * upper-right corner is FP, which stands for False positive * lower-right corner is TP, True positive\nWe also care about the FPR, which stands for the false positive rate, which is top-right corner of the confusion matrix (after we normalize). FNR is false negative rate.\n\nmy_matr = confusion_matrix(y_test, model.predict(X_test))\nfig, ax = plt.subplots(figsize=(4,4))\nax.imshow(my_matr)\nax.xaxis.set(ticks=(0,1), ticklabels=('Predicted False', 'Predicted True'))\nax.yaxis.set(ticks=(0,1), ticklabels=('Actually False', 'Actually True'))\nax.set_ylim(1.5, -0.5)\n\nfor i in range(2):\n    for j in range(2):\n        ax.text(j,i, my_matr[i,j], ha='center', va='center', color='black')\n\n\n\n\n\n\nmy_matr = confusion_matrix(y_test, model.predict(X_test), normalize=\"true\")\nfig, ax = plt.subplots(figsize=(4,4))\nax.imshow(my_matr)\nax.xaxis.set(ticks=(0,1), ticklabels=('Predicted False', 'Predicted True'))\nax.yaxis.set(ticks=(0,1), ticklabels=('Actually False', 'Actually True'))\nax.set_ylim(1.5, -0.5)\n\nfor i in range(2):\n    for j in range(2):\n        ax.text(j,i, my_matr[i,j].round(4), ha='center', va='center', color='black')\n\n\n\n\nThe positive predictive value (PPV) is obtained by using this formula: \\[PPV = \\frac{TP}{TP+FP},\\] where \\(TP\\) denotes True Positive, and \\(FP\\) denotes True Negative. Hence, we need the value from lower-right corner (TP) of the confusion matrix divided by the value from lower right corner (TP) plus upper-right corner (FP).\n\n\nConcluding Discussion"
  },
  {
    "objectID": "posts/my-blog-post-06-unsupervised-learning/index.html",
    "href": "posts/my-blog-post-06-unsupervised-learning/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "# %reload_ext autoreload\n\n\nLink to Source Code\nHereis a link to the source code for this post.\n\n\nLink to reference for this blog post\nHere is a link to the main reference we use as we implement this post.\n\n\nPart I: Image Compression with the Singular Value Decomposition\nWe use the image of a cat which can be accessed here for free download: www.pexels.com. I have already downloaded a copy of an image of a tabby cat, and I have stored it in the same directory as this jupyter notebook.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nimport PIL\nfrom PIL import Image\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n# url = \"https://images.pexels.com/photos/1170986/pexels-photo-1170986.jpeg?cs=srgb&dl=pexels-evg-kowalievska-1170986.jpg&fm=jpg\"\n# myimg = read_image(url)\n\n\n# open the image from working directory\nimg = Image.open(\"./tabby_cat.png\")\nprint(f\"format: {img.format}\")\nprint(f\"size: {img.size}\")\nprint(f\"mode: {img.mode}\")\n# convert PIL images into numpy arrays.\nmyimg = np.asarray(img)\n\nformat: JPEG\nsize: (1771, 2657)\nmode: RGB\n\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(myimg)\n\naxarr[0].imshow(myimg)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\nprint(grey_img.shape[0])\nprint(grey_img.shape[1])\n\n2657\n1771\n\n\n\nfrom hidden_images import svd\nsvd1 = svd()\nk = 18 \nA_ = svd1.reconstruct(grey_img, k)\nsvd1.compare_images(grey_img, A_)\n\n\n\n\n\nsvd1.experiment(grey_img) \n\n\n\n\n\n\nPart II: Spectral Community Detection\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nWe need to return a vector of binary labels to split the graph.\n\nclubs = nx.get_node_attributes(G, \"club\")\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\n\nImplementing Laplacian Spectral Clustering\n\nfrom sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\nX, y = make_blobs(n_samples=100, n_features=2, \n                                centers=2, random_state=1)\n\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nnp.random.seed(42)\n\nn = 500\nX, y = make_circles(n_samples=n, shuffle=True, noise=0.07, random_state=None, factor = 0.5)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\nk = 10\nnbrs = NearestNeighbors(n_neighbors=k).fit(X)\nA = nbrs.kneighbors_graph().toarray()\n\n# symmetrize the matrix\nA = A + A.T\nA[A > 1] = 1\n\n\nimport networkx as nx\nfrom hidden_spectral import spectral \nspec  = spectral()\nspec.plot_graph(X, A)\n\n\n\n\n\n# fig, axarr = plt.subplots(1, 2, figsize = (8, 4))\ny_bad = np.random.randint(0, 2, n)\n\n# plot_graph(X, A, z = y, ax = axarr[0])\n# plot_graph(X, A, z = y_bad, ax = axarr[1])\n\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import pairwise_distances\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ngood labels cut = 22.0\nbad labels cut = 3000.0\ngood labels cut = 22.0\nbad labels cut = 3000.0\n\n\n\n\nUse a theorem from linear algebra\n\\(z\\) should be the eigenvector with the second smallest eigenvalue of the matrix \\[L = D^{-1}\\left[ D-A \\right]. \\] This matrix \\(L\\) is called the normalized Laplacian.\n\nfrom hidden_spectral import spectral \nspec = spectral()\nfig, ax = plt.subplots(figsize = (4, 4))\nz_ = spec.second_laplacian_eigenvector(A=A)\nspec.plot_graph(X, A, z=z_, ax = ax, show_edge_cuts = False)\n\n\n\n\n\nz = z_ > 0\nfig, ax = plt.subplots(figsize = (4, 4))\nspec.plot_graph(X, A, z, show_edge_cuts = True, ax = ax)\n\n\n\n\n\nfrom sklearn.datasets import make_moons\n\nH, z = make_moons(n_samples=100, random_state=1, noise = .1)\nfig, ax = plt.subplots(figsize = (4, 4))\na = ax.scatter(H[:, 0], H[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfig, ax = plt.subplots(figsize = (4, 4))\nz_hat = spec.spectral_clustering(H, k=6)\na = ax.scatter(H[:, 0], H[:, 1], c = z_hat, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spec.spectral_clustering(X, k = i)\n    a = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\n\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spec.spectral_clustering(H, k = i)\n    a = ax.scatter(H[:, 0], H[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\n\n\nTesting\n\n\nL = np.diag([4,5,22,2,3])\n# print(L)\nevalue, evector = np.linalg.eig(L)\nprint(\"evalue\")\nprint(evalue)\nprint(\"evector\")\nprint(evector)\nk = L.shape[1] \nidx = evalue.argsort()[:k][::-1] \nevalue = evalue[idx]\nevector = evector[:, idx]\n\nprint(\"evalue after change\")\nprint(evalue)\nprint(\"evector after change\")\nprint(evector)\n\nindex = evector.shape[1]\nprint(\"col\")\nprint(evector[:,index-2])\n\nevalue\n[ 4.  5. 22.  2.  3.]\nevector\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\nevalue after change\n[22.  5.  4.  3.  2.]\nevector after change\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 1. 0.]]\ncol\n[0. 0. 0. 0. 1.]"
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Here is a link to the source code for this Penguin Classification blog post."
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html#try-on-a-bigger-example",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html#try-on-a-bigger-example",
    "title": "Classifying Palmer Penguins",
    "section": "Try on a bigger example",
    "text": "Try on a bigger example"
  },
  {
    "objectID": "posts/my-blog-post-08-Palmer-Penguins/index.html#lets-try-a-different-data-set",
    "href": "posts/my-blog-post-08-Palmer-Penguins/index.html#lets-try-a-different-data-set",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s try a different data set",
    "text": "Let’s try a different data set"
  }
]